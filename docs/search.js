window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "dl4ds", "modulename": "dl4ds", "type": "module", "doc": "<p><a href=\"https://github.com/carlgogo/dl4ds\"><img src=\"https://img.shields.io/badge/Tensorflow%2FKeras-2.6+-blue&amp;logo=tensorflow\" alt=\"Tensorflow - Version\" /></a> \n<a href=\"https://github.com/carlgogo/dl4ds\"><img src=\"https://img.shields.io/badge/PYTHON-3.6+-red?style=flat&amp;logo=python&amp;logoColor=white\" alt=\"Python - Version\" /></a> </p>\n\n<h1 id=\"dl4ds-deep-learning-for-empirical-downscaling\">DL4DS - Deep Learning for empirical DownScaling</h1>\n\n<p>Python package implementing state-of-the-art and novel deep learning algorithms for empirical downscaling of gridded data. DL4DS is built on top of Tensorflow/Keras and supports distributed GPU training (data parallelism) thanks to Horovod. The training can be done from explicit pairs of HR and LR samples (e.g., HR observations and LR numerical weather prediction model output) or only with a HR dataset (e.g., HR observations or HR model output). All the models are able to handle multiple predictors and an arbitrary number of static variables (e.g., elevation or land-ocean mask).</p>\n\n<p><img src=\"https://github.com/carlos-gg/dl4ds/raw/master/docs/img/fig_workflow.png\" alt=\"drawing\" width=\"800\"/></p>\n\n<p>A wide variety of network architectures have been implemented. The main modelling approaches can be combined into many different architectures:</p>\n\n<table>\n<thead>\n<tr>\n  <th>Downscaling type</th>\n  <th>Training (loss type)</th>\n  <th>Sample type</th>\n  <th>Backbone module</th>\n  <th>Upsampling method</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>MOS (explicit pairs)</td>\n  <td>Supervised (non-adversarial)</td>\n  <td>Spatial</td>\n  <td>Plain convolutional</td>\n  <td>Pre-upsampling: interpolation (PIN)</td>\n</tr>\n<tr>\n  <td>PerfectProg (implicit pairs)</td>\n  <td>Adversarial (conditional)</td>\n  <td>Spatio-temporal</td>\n  <td>Residual</td>\n  <td>Post-upsampling: sub-pixel convolution (SPC)</td>\n</tr>\n<tr>\n  <td></td>\n  <td></td>\n  <td></td>\n  <td>Dense</td>\n  <td>Post-upsampling: resize convolution (RC)</td>\n</tr>\n<tr>\n  <td></td>\n  <td></td>\n  <td></td>\n  <td>Unet (PIN, Spatial samples)</td>\n  <td>Post-upsampling: deconvolution (DC)</td>\n</tr>\n<tr>\n  <td></td>\n  <td></td>\n  <td></td>\n  <td>Convnext (Spatial samples)</td>\n  <td></td>\n</tr>\n</tbody>\n</table>\n\n<h2 id=\"extended-documentation\">Extended documentation</h2>\n\n<p>Check out the API documentation <a href=\"https://carlos-gg.github.io/dl4ds/\">here</a>.</p>\n\n<h2 id=\"examples\">Examples</h2>\n\n<p>Colab notebooks are WIP. Stay tuned!</p>\n"}, {"fullname": "dl4ds.app", "modulename": "dl4ds.app", "type": "module", "doc": "<p>absl.FLAGS-based command line app. To be executed run something like this:</p>\n\n<p>python -m dl4ds.app --flagfile=params.cfg</p>\n"}, {"fullname": "dl4ds.app.dl4ds", "modulename": "dl4ds.app", "qualname": "dl4ds", "type": "function", "doc": "<p>DL4DS absl.FLAGS-based command line app.</p>\n", "signature": "(argv)", "funcdef": "def"}, {"fullname": "dl4ds.dataloader", "modulename": "dl4ds.dataloader", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.dataloader.create_pair_hr_lr", "modulename": "dl4ds.dataloader", "qualname": "create_pair_hr_lr", "type": "function", "doc": "<p>Create a pair of HR and LR square sub-patches. In this case, the LR \ncorresponds to a coarsen version of the HR reference with land-ocean mask,\ntopography and auxiliary predictors added as \"image channels\".</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>array</strong> (np.ndarray):\nHR gridded data.</li>\n<li><strong>array_lr</strong> (np.ndarray):\nLR gridded data. If not provided, then implicit/coarsened pairs are\ncreated from <code>array</code>.</li>\n<li><strong>upsampling</strong> (str):\nString with the name of the upsampling method.</li>\n<li><strong>scale</strong> (int):\nScaling factor.</li>\n<li><strong>patch_size</strong> (int or None):\nSize of the square patches to be extracted, in pixels for the HR grid.</li>\n<li><strong>static_vars</strong> (None or list of 2D ndarrays, optional):\nStatic variables such as elevation data or a binary land-ocean mask.</li>\n<li><strong>predictors</strong> (np.ndarray, optional):\nPredictor variables in HR. To be concatenated to the LR version of \n<code>array</code>.</li>\n<li><strong>interpolation</strong> (str, optional):\nInterpolation used when upsampling/downsampling the training samples.\nBy default 'bicubic'.</li>\n<li><strong>debug</strong> (bool, optional):\nIf True, plots and debugging information are shown.</li>\n</ul>\n", "signature": "(\n    array,\n    array_lr,\n    upsampling,\n    scale,\n    patch_size,\n    static_vars=None,\n    predictors=None,\n    season=None,\n    debug=False,\n    interpolation='inter_area'\n)", "funcdef": "def"}, {"fullname": "dl4ds.dataloader.create_batch_hr_lr", "modulename": "dl4ds.dataloader", "qualname": "create_batch_hr_lr", "type": "function", "doc": "<p>Create a batch of HR/LR samples.</p>\n", "signature": "(\n    all_indices,\n    index,\n    array,\n    array_lr,\n    upsampling,\n    scale=4,\n    batch_size=32,\n    patch_size=None,\n    time_window=None,\n    static_vars=None,\n    predictors=None,\n    interpolation='inter_area',\n    time_metadata=None\n)", "funcdef": "def"}, {"fullname": "dl4ds.dataloader.DataGenerator", "modulename": "dl4ds.dataloader", "qualname": "DataGenerator", "type": "class", "doc": "<p>DataGenerator creates batches of paired training samples according to the\nupsampling and other parameters. This class is used within the \n<code>dl4ds.SupervisedTrainer</code>.</p>\n\n<p>A sequence structure guarantees that the network will only train once on \neach sample per epoch which is not the case with generators. \nEvery Sequence must implement the __getitem__ and the __len__ methods. If \nyou want to modify your dataset between epochs you may implement \non_epoch_end. The method __getitem__ should return a complete batch.</p>\n", "bases": "keras.utils.data_utils.Sequence"}, {"fullname": "dl4ds.dataloader.DataGenerator.__init__", "modulename": "dl4ds.dataloader", "qualname": "DataGenerator.__init__", "type": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>array</strong> (np.ndarray):\nHR gridded data.</li>\n<li><strong>array_lr</strong> (np.ndarray):\nLR gridded data. If not provided, then implicit/coarsened pairs are\ncreated from <code>array</code>.</li>\n<li><strong>backbone</strong> (str):\nString with the name of the backbone block.</li>\n<li><strong>upsampling</strong> (str):\nString with the name of the upsampling method.</li>\n<li><strong>scale</strong> (int):\nScaling factor.</li>\n<li><strong>batch_size</strong> (int, optional):\nHow many samples are included in each batch.</li>\n<li><strong>patch_size</strong> (int or None):\nSize of the square patches to be extracted, in pixels for the HR grid.</li>\n<li><strong>time_window</strong> (int or None, optional):\nIf not None, then each sample will have a temporal dimension \n(<code>time_window</code> slices to the past are grabbed for the LR array).</li>\n<li><strong>static_vars</strong> (None or list of 2D ndarrays, optional):\nStatic variables such as elevation data or a binary land-ocean mask.</li>\n<li><strong>predictors</strong> (list of ndarray):\nList of predictor ndarrays.</li>\n<li><strong>interpolation</strong> (str, optional):\nInterpolation used when upsampling/downsampling the training samples.</li>\n<li><strong>repeat</strong> (int or None, optional):\nFactor to repeat the samples in <code>array</code>. Useful when <code>patch_size</code>\nis not None.</li>\n<li><strong>TO-DO</strong></li>\n<li><strong>-----</strong></li>\n<li><strong>* instead of the in-memory array, we could input the path and load the</strong></li>\n<li><strong>netcdf files lazily or memmap a numpy array</strong></li>\n</ul>\n", "signature": "(\n    self,\n    array,\n    array_lr,\n    backbone,\n    upsampling,\n    scale,\n    batch_size=32,\n    patch_size=None,\n    time_window=None,\n    static_vars=None,\n    predictors=None,\n    interpolation='inter_area',\n    repeat=None\n)", "funcdef": "def"}, {"fullname": "dl4ds.inference", "modulename": "dl4ds.inference", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.inference.Predictor", "modulename": "dl4ds.inference", "qualname": "Predictor", "type": "class", "doc": "<p>Predictor class for performing inference on unseen HR or LR data. The data \n(<code>array</code>) is super-resolved or downscaled using the trained \nsuper-resolution network (contained in <code>trainer</code>).</p>\n"}, {"fullname": "dl4ds.inference.Predictor.__init__", "modulename": "dl4ds.inference", "qualname": "Predictor.__init__", "type": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>trainer</strong> (dl4ds.SupervisedTrainer or dl4ds.CGANTrainer):\nTrainer containing a keras model (<code>model</code> or <code>generator</code>). \nOptionally, you can direclty pass the tf.keras model.</li>\n<li><strong>array</strong> (ndarray):\nBatch of HR grids.</li>\n<li><strong>scale</strong> (int):\nScaling factor.</li>\n<li><strong>array_in_hr</strong> (bool, optional):\nIf True, the data is assumed to be a HR groundtruth to be downsampled. \nOtherwise, data is a LR gridded dataset to be downscaled.</li>\n<li><strong>static_vars</strong> (None or list of 2D ndarrays, optional):\nStatic variables such as elevation data or binary masks.</li>\n<li><strong>predictors</strong> (list of ndarray, optional):\nPredictor variables for trianing. Given as list of 4D ndarrays with \ndims [nsamples, lat, lon, 1] or 5D ndarrays with dims \n[nsamples, time, lat, lon, 1].</li>\n<li><strong>time_window</strong> (int or None, optional):\nIf None, then the function assumes the <code>model</code> is spatial only. If \nan integer is given, then the <code>model</code> should be spatio-temporal \nand the samples are pre-processed accordingly.</li>\n<li><strong>interpolation</strong> (str, optional):\nInterpolation used when upsampling/downsampling the training samples.\nBy default 'bicubic'.</li>\n<li><strong>batch_size</strong> (int, optional):\nBatch size for feeding samples for inference.</li>\n<li><strong>scaler</strong> (None or dl4ds scaler object, optional):\nScaler for backward scaling and restoring original distribution.</li>\n<li><strong>save_path</strong> (str or None, optional):\nIf not None, the prediction (gridded variable at HR) is saved to disk.</li>\n<li><strong>save_fname</strong> (str, optional):\nFilename to complete the path were the prediciton is saved.</li>\n<li><strong>return_lr</strong> (bool, optional):\nIf True, the LR array is returned along with the downscaled one.</li>\n</ul>\n", "signature": "(\n    self,\n    trainer,\n    array,\n    scale,\n    array_in_hr=False,\n    static_vars=None,\n    predictors=None,\n    time_window=None,\n    time_metadata=None,\n    interpolation='inter_area',\n    batch_size=64,\n    scaler=None,\n    save_path=None,\n    save_fname='y_hat.npy',\n    return_lr=False,\n    device='GPU'\n)", "funcdef": "def"}, {"fullname": "dl4ds.inference.Predictor.run", "modulename": "dl4ds.inference", "qualname": "Predictor.run", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dl4ds.inference.predict", "modulename": "dl4ds.inference", "qualname": "predict", "type": "function", "doc": "<p>Inference on unseen HR or LR data. The data (<code>array</code>) is super-resolved \nor downscaled using the trained super-resolution network (<code>model</code>). </p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>trainer</strong> (dl4ds.SupervisedTrainer or dl4ds.CGANTrainer):\nTrainer containing a keras model (<code>model</code> or <code>generator</code>). \nOptionally, you can direclty pass the tf.keras model.</li>\n<li><strong>array</strong> (ndarray):\nBatch of HR grids.</li>\n<li><strong>scale</strong> (int):\nScaling factor.</li>\n<li><strong>array_in_hr</strong> (bool, optional):\nIf True, the data is assumed to be a HR groundtruth to be downsampled. \nOtherwise, data is a LR gridded dataset to be downscaled.</li>\n<li><strong>static_vars</strong> (None or list of 2D ndarrays, optional):\nStatic variables such as elevation data or a binary land-ocean mask.</li>\n<li><strong>predictors</strong> (list of ndarray, optional):\nPredictor variables for trianing. Given as list of 4D ndarrays with dims \n[nsamples, lat, lon, 1] or 5D ndarrays with dims [nsamples, time, lat, lon, 1].</li>\n<li><strong>time_window</strong> (int or None, optional):\nIf None, then the function assumes the <code>model</code> is spatial only. If an \ninteger is given, then the <code>model</code> should be spatio-temporal and the \nsamples are pre-processed accordingly.</li>\n<li><strong>interpolation</strong> (str, optional):\nInterpolation used when upsampling/downsampling the training samples.\nBy default 'bicubic'.</li>\n<li><strong>batch_size</strong> (int, optional):\nBatch size for feeding samples for inference.</li>\n<li><strong>scaler</strong> (None or dl4ds scaler object, optional):\nScaler for backward scaling and restoring original distribution.</li>\n<li><strong>save_path</strong> (str or None, optional):\nIf not None, the prediction (gridded variable at HR) is saved to disk.</li>\n<li><strong>save_fname</strong> (str, optional):\nFilename to complete the path were the prediciton is saved.</li>\n<li><strong>return_lr</strong> (bool, optional):\nIf True, the LR array is returned along with the downscaled one.</li>\n</ul>\n", "signature": "(\n    trainer,\n    array,\n    scale,\n    array_in_hr=True,\n    static_vars=None,\n    predictors=None,\n    time_window=None,\n    time_metadata=None,\n    interpolation='inter_area',\n    batch_size=64,\n    scaler=None,\n    save_path=None,\n    save_fname='y_hat.npy',\n    return_lr=False,\n    device='GPU'\n)", "funcdef": "def"}, {"fullname": "dl4ds.losses", "modulename": "dl4ds.losses", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.losses.mae", "modulename": "dl4ds.losses", "qualname": "mae", "type": "function", "doc": "<p>Mean absolute error, L1 pixel loss</p>\n", "signature": "(y_true, y_pred)", "funcdef": "def"}, {"fullname": "dl4ds.losses.mse", "modulename": "dl4ds.losses", "qualname": "mse", "type": "function", "doc": "<p>Mean squared error, L2 pixel loss</p>\n", "signature": "(y_true, y_pred)", "funcdef": "def"}, {"fullname": "dl4ds.losses.dssim", "modulename": "dl4ds.losses", "qualname": "dssim", "type": "function", "doc": "<p>Structural Dissimilarity (DSSIM). DSSIM is derived from the structural \nsimilarity index measure.</p>\n\n<h6 id=\"references\">References</h6>\n\n<p>Wang, Z. et al. 2004, Image quality assessment: from error visibility to \nstructural similarity: https://ieeexplore.ieee.org/document/1284395</p>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>https://www.tensorflow.org/api_docs/python/tf/image/ssim\ntf.image.ssim(img1, img2, max_val, filter_size=11, filter_sigma=1.5, k1=0.01, k2=0.03)</p>\n\n<p>https://github.com/keras-team/keras-contrib/issues/464\nhttps://github.com/keras-team/keras-contrib/blob/master/keras_contrib/losses/dssim.py</p>\n", "signature": "(y_true, y_pred)", "funcdef": "def"}, {"fullname": "dl4ds.losses.dssim_mae", "modulename": "dl4ds.losses", "qualname": "dssim_mae", "type": "function", "doc": "<p>DSSIM + MAE (L1)</p>\n", "signature": "(y_true, y_pred)", "funcdef": "def"}, {"fullname": "dl4ds.losses.dssim_mae_mse", "modulename": "dl4ds.losses", "qualname": "dssim_mae_mse", "type": "function", "doc": "<p>DSSIM + MAE (L1) + MSE (L2)</p>\n\n<h6 id=\"references\">References</h6>\n\n<p>Computer Vision in Precipitation Nowcasting: Applying Image Quality \nAssessment Metrics for Training Deep Neural Networks: \nhttps://www.mdpi.com/2073-4433/10/5/244/htm</p>\n", "signature": "(y_true, y_pred)", "funcdef": "def"}, {"fullname": "dl4ds.losses.dssim_mse", "modulename": "dl4ds.losses", "qualname": "dssim_mse", "type": "function", "doc": "<p>DSSIM + MSE (L2)</p>\n", "signature": "(y_true, y_pred)", "funcdef": "def"}, {"fullname": "dl4ds.losses.msdssim", "modulename": "dl4ds.losses", "qualname": "msdssim", "type": "function", "doc": "<p>Multiscale Structural Dissimilarity (MSDSSIM). </p>\n\n<h6 id=\"references\">References</h6>\n\n<p>Wang, Z. 2003, \"Multiscale structural similarity for image quality \nassessment\": https://ieeexplore.ieee.org/document/1292216</p>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>https://www.tensorflow.org/api_docs/python/tf/image/ssim_multiscale</p>\n\n<p>power_factors: Iterable of weights for each of the scales. The number of \nscales used is the length of the list. Index 0 is the unscaled resolution's \nweight and each increasing scale corresponds to the image being downsampled \nby 2. Defaults to (0.0448, 0.2856, 0.3001, 0.2363, 0.1333), which are the \nvalues obtained in the original paper.\nfilter_size: Default value 11 (size of gaussian filter).\nfilter_sigma: Default value 1.5 (width of gaussian filter).</p>\n", "signature": "(y_true, y_pred)", "funcdef": "def"}, {"fullname": "dl4ds.losses.msdssim_mae", "modulename": "dl4ds.losses", "qualname": "msdssim_mae", "type": "function", "doc": "<p>MSDSSIM + MAE (L1)</p>\n", "signature": "(y_true, y_pred)", "funcdef": "def"}, {"fullname": "dl4ds.losses.msdssim_mae_mse", "modulename": "dl4ds.losses", "qualname": "msdssim_mae_mse", "type": "function", "doc": "<p>MSDSSIM + MAE (L1) + MSE (L2)</p>\n", "signature": "(y_true, y_pred)", "funcdef": "def"}, {"fullname": "dl4ds.metrics", "modulename": "dl4ds.metrics", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.metrics.compute_rmse", "modulename": "dl4ds.metrics", "qualname": "compute_rmse", "type": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>squared</strong> (bool):\nIf True returns MSE value, if False returns RMSE value.</li>\n<li><strong>https</strong> (//scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html):</li>\n</ul>\n", "signature": "(y, y_hat, over='time', squared=False, n_jobs=40)", "funcdef": "def"}, {"fullname": "dl4ds.metrics.compute_correlation", "modulename": "dl4ds.metrics", "qualname": "compute_correlation", "type": "function", "doc": "<p>https://scipy.github.io/devdocs/generated/scipy.stats.spearmanr.html</p>\n\n<p>https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html</p>\n", "signature": "(y, y_hat, over='time', mode='spearman', n_jobs=40)", "funcdef": "def"}, {"fullname": "dl4ds.metrics.compute_metrics", "modulename": "dl4ds.metrics", "qualname": "compute_metrics", "type": "function", "doc": "<p>Compute temporal and spatial-wise metrics, e.g., RMSE and CORRELATION, \nbased on the groundtruth and prediction ndarrays.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>y_test</strong> (np.ndarray):\nGroundtruth.</li>\n<li><strong>y_test_hat</strong> (np.ndarray):\nPrediction.</li>\n<li><strong>dpi</strong> (int, optional):\nDPI of the plots.</li>\n<li><strong>n_jobs</strong> (int, optional):\nNumber of cores for the computation of metrics (parallelizing over\ngrid points). Passed to joblib.Parallel. If -1 all CPUs are used. If 1 \nor None is given, no parallel computing code is used at all, which is \nuseful for debugging.</li>\n<li><strong>scaler</strong> (scaler object):\nScaler object from preprocessing module.</li>\n<li><strong>mask</strong> (np.ndarray or None):\nBinary mask with valid (ones) and non-valid (zeroes) grid points.</li>\n<li><strong>save_path</strong> (str or None, optional):\nPath to save results to disk.</li>\n</ul>\n", "signature": "(\n    y_test,\n    y_test_hat,\n    dpi=150,\n    plot_size_px=1000,\n    n_jobs=-1,\n    scaler=None,\n    mask=None,\n    save_path=None\n)", "funcdef": "def"}, {"fullname": "dl4ds.models", "modulename": "dl4ds.models", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.models.blocks", "modulename": "dl4ds.models.blocks", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.models.blocks.ConvBlock", "modulename": "dl4ds.models.blocks", "qualname": "ConvBlock", "type": "class", "doc": "<p>Convolutional block.</p>\n\n<h6 id=\"references\">References</h6>\n\n<p>[1] Effective and Efficient Dropout for Deep Convolutional Neural Networks: \nhttps://arxiv.org/abs/1904.03392\n[2] Rethinking the Usage of Batch Normalization and Dropout: \nhttps://arxiv.org/abs/1905.05928</p>\n", "bases": "keras.engine.base_layer.Layer"}, {"fullname": "dl4ds.models.blocks.ConvBlock.__init__", "modulename": "dl4ds.models.blocks", "qualname": "ConvBlock.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    filters,\n    strides=1,\n    ks_cl1=(3, 3),\n    ks_cl2=(3, 3),\n    activation='relu',\n    normalization=None,\n    attention=False,\n    dropout_rate=0,\n    dropout_variant=None,\n    depthwise_separable=False,\n    name=None,\n    **conv_kwargs\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.ConvBlock.call", "modulename": "dl4ds.models.blocks", "qualname": "ConvBlock.call", "type": "function", "doc": "<p>Model's forward pass.</p>\n", "signature": "(self, X)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.DropPath", "modulename": "dl4ds.models.blocks", "qualname": "DropPath", "type": "class", "doc": "<p>Drop path layer. </p>\n\n<p>Adapted from \nhttps://github.com/rishigami/Swin-Transformer-TF/blob/main/swintransformer/model.py</p>\n", "bases": "keras.engine.base_layer.Layer"}, {"fullname": "dl4ds.models.blocks.DropPath.__init__", "modulename": "dl4ds.models.blocks", "qualname": "DropPath.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, drop_prob=None)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.DropPath.call", "modulename": "dl4ds.models.blocks", "qualname": "DropPath.call", "type": "function", "doc": "<p>This is where the layer's logic lives.</p>\n\n<p>The <code>call()</code> method may not create state (except in its first invocation,\nwrapping the creation of variables or other resources in <code>tf.init_scope()</code>).\nIt is recommended to create state in <code>__init__()</code>, or the <code>build()</code> method\nthat is called automatically before <code>call()</code> executes the first time.</p>\n\n<p>Args:\n  inputs: Input tensor, or dict/list/tuple of input tensors.\n    The first positional <code>inputs</code> argument is subject to special rules:\n    - <code>inputs</code> must be explicitly passed. A layer cannot have zero\n      arguments, and <code>inputs</code> cannot be provided via the default value\n      of a keyword argument.\n    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.\n    - Keras mask metadata is only collected from <code>inputs</code>.\n    - Layers are built (<code>build(input_shape)</code> method)\n      using shape info from <code>inputs</code> only.\n    - <code>input_spec</code> compatibility is only checked against <code>inputs</code>.\n    - Mixed precision input casting is only applied to <code>inputs</code>.\n      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their\n      casting behavior in mixed precision should be handled manually.\n    - The SavedModel input specification is generated using <code>inputs</code> only.\n    - Integration with various ecosystem packages like TFMOT, TFLite,\n      TF.js, etc is only supported for <code>inputs</code> and not for tensors in\n      positional and keyword arguments.\n  <em>args: Additional positional arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n  *</em>kwargs: Additional keyword arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n    The following optional keyword arguments are reserved:\n    - <code>training</code>: Boolean scalar tensor of Python boolean indicating\n      whether the <code>call</code> is meant for training or inference.\n    - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a\n      <code>mask</code> argument, its default value will be set to the mask generated\n      for <code>inputs</code> by the previous layer (if <code>input</code> did come from a layer\n      that generated a corresponding mask, i.e. if it came from a Keras\n      layer with masking support).</p>\n\n<p>Returns:\n  A tensor or list/tuple of tensors.</p>\n", "signature": "(self, x, training=False)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.ConvNextBlock", "modulename": "dl4ds.models.blocks", "qualname": "ConvNextBlock", "type": "class", "doc": "<p>ConvNext block.</p>\n\n<h6 id=\"references\">References</h6>\n\n<p>[1] A ConvNet for the 2020s: https://arxiv.org/abs/2201.03545</p>\n", "bases": "keras.engine.base_layer.Layer"}, {"fullname": "dl4ds.models.blocks.ConvNextBlock.__init__", "modulename": "dl4ds.models.blocks", "qualname": "ConvNextBlock.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    filters,\n    drop_path=0.0,\n    layer_scale_init_value=0,\n    use_1x1conv=False,\n    activation='gelu',\n    normalization='ln',\n    name=None,\n    **conv_kwargs\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.ConvNextBlock.build", "modulename": "dl4ds.models.blocks", "qualname": "ConvNextBlock.build", "type": "function", "doc": "<p>Creates the variables of the layer (optional, for subclass implementers).</p>\n\n<p>This is a method that implementers of subclasses of <code>Layer</code> or <code>Model</code>\ncan override if they need a state-creation step in-between\nlayer instantiation and layer call. It is invoked automatically before\nthe first execution of <code>call()</code>.</p>\n\n<p>This is typically used to create the weights of <code>Layer</code> subclasses\n(at the discretion of the subclass implementer).</p>\n\n<p>Args:\n  input_shape: Instance of <code>TensorShape</code>, or list of instances of\n    <code>TensorShape</code> if the layer expects a list of inputs\n    (one instance per input).</p>\n", "signature": "(self, input_shape)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.ConvNextBlock.call", "modulename": "dl4ds.models.blocks", "qualname": "ConvNextBlock.call", "type": "function", "doc": "<p>This is where the layer's logic lives.</p>\n\n<p>The <code>call()</code> method may not create state (except in its first invocation,\nwrapping the creation of variables or other resources in <code>tf.init_scope()</code>).\nIt is recommended to create state in <code>__init__()</code>, or the <code>build()</code> method\nthat is called automatically before <code>call()</code> executes the first time.</p>\n\n<p>Args:\n  inputs: Input tensor, or dict/list/tuple of input tensors.\n    The first positional <code>inputs</code> argument is subject to special rules:\n    - <code>inputs</code> must be explicitly passed. A layer cannot have zero\n      arguments, and <code>inputs</code> cannot be provided via the default value\n      of a keyword argument.\n    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.\n    - Keras mask metadata is only collected from <code>inputs</code>.\n    - Layers are built (<code>build(input_shape)</code> method)\n      using shape info from <code>inputs</code> only.\n    - <code>input_spec</code> compatibility is only checked against <code>inputs</code>.\n    - Mixed precision input casting is only applied to <code>inputs</code>.\n      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their\n      casting behavior in mixed precision should be handled manually.\n    - The SavedModel input specification is generated using <code>inputs</code> only.\n    - Integration with various ecosystem packages like TFMOT, TFLite,\n      TF.js, etc is only supported for <code>inputs</code> and not for tensors in\n      positional and keyword arguments.\n  <em>args: Additional positional arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n  *</em>kwargs: Additional keyword arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n    The following optional keyword arguments are reserved:\n    - <code>training</code>: Boolean scalar tensor of Python boolean indicating\n      whether the <code>call</code> is meant for training or inference.\n    - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a\n      <code>mask</code> argument, its default value will be set to the mask generated\n      for <code>inputs</code> by the previous layer (if <code>input</code> did come from a layer\n      that generated a corresponding mask, i.e. if it came from a Keras\n      layer with masking support).</p>\n\n<p>Returns:\n  A tensor or list/tuple of tensors.</p>\n", "signature": "(self, x)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.ResidualBlock", "modulename": "dl4ds.models.blocks", "qualname": "ResidualBlock", "type": "class", "doc": "<p>Residual block. </p>\n\n<p>Two examples:</p>\n\n<ul>\n<li>Standard residual block [1]: Conv2D -> BN -> ReLU -> Conv2D -> BN -> Add _&gt; ReLU</li>\n<li>EDSR-style block: Conv2D -> ReLU -> Conv2D -> Add -> ReLU</li>\n</ul>\n\n<h6 id=\"references\">References</h6>\n\n<p>[1] Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385</p>\n", "bases": "ConvBlock"}, {"fullname": "dl4ds.models.blocks.ResidualBlock.__init__", "modulename": "dl4ds.models.blocks", "qualname": "ResidualBlock.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    filters,\n    strides=1,\n    ks_cl1=(3, 3),\n    ks_cl2=(3, 3),\n    activation='relu',\n    normalization=None,\n    attention=False,\n    dropout_rate=0,\n    dropout_variant=None,\n    use_1x1conv=False,\n    name=None,\n    **conv_kwargs\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.ResidualBlock.call", "modulename": "dl4ds.models.blocks", "qualname": "ResidualBlock.call", "type": "function", "doc": "<p>Model's forward pass.</p>\n", "signature": "(self, X)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.DenseBlock", "modulename": "dl4ds.models.blocks", "qualname": "DenseBlock", "type": "class", "doc": "<p>Dense block.</p>\n\n<h6 id=\"references\">References</h6>\n\n<p>[1] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger\n    Densely Connected Convolutional Networks: \n    https://arxiv.org/abs/1608.06993</p>\n", "bases": "ConvBlock"}, {"fullname": "dl4ds.models.blocks.DenseBlock.__init__", "modulename": "dl4ds.models.blocks", "qualname": "DenseBlock.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    filters,\n    strides=1,\n    ks_cl1=(1, 1),\n    ks_cl2=(3, 3),\n    activation='relu',\n    normalization=None,\n    attention=False,\n    dropout_rate=0,\n    dropout_variant=None,\n    name=None,\n    **conv_kwargs\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.DenseBlock.call", "modulename": "dl4ds.models.blocks", "qualname": "DenseBlock.call", "type": "function", "doc": "<p>Model's forward pass.</p>\n", "signature": "(self, X)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.TransitionBlock", "modulename": "dl4ds.models.blocks", "qualname": "TransitionBlock", "type": "class", "doc": "<p>Transition layer to control the complexity of the model by using 1x1 \nconvolutions. Used in architectures, such as the Densenet.</p>\n\n<h6 id=\"references\">References</h6>\n\n<p>[1] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger\n    Densely Connected Convolutional Networks: \n    https://arxiv.org/abs/1608.06993</p>\n", "bases": "keras.engine.base_layer.Layer"}, {"fullname": "dl4ds.models.blocks.TransitionBlock.__init__", "modulename": "dl4ds.models.blocks", "qualname": "TransitionBlock.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    filters,\n    activation='relu',\n    normalization=None,\n    name=None,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.TransitionBlock.call", "modulename": "dl4ds.models.blocks", "qualname": "TransitionBlock.call", "type": "function", "doc": "<p>This is where the layer's logic lives.</p>\n\n<p>The <code>call()</code> method may not create state (except in its first invocation,\nwrapping the creation of variables or other resources in <code>tf.init_scope()</code>).\nIt is recommended to create state in <code>__init__()</code>, or the <code>build()</code> method\nthat is called automatically before <code>call()</code> executes the first time.</p>\n\n<p>Args:\n  inputs: Input tensor, or dict/list/tuple of input tensors.\n    The first positional <code>inputs</code> argument is subject to special rules:\n    - <code>inputs</code> must be explicitly passed. A layer cannot have zero\n      arguments, and <code>inputs</code> cannot be provided via the default value\n      of a keyword argument.\n    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.\n    - Keras mask metadata is only collected from <code>inputs</code>.\n    - Layers are built (<code>build(input_shape)</code> method)\n      using shape info from <code>inputs</code> only.\n    - <code>input_spec</code> compatibility is only checked against <code>inputs</code>.\n    - Mixed precision input casting is only applied to <code>inputs</code>.\n      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their\n      casting behavior in mixed precision should be handled manually.\n    - The SavedModel input specification is generated using <code>inputs</code> only.\n    - Integration with various ecosystem packages like TFMOT, TFLite,\n      TF.js, etc is only supported for <code>inputs</code> and not for tensors in\n      positional and keyword arguments.\n  <em>args: Additional positional arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n  *</em>kwargs: Additional keyword arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n    The following optional keyword arguments are reserved:\n    - <code>training</code>: Boolean scalar tensor of Python boolean indicating\n      whether the <code>call</code> is meant for training or inference.\n    - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a\n      <code>mask</code> argument, its default value will be set to the mask generated\n      for <code>inputs</code> by the previous layer (if <code>input</code> did come from a layer\n      that generated a corresponding mask, i.e. if it came from a Keras\n      layer with masking support).</p>\n\n<p>Returns:\n  A tensor or list/tuple of tensors.</p>\n", "signature": "(self, X)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.LocalizedConvBlock", "modulename": "dl4ds.models.blocks", "qualname": "LocalizedConvBlock", "type": "class", "doc": "<p>Localized convolutional block through a locally connected layer (1x1 kernel) \nwith biases.</p>\n", "bases": "keras.engine.base_layer.Layer"}, {"fullname": "dl4ds.models.blocks.LocalizedConvBlock.__init__", "modulename": "dl4ds.models.blocks", "qualname": "LocalizedConvBlock.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    filters=2,\n    activation=None,\n    use_bias=True,\n    name_sufix='',\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.LocalizedConvBlock.call", "modulename": "dl4ds.models.blocks", "qualname": "LocalizedConvBlock.call", "type": "function", "doc": "<p>This is where the layer's logic lives.</p>\n\n<p>The <code>call()</code> method may not create state (except in its first invocation,\nwrapping the creation of variables or other resources in <code>tf.init_scope()</code>).\nIt is recommended to create state in <code>__init__()</code>, or the <code>build()</code> method\nthat is called automatically before <code>call()</code> executes the first time.</p>\n\n<p>Args:\n  inputs: Input tensor, or dict/list/tuple of input tensors.\n    The first positional <code>inputs</code> argument is subject to special rules:\n    - <code>inputs</code> must be explicitly passed. A layer cannot have zero\n      arguments, and <code>inputs</code> cannot be provided via the default value\n      of a keyword argument.\n    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.\n    - Keras mask metadata is only collected from <code>inputs</code>.\n    - Layers are built (<code>build(input_shape)</code> method)\n      using shape info from <code>inputs</code> only.\n    - <code>input_spec</code> compatibility is only checked against <code>inputs</code>.\n    - Mixed precision input casting is only applied to <code>inputs</code>.\n      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their\n      casting behavior in mixed precision should be handled manually.\n    - The SavedModel input specification is generated using <code>inputs</code> only.\n    - Integration with various ecosystem packages like TFMOT, TFLite,\n      TF.js, etc is only supported for <code>inputs</code> and not for tensors in\n      positional and keyword arguments.\n  <em>args: Additional positional arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n  *</em>kwargs: Additional keyword arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n    The following optional keyword arguments are reserved:\n    - <code>training</code>: Boolean scalar tensor of Python boolean indicating\n      whether the <code>call</code> is meant for training or inference.\n    - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a\n      <code>mask</code> argument, its default value will be set to the mask generated\n      for <code>inputs</code> by the previous layer (if <code>input</code> did come from a layer\n      that generated a corresponding mask, i.e. if it came from a Keras\n      layer with masking support).</p>\n\n<p>Returns:\n  A tensor or list/tuple of tensors.</p>\n", "signature": "(self, X)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.LocalizedConvBlock.compute_output_shape", "modulename": "dl4ds.models.blocks", "qualname": "LocalizedConvBlock.compute_output_shape", "type": "function", "doc": "<p>Computes the output shape of the layer.</p>\n\n<p>This method will cause the layer's state to be built, if that has not\nhappened before. This requires that the layer will later be used with\ninputs that match the input shape provided here.</p>\n\n<p>Args:\n    input_shape: Shape tuple (tuple of integers)\n        or list of shape tuples (one per output tensor of the layer).\n        Shape tuples can include None for free dimensions,\n        instead of an integer.</p>\n\n<p>Returns:\n    An input shape tuple.</p>\n", "signature": "(self, input_shape)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.RecurrentConvBlock", "modulename": "dl4ds.models.blocks", "qualname": "RecurrentConvBlock", "type": "class", "doc": "<p>Recurrent convolutional block.</p>\n", "bases": "keras.engine.base_layer.Layer"}, {"fullname": "dl4ds.models.blocks.RecurrentConvBlock.__init__", "modulename": "dl4ds.models.blocks", "qualname": "RecurrentConvBlock.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    filters,\n    ks_cl1=(5, 5),\n    ks_cl2=(3, 3),\n    activation='relu',\n    normalization=None,\n    dropout_rate=0,\n    dropout_variant=None,\n    name_suffix='',\n    **conv_kwargs\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.RecurrentConvBlock.call", "modulename": "dl4ds.models.blocks", "qualname": "RecurrentConvBlock.call", "type": "function", "doc": "<p>Forward pass.</p>\n", "signature": "(self, X)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.SubpixelConvolutionBlock", "modulename": "dl4ds.models.blocks", "qualname": "SubpixelConvolutionBlock", "type": "class", "doc": "<p>Subpixel convolution (pixel shuffle) block.</p>\n\n<h6 id=\"references\">References</h6>\n\n<p>[1] Real-Time Single Image and Video Super-Resolution Using an Efficient \nSub-Pixel Convolutional Neural Network: https://arxiv.org/abs/1609.05158</p>\n", "bases": "keras.engine.base_layer.Layer"}, {"fullname": "dl4ds.models.blocks.SubpixelConvolutionBlock.__init__", "modulename": "dl4ds.models.blocks", "qualname": "SubpixelConvolutionBlock.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, scale, n_filters, name_suffix='', **kwargs)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.SubpixelConvolutionBlock.upsample_conv", "modulename": "dl4ds.models.blocks", "qualname": "SubpixelConvolutionBlock.upsample_conv", "type": "function", "doc": "<p>Sub-pixel convolution (pixel shuffle)</p>\n", "signature": "(self, x, factor)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.SubpixelConvolutionBlock.compute_output_shape", "modulename": "dl4ds.models.blocks", "qualname": "SubpixelConvolutionBlock.compute_output_shape", "type": "function", "doc": "<p>Computes the output shape of the layer.</p>\n\n<p>This method will cause the layer's state to be built, if that has not\nhappened before. This requires that the layer will later be used with\ninputs that match the input shape provided here.</p>\n\n<p>Args:\n    input_shape: Shape tuple (tuple of integers)\n        or list of shape tuples (one per output tensor of the layer).\n        Shape tuples can include None for free dimensions,\n        instead of an integer.</p>\n\n<p>Returns:\n    An input shape tuple.</p>\n", "signature": "(self, input_shape)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.SubpixelConvolutionBlock.call", "modulename": "dl4ds.models.blocks", "qualname": "SubpixelConvolutionBlock.call", "type": "function", "doc": "<p>Forward pass.</p>\n", "signature": "(self, x)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.ResizeConvolutionBlock", "modulename": "dl4ds.models.blocks", "qualname": "ResizeConvolutionBlock", "type": "class", "doc": "<p>Upsampling via bilinear interpolation followed by a 2D Convolution. </p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>interpolation</strong> (str):\nThe interpolation method. Defaults to \"bilinear\". Supports \"bilinear\", \n\"nearest\", \"bicubic\", \"area\", \"lanczos3\", \"lanczos5\", \"gaussian\", \n\"mitchellcubic\".</li>\n</ul>\n\n<h6 id=\"references\">References</h6>\n\n<p>[1] Deconvolution and Checkerboard Artifacts: \nhttps://distill.pub/2016/deconv-checkerboard/</p>\n", "bases": "keras.engine.base_layer.Layer"}, {"fullname": "dl4ds.models.blocks.ResizeConvolutionBlock.__init__", "modulename": "dl4ds.models.blocks", "qualname": "ResizeConvolutionBlock.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    scale,\n    n_filters,\n    interpolation='bilinear',\n    name_suffix='',\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.ResizeConvolutionBlock.compute_output_shape", "modulename": "dl4ds.models.blocks", "qualname": "ResizeConvolutionBlock.compute_output_shape", "type": "function", "doc": "<p>Computes the output shape of the layer.</p>\n\n<p>This method will cause the layer's state to be built, if that has not\nhappened before. This requires that the layer will later be used with\ninputs that match the input shape provided here.</p>\n\n<p>Args:\n    input_shape: Shape tuple (tuple of integers)\n        or list of shape tuples (one per output tensor of the layer).\n        Shape tuples can include None for free dimensions,\n        instead of an integer.</p>\n\n<p>Returns:\n    An input shape tuple.</p>\n", "signature": "(self, input_shape)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.ResizeConvolutionBlock.call", "modulename": "dl4ds.models.blocks", "qualname": "ResizeConvolutionBlock.call", "type": "function", "doc": "<p>This is where the layer's logic lives.</p>\n\n<p>The <code>call()</code> method may not create state (except in its first invocation,\nwrapping the creation of variables or other resources in <code>tf.init_scope()</code>).\nIt is recommended to create state in <code>__init__()</code>, or the <code>build()</code> method\nthat is called automatically before <code>call()</code> executes the first time.</p>\n\n<p>Args:\n  inputs: Input tensor, or dict/list/tuple of input tensors.\n    The first positional <code>inputs</code> argument is subject to special rules:\n    - <code>inputs</code> must be explicitly passed. A layer cannot have zero\n      arguments, and <code>inputs</code> cannot be provided via the default value\n      of a keyword argument.\n    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.\n    - Keras mask metadata is only collected from <code>inputs</code>.\n    - Layers are built (<code>build(input_shape)</code> method)\n      using shape info from <code>inputs</code> only.\n    - <code>input_spec</code> compatibility is only checked against <code>inputs</code>.\n    - Mixed precision input casting is only applied to <code>inputs</code>.\n      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their\n      casting behavior in mixed precision should be handled manually.\n    - The SavedModel input specification is generated using <code>inputs</code> only.\n    - Integration with various ecosystem packages like TFMOT, TFLite,\n      TF.js, etc is only supported for <code>inputs</code> and not for tensors in\n      positional and keyword arguments.\n  <em>args: Additional positional arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n  *</em>kwargs: Additional keyword arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n    The following optional keyword arguments are reserved:\n    - <code>training</code>: Boolean scalar tensor of Python boolean indicating\n      whether the <code>call</code> is meant for training or inference.\n    - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a\n      <code>mask</code> argument, its default value will be set to the mask generated\n      for <code>inputs</code> by the previous layer (if <code>input</code> did come from a layer\n      that generated a corresponding mask, i.e. if it came from a Keras\n      layer with masking support).</p>\n\n<p>Returns:\n  A tensor or list/tuple of tensors.</p>\n", "signature": "(self, x)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.DeconvolutionBlock", "modulename": "dl4ds.models.blocks", "qualname": "DeconvolutionBlock", "type": "class", "doc": "<p>Deconvolution or transposed convolution block.</p>\n\n<h6 id=\"references\">References</h6>\n\n<p>[1] FSRCNN - Accelerating the Super-Resolution Convolutional Neural Network: \nhttps://arxiv.org/abs/1608.00367</p>\n", "bases": "keras.engine.base_layer.Layer"}, {"fullname": "dl4ds.models.blocks.DeconvolutionBlock.__init__", "modulename": "dl4ds.models.blocks", "qualname": "DeconvolutionBlock.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, scale, n_filters, output_activation=None, name_suffix='')", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.DeconvolutionBlock.compute_output_shape", "modulename": "dl4ds.models.blocks", "qualname": "DeconvolutionBlock.compute_output_shape", "type": "function", "doc": "<p>Computes the output shape of the layer.</p>\n\n<p>This method will cause the layer's state to be built, if that has not\nhappened before. This requires that the layer will later be used with\ninputs that match the input shape provided here.</p>\n\n<p>Args:\n    input_shape: Shape tuple (tuple of integers)\n        or list of shape tuples (one per output tensor of the layer).\n        Shape tuples can include None for free dimensions,\n        instead of an integer.</p>\n\n<p>Returns:\n    An input shape tuple.</p>\n", "signature": "(self, input_shape)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.DeconvolutionBlock.call", "modulename": "dl4ds.models.blocks", "qualname": "DeconvolutionBlock.call", "type": "function", "doc": "<p></p>\n", "signature": "(self, x)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.ChannelAttention2D", "modulename": "dl4ds.models.blocks", "qualname": "ChannelAttention2D", "type": "class", "doc": "<p>Channel Attention for CNNs. Inputs need to be Conv2D feature maps.\nThe layer implements the following:\n    1. Average Pooling to create <code>[1,1,C]</code> vectors\n    2. Conv2D with k=1 for fully connected features and relu ac\n    3. Sigmoid activation to create attention maps</p>\n\n<p>Adapted from visual_attention_tf: \nhttps://github.com/vinayak19th/Visual_attention_tf/blob/main/src/visual_attention/channel_attention.py</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><p><strong>nf [int]</strong> (number of filters or channels):</p></li>\n<li><p><strong>r[int]</strong> (Reduction factor):</p></li>\n</ul>\n\n<h6 id=\"input\">Input</h6>\n\n<p>Feature maps : Conv2D feature maps of the shape <code>[batch,W,H,C]</code>.</p>\n\n<h6 id=\"output\">Output</h6>\n\n<p>Attention activated Conv2D features of shape <code>[batch,W,H,C]</code>.</p>\n\n<h6 id=\"reference\">Reference</h6>\n\n<p>CBAM: Convolutional Block Attention Module (Sanghyun Woo et al 2018): \nhttps://arxiv.org/abs/1807.06521</p>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>Here is a code example for using <code>ChannelAttention2D</code> in a CNN:</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">inp</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1920</span><span class=\"p\">,</span><span class=\"mi\">1080</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">))</span>\n<span class=\"n\">cnn_layer</span> <span class=\"o\">=</span> <span class=\"n\">Conv2D</span><span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,,</span><span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">&#39;relu&#39;</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;same&#39;</span><span class=\"p\">)(</span><span class=\"n\">inp</span><span class=\"p\">)</span>\n<span class=\"c1\"># Using the .shape[-1] to simplify network modifications. Can directly input number of channels as well</span>\n<span class=\"n\">attention_cnn</span> <span class=\"o\">=</span> <span class=\"n\">ChannelAttention2D</span><span class=\"p\">(</span><span class=\"n\">cnn_layer</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">cnn_layer</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])(</span><span class=\"n\">cnn_layer</span><span class=\"p\">)</span>\n<span class=\"c1\">#ADD DNN layers .....</span>\n</code></pre></div>\n", "bases": "keras.engine.base_layer.Layer"}, {"fullname": "dl4ds.models.blocks.ChannelAttention2D.__init__", "modulename": "dl4ds.models.blocks", "qualname": "ChannelAttention2D.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, nf, r=4, **kwargs)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.ChannelAttention2D.call", "modulename": "dl4ds.models.blocks", "qualname": "ChannelAttention2D.call", "type": "function", "doc": "<p></p>\n", "signature": "(self, x)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.ChannelAttention2D.get_config", "modulename": "dl4ds.models.blocks", "qualname": "ChannelAttention2D.get_config", "type": "function", "doc": "<p>Returns the config of the layer.</p>\n\n<p>A layer config is a Python dictionary (serializable)\ncontaining the configuration of a layer.\nThe same layer can be reinstantiated later\n(without its trained weights) from this configuration.</p>\n\n<p>The config of a layer does not include connectivity\ninformation, nor the layer class name. These are handled\nby <code>Network</code> (one layer of abstraction above).</p>\n\n<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of dict\nevery time it is called. The callers should make a copy of the returned dict\nif they want to modify it.</p>\n\n<p>Returns:\n    Python dictionary.</p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.EncoderBlock", "modulename": "dl4ds.models.blocks", "qualname": "EncoderBlock", "type": "class", "doc": "<p>Encoder block for a decoder-encoder architecture, such as the UNET.</p>\n", "bases": "keras.engine.base_layer.Layer"}, {"fullname": "dl4ds.models.blocks.EncoderBlock.__init__", "modulename": "dl4ds.models.blocks", "qualname": "EncoderBlock.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    n_filters,\n    activation=None,\n    dropout_rate=0,\n    dropout_variant=None,\n    normalization=None,\n    attention=False,\n    name_suffix=''\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.EncoderBlock.call", "modulename": "dl4ds.models.blocks", "qualname": "EncoderBlock.call", "type": "function", "doc": "<p>This is where the layer's logic lives.</p>\n\n<p>The <code>call()</code> method may not create state (except in its first invocation,\nwrapping the creation of variables or other resources in <code>tf.init_scope()</code>).\nIt is recommended to create state in <code>__init__()</code>, or the <code>build()</code> method\nthat is called automatically before <code>call()</code> executes the first time.</p>\n\n<p>Args:\n  inputs: Input tensor, or dict/list/tuple of input tensors.\n    The first positional <code>inputs</code> argument is subject to special rules:\n    - <code>inputs</code> must be explicitly passed. A layer cannot have zero\n      arguments, and <code>inputs</code> cannot be provided via the default value\n      of a keyword argument.\n    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.\n    - Keras mask metadata is only collected from <code>inputs</code>.\n    - Layers are built (<code>build(input_shape)</code> method)\n      using shape info from <code>inputs</code> only.\n    - <code>input_spec</code> compatibility is only checked against <code>inputs</code>.\n    - Mixed precision input casting is only applied to <code>inputs</code>.\n      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their\n      casting behavior in mixed precision should be handled manually.\n    - The SavedModel input specification is generated using <code>inputs</code> only.\n    - Integration with various ecosystem packages like TFMOT, TFLite,\n      TF.js, etc is only supported for <code>inputs</code> and not for tensors in\n      positional and keyword arguments.\n  <em>args: Additional positional arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n  *</em>kwargs: Additional keyword arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n    The following optional keyword arguments are reserved:\n    - <code>training</code>: Boolean scalar tensor of Python boolean indicating\n      whether the <code>call</code> is meant for training or inference.\n    - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a\n      <code>mask</code> argument, its default value will be set to the mask generated\n      for <code>inputs</code> by the previous layer (if <code>input</code> did come from a layer\n      that generated a corresponding mask, i.e. if it came from a Keras\n      layer with masking support).</p>\n\n<p>Returns:\n  A tensor or list/tuple of tensors.</p>\n", "signature": "(self, X)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.PadConcat", "modulename": "dl4ds.models.blocks", "qualname": "PadConcat", "type": "class", "doc": "<p>Concatenate layer that takes two tensors, if needed it pads to match \nheight and width.</p>\n", "bases": "keras.engine.base_layer.Layer"}, {"fullname": "dl4ds.models.blocks.PadConcat.__init__", "modulename": "dl4ds.models.blocks", "qualname": "PadConcat.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, debug=False, name_suffix='')", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.PadConcat.call", "modulename": "dl4ds.models.blocks", "qualname": "PadConcat.call", "type": "function", "doc": "<p>This is where the layer's logic lives.</p>\n\n<p>The <code>call()</code> method may not create state (except in its first invocation,\nwrapping the creation of variables or other resources in <code>tf.init_scope()</code>).\nIt is recommended to create state in <code>__init__()</code>, or the <code>build()</code> method\nthat is called automatically before <code>call()</code> executes the first time.</p>\n\n<p>Args:\n  inputs: Input tensor, or dict/list/tuple of input tensors.\n    The first positional <code>inputs</code> argument is subject to special rules:\n    - <code>inputs</code> must be explicitly passed. A layer cannot have zero\n      arguments, and <code>inputs</code> cannot be provided via the default value\n      of a keyword argument.\n    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.\n    - Keras mask metadata is only collected from <code>inputs</code>.\n    - Layers are built (<code>build(input_shape)</code> method)\n      using shape info from <code>inputs</code> only.\n    - <code>input_spec</code> compatibility is only checked against <code>inputs</code>.\n    - Mixed precision input casting is only applied to <code>inputs</code>.\n      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their\n      casting behavior in mixed precision should be handled manually.\n    - The SavedModel input specification is generated using <code>inputs</code> only.\n    - Integration with various ecosystem packages like TFMOT, TFLite,\n      TF.js, etc is only supported for <code>inputs</code> and not for tensors in\n      positional and keyword arguments.\n  <em>args: Additional positional arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n  *</em>kwargs: Additional keyword arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n    The following optional keyword arguments are reserved:\n    - <code>training</code>: Boolean scalar tensor of Python boolean indicating\n      whether the <code>call</code> is meant for training or inference.\n    - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a\n      <code>mask</code> argument, its default value will be set to the mask generated\n      for <code>inputs</code> by the previous layer (if <code>input</code> did come from a layer\n      that generated a corresponding mask, i.e. if it came from a Keras\n      layer with masking support).</p>\n\n<p>Returns:\n  A tensor or list/tuple of tensors.</p>\n", "signature": "(self, X)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.MCDropout", "modulename": "dl4ds.models.blocks", "qualname": "MCDropout", "type": "class", "doc": "<p>Applies Dropout to the input.</p>\n\n<p>The Dropout layer randomly sets input units to 0 with a frequency of <code>rate</code>\nat each step during training time, which helps prevent overfitting.\nInputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over\nall inputs is unchanged.</p>\n\n<p>Note that the Dropout layer only applies when <code>training</code> is set to True\nsuch that no values are dropped during inference. When using <code>model.fit</code>,\n<code>training</code> will be appropriately set to True automatically, and in other\ncontexts, you can set the kwarg explicitly to True when calling the layer.</p>\n\n<p>(This is in contrast to setting <code>trainable=False</code> for a Dropout layer.\n<code>trainable</code> does not affect the layer's behavior, as Dropout does\nnot have any variables/weights that can be frozen during training.)</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">set_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">layer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"mf\">.2</span><span class=\"p\">,</span> <span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,))</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"go\">[[0. 1.]</span>\n<span class=\"go\"> [2. 3.]</span>\n<span class=\"go\"> [4. 5.]</span>\n<span class=\"go\"> [6. 7.]</span>\n<span class=\"go\"> [8. 9.]]</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">layer</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">training</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">outputs</span><span class=\"p\">)</span>\n<span class=\"go\">tf.Tensor(</span>\n<span class=\"go\">[[ 0.    1.25]</span>\n<span class=\"go\"> [ 2.5   3.75]</span>\n<span class=\"go\"> [ 5.    6.25]</span>\n<span class=\"go\"> [ 7.5   8.75]</span>\n<span class=\"go\"> [10.    0.  ]], shape=(5, 2), dtype=float32)</span>\n</code></pre></div>\n\n<p>Args:\n  rate: Float between 0 and 1. Fraction of the input units to drop.\n  noise_shape: 1D integer tensor representing the shape of the\n    binary dropout mask that will be multiplied with the input.\n    For instance, if your inputs have shape\n    <code>(batch_size, timesteps, features)</code> and\n    you want the dropout mask to be the same for all timesteps,\n    you can use <code>noise_shape=(batch_size, 1, features)</code>.\n  seed: A Python integer to use as random seed.</p>\n\n<p>Call arguments:\n  inputs: Input tensor (of any rank).\n  training: Python boolean indicating whether the layer should behave in\n    training mode (adding dropout) or in inference mode (doing nothing).</p>\n", "bases": "keras.layers.core.dropout.Dropout"}, {"fullname": "dl4ds.models.blocks.MCDropout.call", "modulename": "dl4ds.models.blocks", "qualname": "MCDropout.call", "type": "function", "doc": "<p>This is where the layer's logic lives.</p>\n\n<p>The <code>call()</code> method may not create state (except in its first invocation,\nwrapping the creation of variables or other resources in <code>tf.init_scope()</code>).\nIt is recommended to create state in <code>__init__()</code>, or the <code>build()</code> method\nthat is called automatically before <code>call()</code> executes the first time.</p>\n\n<p>Args:\n  inputs: Input tensor, or dict/list/tuple of input tensors.\n    The first positional <code>inputs</code> argument is subject to special rules:\n    - <code>inputs</code> must be explicitly passed. A layer cannot have zero\n      arguments, and <code>inputs</code> cannot be provided via the default value\n      of a keyword argument.\n    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.\n    - Keras mask metadata is only collected from <code>inputs</code>.\n    - Layers are built (<code>build(input_shape)</code> method)\n      using shape info from <code>inputs</code> only.\n    - <code>input_spec</code> compatibility is only checked against <code>inputs</code>.\n    - Mixed precision input casting is only applied to <code>inputs</code>.\n      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their\n      casting behavior in mixed precision should be handled manually.\n    - The SavedModel input specification is generated using <code>inputs</code> only.\n    - Integration with various ecosystem packages like TFMOT, TFLite,\n      TF.js, etc is only supported for <code>inputs</code> and not for tensors in\n      positional and keyword arguments.\n  <em>args: Additional positional arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n  *</em>kwargs: Additional keyword arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n    The following optional keyword arguments are reserved:\n    - <code>training</code>: Boolean scalar tensor of Python boolean indicating\n      whether the <code>call</code> is meant for training or inference.\n    - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a\n      <code>mask</code> argument, its default value will be set to the mask generated\n      for <code>inputs</code> by the previous layer (if <code>input</code> did come from a layer\n      that generated a corresponding mask, i.e. if it came from a Keras\n      layer with masking support).</p>\n\n<p>Returns:\n  A tensor or list/tuple of tensors.</p>\n", "signature": "(self, inputs)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.MCGaussianDropout", "modulename": "dl4ds.models.blocks", "qualname": "MCGaussianDropout", "type": "class", "doc": "<p>Apply multiplicative 1-centered Gaussian noise.</p>\n\n<p>As it is a regularization layer, it is only active at training time.</p>\n\n<p>Args:\n  rate: Float, drop probability (as with <code>Dropout</code>).\n    The multiplicative noise will have\n    standard deviation <code>sqrt(rate / (1 - rate))</code>.\n  seed: Integer, optional random seed to enable deterministic behavior.</p>\n\n<p>Call arguments:\n  inputs: Input tensor (of any rank).\n  training: Python boolean indicating whether the layer should behave in\n    training mode (adding dropout) or in inference mode (doing nothing).</p>\n\n<p>Input shape:\n  Arbitrary. Use the keyword argument <code>input_shape</code>\n  (tuple of integers, does not include the samples axis)\n  when using this layer as the first layer in a model.</p>\n\n<p>Output shape:\n  Same shape as input.</p>\n", "bases": "keras.layers.noise.GaussianDropout"}, {"fullname": "dl4ds.models.blocks.MCGaussianDropout.call", "modulename": "dl4ds.models.blocks", "qualname": "MCGaussianDropout.call", "type": "function", "doc": "<p>This is where the layer's logic lives.</p>\n\n<p>The <code>call()</code> method may not create state (except in its first invocation,\nwrapping the creation of variables or other resources in <code>tf.init_scope()</code>).\nIt is recommended to create state in <code>__init__()</code>, or the <code>build()</code> method\nthat is called automatically before <code>call()</code> executes the first time.</p>\n\n<p>Args:\n  inputs: Input tensor, or dict/list/tuple of input tensors.\n    The first positional <code>inputs</code> argument is subject to special rules:\n    - <code>inputs</code> must be explicitly passed. A layer cannot have zero\n      arguments, and <code>inputs</code> cannot be provided via the default value\n      of a keyword argument.\n    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.\n    - Keras mask metadata is only collected from <code>inputs</code>.\n    - Layers are built (<code>build(input_shape)</code> method)\n      using shape info from <code>inputs</code> only.\n    - <code>input_spec</code> compatibility is only checked against <code>inputs</code>.\n    - Mixed precision input casting is only applied to <code>inputs</code>.\n      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their\n      casting behavior in mixed precision should be handled manually.\n    - The SavedModel input specification is generated using <code>inputs</code> only.\n    - Integration with various ecosystem packages like TFMOT, TFLite,\n      TF.js, etc is only supported for <code>inputs</code> and not for tensors in\n      positional and keyword arguments.\n  <em>args: Additional positional arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n  *</em>kwargs: Additional keyword arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n    The following optional keyword arguments are reserved:\n    - <code>training</code>: Boolean scalar tensor of Python boolean indicating\n      whether the <code>call</code> is meant for training or inference.\n    - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a\n      <code>mask</code> argument, its default value will be set to the mask generated\n      for <code>inputs</code> by the previous layer (if <code>input</code> did come from a layer\n      that generated a corresponding mask, i.e. if it came from a Keras\n      layer with masking support).</p>\n\n<p>Returns:\n  A tensor or list/tuple of tensors.</p>\n", "signature": "(self, inputs)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.MCSpatialDropout2D", "modulename": "dl4ds.models.blocks", "qualname": "MCSpatialDropout2D", "type": "class", "doc": "<p>Spatial 2D version of Dropout.</p>\n\n<p>This version performs the same function as Dropout, however, it drops\nentire 2D feature maps instead of individual elements. If adjacent pixels\nwithin feature maps are strongly correlated (as is normally the case in\nearly convolution layers) then regular dropout will not regularize the\nactivations and will otherwise just result in an effective learning rate\ndecrease. In this case, SpatialDropout2D will help promote independence\nbetween feature maps and should be used instead.</p>\n\n<p>Args:\n  rate: Float between 0 and 1. Fraction of the input units to drop.\n  data_format: 'channels_first' or 'channels_last'. In 'channels_first' mode,\n    the channels dimension (the depth) is at index 1, in 'channels_last' mode\n    is it at index 3. It defaults to the <code>image_data_format</code> value found in\n    your Keras config file at <code>~/.keras/keras.json</code>. If you never set it, then\n    it will be \"channels_last\".\nCall arguments:\n  inputs: A 4D tensor.\n  training: Python boolean indicating whether the layer should behave in\n    training mode (adding dropout) or in inference mode (doing nothing).\nInput shape:\n  4D tensor with shape: <code>(samples, channels, rows, cols)</code> if\n    data_format='channels_first'\n  or 4D tensor with shape: <code>(samples, rows, cols, channels)</code> if\n    data_format='channels_last'.\nOutput shape: Same as input.\nReferences: - <a href=\"https://arxiv.org/abs/1411.4280\">Efficient Object Localization Using Convolutional\n    Networks</a></p>\n", "bases": "keras.layers.core.spatial_dropout.SpatialDropout2D"}, {"fullname": "dl4ds.models.blocks.MCSpatialDropout2D.call", "modulename": "dl4ds.models.blocks", "qualname": "MCSpatialDropout2D.call", "type": "function", "doc": "<p>This is where the layer's logic lives.</p>\n\n<p>The <code>call()</code> method may not create state (except in its first invocation,\nwrapping the creation of variables or other resources in <code>tf.init_scope()</code>).\nIt is recommended to create state in <code>__init__()</code>, or the <code>build()</code> method\nthat is called automatically before <code>call()</code> executes the first time.</p>\n\n<p>Args:\n  inputs: Input tensor, or dict/list/tuple of input tensors.\n    The first positional <code>inputs</code> argument is subject to special rules:\n    - <code>inputs</code> must be explicitly passed. A layer cannot have zero\n      arguments, and <code>inputs</code> cannot be provided via the default value\n      of a keyword argument.\n    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.\n    - Keras mask metadata is only collected from <code>inputs</code>.\n    - Layers are built (<code>build(input_shape)</code> method)\n      using shape info from <code>inputs</code> only.\n    - <code>input_spec</code> compatibility is only checked against <code>inputs</code>.\n    - Mixed precision input casting is only applied to <code>inputs</code>.\n      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their\n      casting behavior in mixed precision should be handled manually.\n    - The SavedModel input specification is generated using <code>inputs</code> only.\n    - Integration with various ecosystem packages like TFMOT, TFLite,\n      TF.js, etc is only supported for <code>inputs</code> and not for tensors in\n      positional and keyword arguments.\n  <em>args: Additional positional arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n  *</em>kwargs: Additional keyword arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n    The following optional keyword arguments are reserved:\n    - <code>training</code>: Boolean scalar tensor of Python boolean indicating\n      whether the <code>call</code> is meant for training or inference.\n    - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a\n      <code>mask</code> argument, its default value will be set to the mask generated\n      for <code>inputs</code> by the previous layer (if <code>input</code> did come from a layer\n      that generated a corresponding mask, i.e. if it came from a Keras\n      layer with masking support).</p>\n\n<p>Returns:\n  A tensor or list/tuple of tensors.</p>\n", "signature": "(self, inputs)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.MCSpatialDropout3D", "modulename": "dl4ds.models.blocks", "qualname": "MCSpatialDropout3D", "type": "class", "doc": "<p>Spatial 3D version of Dropout.</p>\n\n<p>This version performs the same function as Dropout, however, it drops\nentire 3D feature maps instead of individual elements. If adjacent voxels\nwithin feature maps are strongly correlated (as is normally the case in\nearly convolution layers) then regular dropout will not regularize the\nactivations and will otherwise just result in an effective learning rate\ndecrease. In this case, SpatialDropout3D will help promote independence\nbetween feature maps and should be used instead.</p>\n\n<p>Args:\n  rate: Float between 0 and 1. Fraction of the input units to drop.\n  data_format: 'channels_first' or 'channels_last'. In 'channels_first' mode,\n    the channels dimension (the depth) is at index 1, in 'channels_last' mode\n    is it at index 4. It defaults to the <code>image_data_format</code> value found in\n    your Keras config file at <code>~/.keras/keras.json</code>. If you never set it, then\n    it will be \"channels_last\".\nCall arguments:\n  inputs: A 5D tensor.\n  training: Python boolean indicating whether the layer should behave in\n    training mode (adding dropout) or in inference mode (doing nothing).\nInput shape:\n  5D tensor with shape: <code>(samples, channels, dim1, dim2, dim3)</code> if\n    data_format='channels_first'\n  or 5D tensor with shape: <code>(samples, dim1, dim2, dim3, channels)</code> if\n    data_format='channels_last'.\nOutput shape: Same as input.\nReferences: - <a href=\"https://arxiv.org/abs/1411.4280\">Efficient Object Localization Using Convolutional\n    Networks</a></p>\n", "bases": "keras.layers.core.spatial_dropout.SpatialDropout3D"}, {"fullname": "dl4ds.models.blocks.MCSpatialDropout3D.call", "modulename": "dl4ds.models.blocks", "qualname": "MCSpatialDropout3D.call", "type": "function", "doc": "<p>This is where the layer's logic lives.</p>\n\n<p>The <code>call()</code> method may not create state (except in its first invocation,\nwrapping the creation of variables or other resources in <code>tf.init_scope()</code>).\nIt is recommended to create state in <code>__init__()</code>, or the <code>build()</code> method\nthat is called automatically before <code>call()</code> executes the first time.</p>\n\n<p>Args:\n  inputs: Input tensor, or dict/list/tuple of input tensors.\n    The first positional <code>inputs</code> argument is subject to special rules:\n    - <code>inputs</code> must be explicitly passed. A layer cannot have zero\n      arguments, and <code>inputs</code> cannot be provided via the default value\n      of a keyword argument.\n    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.\n    - Keras mask metadata is only collected from <code>inputs</code>.\n    - Layers are built (<code>build(input_shape)</code> method)\n      using shape info from <code>inputs</code> only.\n    - <code>input_spec</code> compatibility is only checked against <code>inputs</code>.\n    - Mixed precision input casting is only applied to <code>inputs</code>.\n      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their\n      casting behavior in mixed precision should be handled manually.\n    - The SavedModel input specification is generated using <code>inputs</code> only.\n    - Integration with various ecosystem packages like TFMOT, TFLite,\n      TF.js, etc is only supported for <code>inputs</code> and not for tensors in\n      positional and keyword arguments.\n  <em>args: Additional positional arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n  *</em>kwargs: Additional keyword arguments. May contain tensors, although\n    this is not recommended, for the reasons above.\n    The following optional keyword arguments are reserved:\n    - <code>training</code>: Boolean scalar tensor of Python boolean indicating\n      whether the <code>call</code> is meant for training or inference.\n    - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a\n      <code>mask</code> argument, its default value will be set to the mask generated\n      for <code>inputs</code> by the previous layer (if <code>input</code> did come from a layer\n      that generated a corresponding mask, i.e. if it came from a Keras\n      layer with masking support).</p>\n\n<p>Returns:\n  A tensor or list/tuple of tensors.</p>\n", "signature": "(self, inputs)", "funcdef": "def"}, {"fullname": "dl4ds.models.blocks.get_dropout_layer", "modulename": "dl4ds.models.blocks", "qualname": "get_dropout_layer", "type": "function", "doc": "<p>Choose an return a dropout layer depending on the input arguments. If\n<code>dropout_rate=0</code> then an identity layer is returned (the input tensor \nis returned without any modification).</p>\n", "signature": "(dropout_rate, dropout_variant, dim=2)", "funcdef": "def"}, {"fullname": "dl4ds.models.discriminator", "modulename": "dl4ds.models.discriminator", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.models.discriminator.residual_discriminator", "modulename": "dl4ds.models.discriminator", "qualname": "residual_discriminator", "type": "function", "doc": "<p></p>\n", "signature": "(\n    n_channels,\n    upsampling,\n    is_spatiotemporal,\n    scale,\n    lr_size,\n    n_filters=8,\n    n_res_blocks=4,\n    normalization=None,\n    activation='relu',\n    attention=False\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.sp_postups", "modulename": "dl4ds.models.sp_postups", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.models.sp_postups.net_postupsampling", "modulename": "dl4ds.models.sp_postups", "qualname": "net_postupsampling", "type": "function", "doc": "<p>Deep neural network with different backbone architectures (according to the\n<code>backbone_block</code>) and post-upsampling methods (according to \n<code>upsampling</code>).</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>backbone_block</strong> (str):\nBackbone type. One of dl4ds.BACKBONE_BLOCKS. WARNING: this parameter is\nnot supposed to be set by the user. It's set internallly through \ndl4ds.Trainers.</li>\n<li><strong>upsampling</strong> (str):\nUpsampling method. One of dl4ds.UPSAMPLING_METHODS. WARNING: this \nparameter is not supposed to be set by the user. It's set internallly \nthrough dl4ds.Trainers.</li>\n<li><strong>scale</strong> (int):\nScaling factor, LR wrt HR grid. WARNING: this parameter is not supposed \nto be set by the user. It's set internallly through dl4ds.Trainers.</li>\n<li><strong>n_channels</strong> (int):\nNumber of channels/variables in each sample. WARNING: this parameter is\nnot supposed to be set by the user. It's set internallly through\ndl4ds.Trainers.</li>\n<li><strong>n_aux_channels</strong> (int):\nNumber of auxiliary channels. WARNING: this parameter is not supposed to \nbe set by the user. It's set internallly through dl4ds.Trainers.</li>\n<li><strong>lr_size</strong> (tuple):\nHeight and width of the LR grid. WARNING: this parameter is not supposed \nto be set by the user. It's set internallly through dl4ds.Trainers.</li>\n<li><strong>n_filters</strong> (int, optional):\nNumber of convolutional filters in the first convolutional block. \n<code>n_filters</code> sets the dimensionality of the output space of the first \nconvolutional block (i.e. the number of output filters in the \nconvolution). The number of filters grows in subsequent convolutional \nblocks.</li>\n<li><strong>n_blocks</strong> (int, optional):\nNumber of convolutional blocks (ConvBlock, ResidualBlock, DenseBlock or\nConvNextBlock). Sets the depth of the network.</li>\n<li><strong>normalization</strong> (str or None, optional):\nNormalization method in the residual or dense block. Can be either 'bn'\nfor BatchNormalization or 'ln' for LayerNormalization. If None, then no\nnormalization is performed (eg., for the 'resnet' backbone this results \nin the EDSR-style residual block).</li>\n<li><strong>dropout_rate</strong> (float, optional):\nFloat between 0 and 1. Fraction of the input units to drop. If 0 then no\ndropout is applied.</li>\n<li><strong>dropout_variant</strong> (str or None, optional):\nType of dropout. Defined in dl4ds.DROPOUT_VARIANTS variable.</li>\n<li><strong>attention</strong> (bool, optional):\nIf True, dl4ds.ChannelAttention2D is used in convolutional blocks.</li>\n<li><strong>activation</strong> (str, optional):\nActivation function to use, as supported by tf.keras. E.g., 'relu' or \n'gelu'.</li>\n<li><strong>output_activation</strong> (str, optional):\nActivation function to use in the last ConvBlock. Useful to constraint \nthe values distribution of the output grid.</li>\n<li><strong>rc_interpolation</strong> (str, optional):\nInterpolation used in the ResizeConvolutionBlock. Supported methods: \n\"bilinear\", \"nearest\", \"bicubic\", \"area\", \"lanczos3\", \"lanczos5\", \n\"gaussian\", \"mitchellcubic\".</li>\n<li><strong>localcon_layer</strong> (bool, optional):\nIf True, the LocalizedConvBlock is activated in the output module.</li>\n</ul>\n", "signature": "(\n    backbone_block,\n    upsampling,\n    scale,\n    n_channels,\n    n_aux_channels,\n    lr_size,\n    n_channels_out=1,\n    n_filters=8,\n    n_blocks=6,\n    normalization=None,\n    dropout_rate=0,\n    dropout_variant=None,\n    attention=False,\n    activation='relu',\n    output_activation=None,\n    rc_interpolation='bilinear',\n    localcon_layer=False\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.sp_preups", "modulename": "dl4ds.models.sp_preups", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.models.sp_preups.net_pin", "modulename": "dl4ds.models.sp_preups", "qualname": "net_pin", "type": "function", "doc": "<p>Deep neural network with different backbone architectures (according to the\n<code>backbone_block</code>) and pre-upsampling via interpolation (the samples are \nexpected to be interpolated to the HR grid).</p>\n\n<p>The interpolation method depends on the <code>interpolation</code> argument used in\nthe training procedure (which is passed to the DataGenerator).</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>backbone_block</strong> (str):\nBackbone type. One of dl4ds.BACKBONE_BLOCKS. WARNING: this parameter is\nnot supposed to be set by the user. It's set internallly through\ndl4ds.Trainers.</li>\n<li><strong>n_channels</strong> (int):\nNumber of channels/variables in each sample. WARNING: this parameter is\nnot supposed to be set by the user. It's set internallly through\ndl4ds.Trainers.</li>\n<li><strong>n_aux_channels</strong> (int):\nNumber of auxiliary channels. WARNING: this parameter is not supposed to \nbe set by the user. It's set internallly through dl4ds.Trainers.</li>\n<li><strong>hr_size</strong> (tuple):\nHeight and width of the HR grid. WARNING: this parameter is not supposed \nto be set by the user. It's set internallly through dl4ds.Trainers.</li>\n<li><strong>n_filters</strong> (int, optional):\nNumber of convolutional filters in the first convolutional block. \n<code>n_filters</code> sets the dimensionality of the output space of the first \nconvolutional block (i.e. the number of output filters in the \nconvolution). The number of filters grows in subsequent convolutional \nblocks.</li>\n<li><strong>n_blocks</strong> (int, optional):\nNumber of convolutional blocks (ConvBlock, ResidualBlock, DenseBlock or\nConvNextBlock). Sets the depth of the network.</li>\n<li><strong>normalization</strong> (str or None, optional):\nNormalization method in the residual or dense block. Can be either 'bn'\nfor BatchNormalization or 'ln' for LayerNormalization. If None, then no\nnormalization is performed. For the 'resnet' backbone, it results in the\nEDSR-style residual block.</li>\n<li><strong>dropout_rate</strong> (float, optional):\nFloat between 0 and 1. Fraction of the input units to drop. If 0 then no\ndropout is applied.</li>\n<li><strong>dropout_variant</strong> (str or None, optional):\nType of dropout. Defined in dl4ds.DROPOUT_VARIANTS variable.</li>\n<li><strong>attention</strong> (bool, optional):\nIf True, dl4ds.ChannelAttention2D is used in convolutional blocks.</li>\n<li><strong>activation</strong> (str, optional):\nActivation function to use, as supported by tf.keras. E.g., 'relu' or \n'gelu'.</li>\n<li><strong>output_activation</strong> (str, optional):\nActivation function to use in the last ConvBlock. Useful to constraint \nthe values distribution of the output grid.</li>\n<li><strong>localcon_layer</strong> (bool, optional):\nIf True, the LocalizedConvBlock is activated in the output module.</li>\n</ul>\n", "signature": "(\n    backbone_block,\n    n_channels,\n    n_aux_channels,\n    hr_size,\n    n_channels_out=1,\n    n_filters=8,\n    n_blocks=6,\n    dropout_rate=0,\n    dropout_variant=None,\n    normalization=None,\n    attention=False,\n    activation='relu',\n    output_activation=None,\n    localcon_layer=False\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.sp_preups.unet_pin", "modulename": "dl4ds.models.sp_preups", "qualname": "unet_pin", "type": "function", "doc": "<p>Deep neural network with UNET (encoder-decoder) backbone and pre-upsampling \nvia interpolation.</p>\n\n<p>The interpolation method depends on the <code>interpolation</code> argument used in\nthe training procedure (which is passed to the DataGenerator).</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>normalization</strong> (str or None, optional):\nNormalization method in the residual or dense block. Can be either 'bn'\nfor BatchNormalization or 'ln' for LayerNormalization. If None, then no\nnormalization is performed (eg., for the 'resnet' backbone this results \nin the EDSR-style residual block).</li>\n<li><strong>dropout_rate</strong> (float, optional):\nFloat between 0 and 1. Fraction of the input units to drop. If 0 then no\ndropout is applied.</li>\n<li><strong>dropout_variant</strong> (str or None, optional):\nType of dropout. Defined in dl4ds.DROPOUT_VARIANTS variable.</li>\n</ul>\n", "signature": "(\n    backbone_block,\n    n_channels,\n    n_aux_channels,\n    n_filters,\n    n_blocks,\n    hr_size,\n    n_channels_out=1,\n    activation='relu',\n    dropout_rate=0,\n    dropout_variant=None,\n    normalization=None,\n    attention=False,\n    decoder_upsampling='rc',\n    rc_interpolation='bilinear',\n    output_activation=None,\n    width_cap=256,\n    localcon_layer=False\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.spt_postups", "modulename": "dl4ds.models.spt_postups", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.models.spt_postups.recnet_postupsampling", "modulename": "dl4ds.models.spt_postups", "qualname": "recnet_postupsampling", "type": "function", "doc": "<p>Recurrent deep neural network with different backbone architectures \n(according to the <code>backbone_block</code>) and post-upsampling methods (according \nto <code>upsampling</code>). These models are capable of exploiting spatio-temporal\nsamples.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>backbone_block</strong> (str):\nBackbone type. One of dl4ds.BACKBONE_BLOCKS. WARNING: this parameter is\nnot supposed to be set by the user. It's set internallly through \ndl4ds.Trainers.</li>\n<li><strong>upsampling</strong> (str):\nUpsampling method. One of dl4ds.UPSAMPLING_METHODS. WARNING: this \nparameter is not supposed to be set by the user. It's set internallly \nthrough dl4ds.Trainers.</li>\n<li><strong>scale</strong> (int):\nScaling factor, LR wrt HR grid. WARNING: this parameter is not supposed \nto be set by the user. It's set internallly through dl4ds.Trainers.</li>\n<li><strong>n_channels</strong> (int):\nNumber of channels/variables in each sample. WARNING: this parameter is\nnot supposed to be set by the user. It's set internallly through\ndl4ds.Trainers.</li>\n<li><strong>n_aux_channels</strong> (int):\nNumber of auxiliary channels. WARNING: this parameter is not supposed to \nbe set by the user. It's set internallly through dl4ds.Trainers.</li>\n<li><strong>lr_size</strong> (tuple):\nHeight and width of the LR grid. WARNING: this parameter is not supposed \nto be set by the user. It's set internallly through dl4ds.Trainers.</li>\n<li><strong>time_window</strong> (int):\nTemporal window or number of time steps in each sample. WARNING: this \nparameter is not supposed to be set by the user. It's set internallly <br />\nthrough dl4ds.Trainers.</li>\n<li><strong>n_filters</strong> (int, optional):\nNumber of convolutional filters in RecurrentConvBlock. <code>n_filters</code> sets \nthe number of output filters in the convolution inside the ConvLSTM unit.</li>\n<li><strong>n_blocks</strong> (int, optional):\nNumber of recurrent convolutional blocks (RecurrentConvBlock). \nSets the depth of the network.</li>\n<li><strong>normalization</strong> (str or None, optional):\nNormalization method in the residual or dense block. Can be either 'bn'\nfor BatchNormalization or 'ln' for LayerNormalization. If None, then no\nnormalization is performed (eg., for the 'resnet' backbone this results \nin the EDSR-style residual block).</li>\n<li><strong>dropout_rate</strong> (float, optional):\nFloat between 0 and 1. Fraction of the input units to drop. If 0 then no\ndropout is applied.</li>\n<li><strong>dropout_variant</strong> (str or None, optional):\nType of dropout. Defined in dl4ds.DROPOUT_VARIANTS variable.</li>\n<li><strong>attention</strong> (bool, optional):\nIf True, dl4ds.ChannelAttention2D is used in convolutional blocks.</li>\n<li><strong>activation</strong> (str, optional):\nActivation function to use, as supported by tf.keras. E.g., 'relu' or \n'gelu'.</li>\n<li><strong>output_activation</strong> (str, optional):\nActivation function to use in the last ConvBlock. Useful to constraint \nthe values distribution of the output grid.</li>\n<li><strong>rc_interpolation</strong> (str, optional):\nInterpolation used in the ResizeConvolutionBlock. Supported methods: \n\"bilinear\", \"nearest\", \"bicubic\", \"area\", \"lanczos3\", \"lanczos5\", \n\"gaussian\", \"mitchellcubic\".</li>\n<li><strong>localcon_layer</strong> (bool, optional):\nIf True, the LocalizedConvBlock is activated in the output module.</li>\n</ul>\n", "signature": "(\n    backbone_block,\n    upsampling,\n    scale,\n    n_channels,\n    n_aux_channels,\n    lr_size,\n    time_window,\n    n_channels_out=1,\n    n_filters=8,\n    n_blocks=4,\n    dropout_rate=0,\n    dropout_variant=None,\n    normalization=None,\n    attention=False,\n    activation='relu',\n    output_activation=None,\n    rc_interpolation='bilinear',\n    localcon_layer=False\n)", "funcdef": "def"}, {"fullname": "dl4ds.models.spt_preups", "modulename": "dl4ds.models.spt_preups", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.models.spt_preups.recnet_pin", "modulename": "dl4ds.models.spt_preups", "qualname": "recnet_pin", "type": "function", "doc": "<p>Recurrent deep neural network with different backbone architectures \n(according to the <code>backbone_block</code>) and pre-upsampling via interpolation\n(the samples are expected to be interpolated to the HR grid). This model is \ncapable of exploiting spatio-temporal samples.</p>\n\n<p>The interpolation method depends on the <code>interpolation</code> argument used in\nthe training procedure (which is passed to the DataGenerator).</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>backbone_block</strong> (str):\nBackbone type. One of dl4ds.BACKBONE_BLOCKS. WARNING: this parameter is\nnot supposed to be set by the user. It's set internallly through\ndl4ds.Trainers.</li>\n<li><strong>n_channels</strong> (int):\nNumber of channels/variables in each sample. WARNING: this parameter is\nnot supposed to be set by the user. It's set internallly through\ndl4ds.Trainers.</li>\n<li><strong>n_aux_channels</strong> (int):\nNumber of auxiliary channels. WARNING: this parameter is not supposed to \nbe set by the user. It's set internallly through dl4ds.Trainers.</li>\n<li><strong>hr_size</strong> (tuple):\nHeight and width of the HR grid. WARNING: this parameter is not supposed \nto be set by the user. It's set internallly through dl4ds.Trainers.</li>\n<li><strong>time_window</strong> (int):\nTemporal window or number of time steps in each sample. WARNING: this \nparameter is not supposed to be set by the user. It's set internallly \nthrough dl4ds.Trainers.</li>\n<li><strong>n_filters</strong> (int, optional):\nNumber of convolutional filters in RecurrentConvBlock. <code>n_filters</code> sets \nthe number of output filters in the convolution inside the ConvLSTM unit.</li>\n<li><strong>n_blocks</strong> (int, optional):\nNumber of recurrent convolutional blocks (RecurrentConvBlock). \nSets the depth of the network.</li>\n<li><strong>normalization</strong> (str or None, optional):\nNormalization method in the residual or dense block. Can be either 'bn'\nfor BatchNormalization or 'ln' for LayerNormalization. If None, then no\nnormalization is performed (eg., for the 'resnet' backbone this results \nin the EDSR-style residual block).</li>\n<li><strong>dropout_rate</strong> (float, optional):\nFloat between 0 and 1. Fraction of the input units to drop. If 0 then no\ndropout is applied.</li>\n<li><strong>dropout_variant</strong> (str or None, optional):\nType of dropout. Defined in dl4ds.DROPOUT_VARIANTS variable.</li>\n<li><strong>attention</strong> (bool, optional):\nIf True, dl4ds.ChannelAttention2D is used in convolutional blocks.</li>\n<li><strong>activation</strong> (str, optional):\nActivation function to use, as supported by tf.keras. E.g., 'relu' or \n'gelu'.</li>\n<li><strong>output_activation</strong> (str, optional):\nActivation function to use in the last ConvBlock. Useful to constraint \nthe values distribution of the output grid.</li>\n<li><strong>localcon_layer</strong> (bool, optional):\nIf True, the LocalizedConvBlock is activated in the output module.</li>\n</ul>\n", "signature": "(\n    backbone_block,\n    n_channels,\n    n_aux_channels,\n    hr_size,\n    time_window,\n    n_channels_out=1,\n    n_filters=8,\n    n_blocks=6,\n    normalization=None,\n    dropout_rate=0,\n    dropout_variant=None,\n    attention=False,\n    activation='relu',\n    output_activation=None,\n    localcon_layer=False\n)", "funcdef": "def"}, {"fullname": "dl4ds.preprocessing", "modulename": "dl4ds.preprocessing", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.preprocessing.MinMaxScaler", "modulename": "dl4ds.preprocessing", "qualname": "MinMaxScaler", "type": "class", "doc": "<p>Transform data to a given range.\nThis estimator scales and translates the data distribution such\nthat it is in the given range on the training set, e.g. between\nzero and one. If NaN values are present there will be replaced by a given\nvalue, e.g. minus one. </p>\n\n<p>The transformation is given by::\n    X_std = (X - X.min(axis)) / (X.max(axis) - X.min(axis))\n    X_scaled = X_std * (max - min) + min\nwhere min, max = value_range.</p>\n\n<p>This transformation is often used as an alternative to zero mean,\nunit variance scaling.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>value_range</strong> (tuple (min, max), default=(0, 1)):\nDesired range of transformed data.</li>\n<li><strong>copy</strong> (bool, default=True):\nSet to False to perform inplace row normalization and avoid a\ncopy (if the input is already a numpy array).</li>\n<li><strong>axis</strong> (int, tuple of int or None, default=None):\nAxis or axes along which the minimum and maximum will be computed (via \n<code>np.nanmin</code> and <code>np.nanmax</code> functions). If None then the new range\nis computed from the whole dataset (all dimensions/axes).</li>\n<li><strong>fillnanto</strong> (float or int, deafult=-1):\nValue to be used when filling in NaN values.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>NaNs are disregarded in fit when transforming to the new value range, and \nthen replaced according to <code>fillnanto</code> in transform.</p>\n", "bases": "sklearn.base.TransformerMixin, sklearn.base.BaseEstimator"}, {"fullname": "dl4ds.preprocessing.MinMaxScaler.__init__", "modulename": "dl4ds.preprocessing", "qualname": "MinMaxScaler.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, value_range=(0, 1), copy=True, axis=None, fillnanto=-1)", "funcdef": "def"}, {"fullname": "dl4ds.preprocessing.MinMaxScaler.fit", "modulename": "dl4ds.preprocessing", "qualname": "MinMaxScaler.fit", "type": "function", "doc": "<p>Compute the minimum and maximum to be used for later scaling.</p>\n", "signature": "(self, X, y=None)", "funcdef": "def"}, {"fullname": "dl4ds.preprocessing.MinMaxScaler.partial_fit", "modulename": "dl4ds.preprocessing", "qualname": "MinMaxScaler.partial_fit", "type": "function", "doc": "<p>Computation of min and max on X for later scaling.</p>\n", "signature": "(self, X, y=None)", "funcdef": "def"}, {"fullname": "dl4ds.preprocessing.MinMaxScaler.transform", "modulename": "dl4ds.preprocessing", "qualname": "MinMaxScaler.transform", "type": "function", "doc": "<p>Scale X according to range.</p>\n", "signature": "(self, X)", "funcdef": "def"}, {"fullname": "dl4ds.preprocessing.MinMaxScaler.inverse_transform", "modulename": "dl4ds.preprocessing", "qualname": "MinMaxScaler.inverse_transform", "type": "function", "doc": "<p>Undo the scaling of X according to range.</p>\n", "signature": "(self, X)", "funcdef": "def"}, {"fullname": "dl4ds.preprocessing.StandardScaler", "modulename": "dl4ds.preprocessing", "qualname": "StandardScaler", "type": "class", "doc": "<p>Standardize features by removing the mean and scaling to unit variance.</p>\n\n<p>The standard score of a sample <code>x</code> is calculated as:\n    z = (x - u) / s\nwhere <code>u</code> is the mean of the data or zero if <code>with_mean=False</code>,\nand <code>s</code> is the standard deviation of the data or one if <code>with_std=False</code>.</p>\n\n<p>Mean and standard deviation are then stored to be used on later data using\n<code>transform</code>.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>copy</strong> (bool, default=True):\nIf False, try to avoid a copy and do inplace scaling instead.\nThis is not guaranteed to always work inplace; e.g. if the data is\nnot a NumPy array or scipy.sparse CSR matrix, a copy may still be\nreturned.</li>\n<li><strong>with_mean</strong> (bool, default=True):\nIf True, center the data before scaling.\nThis does not work (and will raise an exception) when attempted on\nsparse matrices, because centering them entails building a dense\nmatrix which in common use cases is likely to be too large to fit in\nmemory.</li>\n<li><strong>with_std</strong> (bool, default=True):\nIf True, scale the data to unit variance (or equivalently,\nunit standard deviation).</li>\n<li><strong>axis</strong> (None or int or tuple of int, default=None):\nAxis or axes along which the minimum and maximum will be computed (via \n<code>np.nanmin</code> and <code>np.nanmax</code> functions). If None then the new range\nis computed from the whole dataset (all dimensions/axes).</li>\n<li><strong>fillnanto</strong> (float or int, deafult=0):\nValue to be used when filling in NaN values.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>NaNs are disregarded in fit when transforming to the new value range, and \nthen replaced according to <code>fillnanto</code> in transform.</p>\n", "bases": "sklearn.base.TransformerMixin, sklearn.base.BaseEstimator"}, {"fullname": "dl4ds.preprocessing.StandardScaler.__init__", "modulename": "dl4ds.preprocessing", "qualname": "StandardScaler.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    copy=True,\n    with_mean=True,\n    with_std=True,\n    axis=None,\n    fillnanto=0\n)", "funcdef": "def"}, {"fullname": "dl4ds.preprocessing.StandardScaler.fit", "modulename": "dl4ds.preprocessing", "qualname": "StandardScaler.fit", "type": "function", "doc": "<p>Compute the minimum and maximum to be used for later scaling.</p>\n", "signature": "(self, X, y=None)", "funcdef": "def"}, {"fullname": "dl4ds.preprocessing.StandardScaler.partial_fit", "modulename": "dl4ds.preprocessing", "qualname": "StandardScaler.partial_fit", "type": "function", "doc": "<p>Computation of mean and standard deviation of X for later scaling.</p>\n", "signature": "(self, X, y=None)", "funcdef": "def"}, {"fullname": "dl4ds.preprocessing.StandardScaler.transform", "modulename": "dl4ds.preprocessing", "qualname": "StandardScaler.transform", "type": "function", "doc": "<p>Scale X according to range.</p>\n", "signature": "(self, X)", "funcdef": "def"}, {"fullname": "dl4ds.preprocessing.StandardScaler.inverse_transform", "modulename": "dl4ds.preprocessing", "qualname": "StandardScaler.inverse_transform", "type": "function", "doc": "<p>Undo the scaling of X according to range.</p>\n", "signature": "(self, X)", "funcdef": "def"}, {"fullname": "dl4ds.training", "modulename": "dl4ds.training", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.training.base", "modulename": "dl4ds.training.base", "type": "module", "doc": "<p>Base training class</p>\n"}, {"fullname": "dl4ds.training.base.Trainer", "modulename": "dl4ds.training.base", "qualname": "Trainer", "type": "class", "doc": "<p></p>\n", "bases": "abc.ABC"}, {"fullname": "dl4ds.training.base.Trainer.__init__", "modulename": "dl4ds.training.base", "qualname": "Trainer.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    backbone,\n    upsampling,\n    data_train,\n    data_train_lr=None,\n    time_window=None,\n    loss='mae',\n    batch_size=64,\n    patch_size=None,\n    scale=4,\n    device='GPU',\n    gpu_memory_growth=True,\n    use_multiprocessing=False,\n    verbose=True,\n    model_list=None,\n    save=True,\n    save_path=None,\n    show_plot=False\n)", "funcdef": "def"}, {"fullname": "dl4ds.training.base.Trainer.run", "modulename": "dl4ds.training.base", "qualname": "Trainer.run", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dl4ds.training.base.Trainer.setup_model", "modulename": "dl4ds.training.base", "qualname": "Trainer.setup_model", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dl4ds.training.base.Trainer.save_results", "modulename": "dl4ds.training.base", "qualname": "Trainer.save_results", "type": "function", "doc": "<p>Save the TF model, learning curve, running time and test score.</p>\n", "signature": "(self, model_to_save=None, folder_prefix=None)", "funcdef": "def"}, {"fullname": "dl4ds.training.cgan", "modulename": "dl4ds.training.cgan", "type": "module", "doc": "<p>Training procedure for Conditional GAN models. Follows Isola et al. 2016</p>\n"}, {"fullname": "dl4ds.training.cgan.CGANTrainer", "modulename": "dl4ds.training.cgan", "qualname": "CGANTrainer", "type": "class", "doc": "<p></p>\n", "bases": "dl4ds.training.base.Trainer"}, {"fullname": "dl4ds.training.cgan.CGANTrainer.__init__", "modulename": "dl4ds.training.cgan", "qualname": "CGANTrainer.__init__", "type": "function", "doc": "<p>Training conditional adversarial generative models.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>backbone</strong> (str):\nString with the name of the backbone block used for the CGAN \ngenerator.</li>\n<li><strong>upsampling</strong> (str):\nString with the name of the upsampling method used for the CGAN \ngenerator.</li>\n<li><strong>data_train</strong> (4D ndarray or xr.DataArray):\nTraining dataset with dims [nsamples, lat, lon, 1]. These grids must \ncorrespond to the observational reference at HR, from which a \ncoarsened version will be created to produce paired samples.</li>\n<li><strong>data_test</strong> (4D ndarray or xr.DataArray):\nTesting dataset with dims [nsamples, lat, lon, 1]. Holdout not used\nduring training, but only to compute metrics with the final model.</li>\n<li><strong>predictors_train</strong> (list of ndarray, optional):\nPredictor variables for trianing. Given as list of 4D ndarrays with \ndims [nsamples, lat, lon, 1] or 5D ndarrays with dims \n[nsamples, time, lat, lon, 1].</li>\n<li><strong>predictors_test</strong> (list of ndarray, optional):\nPredictor variables for testing. Given as list of 4D ndarrays with \ndims [nsamples, lat, lon, 1] or 5D ndarrays with dims \n[nsamples, time, lat, lon, 1].</li>\n<li><strong>epochs</strong> (int, optional):\nNumber of epochs or passes through the whole training dataset.</li>\n<li><strong>steps_per_epoch</strong> (int, optional):\n<code>batch_size * steps_per_epoch</code> samples are passed per epoch.</li>\n<li><strong>scale</strong> (int, optional):\nScaling factor.</li>\n<li><strong>interpolation</strong> (str, optional):\nInterpolation used when upsampling/downsampling the training samples.</li>\n<li><strong>patch_size</strong> (int, optional):\nSize of the square patches used to grab training samples.</li>\n<li><strong>batch_size</strong> (int, optional):\nBatch size per replica.</li>\n<li><strong>learning_rates</strong> (float or tuple of floats or list of floats, optional):\nLearning rate for both the generator and discriminator. If a \ntuple/list is given, it corresponds to the learning rates of the\ngenerator and the discriminator (in that order).</li>\n<li><strong>static_vars</strong> (None or list of 2D ndarrays, optional):\nStatic variables such as elevation data or a binary land-ocean mask.</li>\n<li><strong>checkpoints_frequency</strong> (int, optional):\nThe training loop saves a checkpoint every <code>checkpoints_frequency</code> \nepochs. If None, then no checkpoints are saved during training.</li>\n<li><strong>device</strong> (str):\nChoice of 'GPU' or 'CPU' for the training of the Tensorflow models.</li>\n<li><strong>gpu_memory_growth</strong> (bool, optional):\nBy default, TensorFlow maps nearly all of the GPU memory of all GPUs.\nIf True, we request to only grow the memory usage as is needed by the \nprocess.</li>\n<li><strong>verbose</strong> (bool, optional):\nVerbosity mode. False or 0 = silent. True or 1, max amount of \ninformation is printed out. When equal 2, then less info is shown.</li>\n</ul>\n", "signature": "(\n    self,\n    backbone,\n    upsampling,\n    data_train,\n    data_test,\n    data_train_lr=None,\n    data_test_lr=None,\n    predictors_train=None,\n    predictors_test=None,\n    scale=5,\n    patch_size=None,\n    time_window=True,\n    loss='mae',\n    epochs=60,\n    batch_size=16,\n    learning_rates=(0.0002, 0.0002),\n    device='GPU',\n    gpu_memory_growth=True,\n    model_list=None,\n    steps_per_epoch=None,\n    interpolation='inter_area',\n    static_vars=None,\n    checkpoints_frequency=0,\n    save=False,\n    save_path=None,\n    save_logs=False,\n    save_loss_history=True,\n    generator_params={},\n    discriminator_params={},\n    verbose=True\n)", "funcdef": "def"}, {"fullname": "dl4ds.training.cgan.CGANTrainer.setup_model", "modulename": "dl4ds.training.cgan", "qualname": "CGANTrainer.setup_model", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dl4ds.training.cgan.CGANTrainer.run", "modulename": "dl4ds.training.cgan", "qualname": "CGANTrainer.run", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dl4ds.training.cgan.load_checkpoint", "modulename": "dl4ds.training.cgan", "qualname": "load_checkpoint", "type": "function", "doc": "<p></p>\n", "signature": "(\n    checkpoint_dir,\n    checkpoint_number,\n    backbone,\n    upsampling,\n    scale,\n    input_height_width,\n    n_static_vars=0,\n    n_predictors=0,\n    time_window=None,\n    n_blocks=(20, 4),\n    n_filters=(8, 32),\n    attention=False,\n    localcon_layer=False\n)", "funcdef": "def"}, {"fullname": "dl4ds.training.cgan.generator_loss", "modulename": "dl4ds.training.cgan", "qualname": "generator_loss", "type": "function", "doc": "<p>Generator loss:\nThe generator loss is then calculated from the discriminator\u2019s \nclassification \u2013 it gets rewarded if it successfully fools the discriminator, \nand gets penalized otherwise. </p>\n\n<ul>\n<li>It is a sigmoid cross entropy loss of the discriminator output and an \narray of ones.</li>\n<li>The paper also includes L1 loss which is MAE (mean absolute error) between \nthe generated image and the target image. The px loss is controlled with the \n<code>gen_pxloss_function</code> argument</li>\n<li>This allows the generated image to become structurally similar to the \ntarget image.</li>\n<li>The formula to calculate the total generator loss is:</li>\n</ul>\n\n<p>loss = gan_loss + LAMBDA * px_loss,</p>\n\n<p>where LAMBDA = 100 was decided by the authors of the paper.</p>\n", "signature": "(\n    disc_generated_output,\n    gen_output,\n    target,\n    gen_pxloss_function,\n    lambda_scaling_factor=100\n)", "funcdef": "def"}, {"fullname": "dl4ds.training.cgan.discriminator_loss", "modulename": "dl4ds.training.cgan", "qualname": "discriminator_loss", "type": "function", "doc": "<p>Discriminator loss:</p>\n\n<ul>\n<li>The discriminator loss function takes 2 inputs; real images, generated \nimages</li>\n<li>real_loss is a sigmoid cross entropy loss of the real images and an array \nof ones(since these are the real images)</li>\n<li>generated_loss is a sigmoid cross entropy loss of the generated images and \nan array of zeros(since these are the fake images)</li>\n<li>Then the total_loss is the sum of real_loss and the generated_loss</li>\n</ul>\n", "signature": "(disc_real_output, disc_generated_output)", "funcdef": "def"}, {"fullname": "dl4ds.training.cgan.train_step", "modulename": "dl4ds.training.cgan", "qualname": "train_step", "type": "function", "doc": "<p>Training:</p>\n\n<ul>\n<li>For each example input generate an output.</li>\n<li>The discriminator receives the input_image and the generated image as the \nfirst input. The second input is the input_image and the target_image.</li>\n<li>Next, we calculate the generator and the discriminator loss.</li>\n<li>Then, we calculate the gradients of loss with respect to both the \ngenerator and the discriminator variables(inputs) and apply those to the optimizer.</li>\n</ul>\n", "signature": "(\n    lr_array,\n    hr_array,\n    generator,\n    discriminator,\n    generator_optimizer,\n    discriminator_optimizer,\n    epoch,\n    gen_pxloss_function,\n    summary_writer,\n    first_batch,\n    static_array=None\n)", "funcdef": "def"}, {"fullname": "dl4ds.training.supervised", "modulename": "dl4ds.training.supervised", "type": "module", "doc": "<p>Training procedure for supervised models</p>\n"}, {"fullname": "dl4ds.training.supervised.SupervisedTrainer", "modulename": "dl4ds.training.supervised", "qualname": "SupervisedTrainer", "type": "class", "doc": "<p></p>\n", "bases": "dl4ds.training.base.Trainer"}, {"fullname": "dl4ds.training.supervised.SupervisedTrainer.__init__", "modulename": "dl4ds.training.supervised", "qualname": "SupervisedTrainer.__init__", "type": "function", "doc": "<p>Training procedure for supervised models.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>backbone</strong> (str):\nString with the name of the backbone block.</li>\n<li><strong>upsampling</strong> (str):\nString with the name of the upsampling method.</li>\n<li><strong>data_train</strong> (4D ndarray or xr.DataArray):\nTraining dataset with dims [nsamples, lat, lon, 1]. These grids must \ncorrespond to the observational reference at HR, from which a \ncoarsened version will be created to produce paired samples.</li>\n<li><strong>data_val</strong> (4D ndarray or xr.DataArray):\nValidation dataset with dims [nsamples, lat, lon, 1]. This holdout \ndataset is used at the end of each epoch to check the losses and \ndiagnose overfitting.</li>\n<li><strong>data_test</strong> (4D ndarray or xr.DataArray):\nTesting dataset with dims [nsamples, lat, lon, 1]. Holdout not used\nduring training, but only to compute metrics with the final model.</li>\n<li><strong>predictors_train</strong> (list of ndarray, optional):\nPredictor variables for trianing. Given as list of 4D ndarrays with \ndims [nsamples, lat, lon, 1] or 5D ndarrays with dims \n[nsamples, time, lat, lon, 1].</li>\n<li><strong>predictors_val</strong> (list of ndarray, optional):\nPredictor variables for validation. Given as list of 4D ndarrays\nwith dims [nsamples, lat, lon, 1] or 5D ndarrays with dims \n[nsamples, time, lat, lon, 1].</li>\n<li><strong>predictors_test</strong> (list of ndarrays, optional):\nPredictor variables for testing. Given as list of 4D ndarrays with \ndims [nsamples, lat, lon, 1] or 5D ndarrays with dims \n[nsamples, time, lat, lon, 1].</li>\n<li><strong>static_vars</strong> (None or list of 2D ndarrays, optional):\nStatic variables such as elevation data or a binary land-ocean mask.</li>\n<li><strong>scale</strong> (int, optional):\nScaling factor.</li>\n<li><strong>interpolation</strong> (str, optional):\nInterpolation used when upsampling/downsampling the training samples.</li>\n<li><strong>patch_size</strong> (int or None, optional):\nSize of the square patches used to grab training samples.</li>\n<li><strong>time_window</strong> (int or None, optional):\nIf not None, then each sample will have a temporal dimension \n(<code>time_window</code> slices to the past are grabbed for the LR array).</li>\n<li><strong>batch_size</strong> (int, optional):\nBatch size per replica.</li>\n<li><strong>epochs</strong> (int, optional):\nNumber of epochs or passes through the whole training dataset.</li>\n<li><strong>steps_per_epoch</strong> (int or None, optional):\nTotal number of steps (batches of samples) before declaring one epoch\nfinished.<code>batch_size * steps_per_epoch</code> samples are passed per \nepoch. If None, <code>then steps_per_epoch</code> is equal to the number of \nsamples diviced by the <code>batch_size</code>.</li>\n<li><strong>validation_steps</strong> (int, optional):\nSteps using at the end of each epoch for drawing validation samples.</li>\n<li><strong>test_steps</strong> (int, optional):\nSteps using after training for drawing testing samples.</li>\n<li><strong>learning_rate</strong> (float or tuple of floats or list of floats, optional):\nLearning rate. If a tuple/list is given, it corresponds to the min \nand max LR used for a PiecewiseConstantDecay scheduler.</li>\n<li><strong>lr_decay_after</strong> (float or None, optional):\nUsed for the PiecewiseConstantDecay scheduler.</li>\n<li><strong>early_stopping</strong> (bool, optional):\nWhether to use early stopping.</li>\n<li><strong>patience</strong> (int, optional):\nPatience for early stopping.</li>\n<li><strong>min_delta</strong> (float, otional):\nMin delta for early stopping.</li>\n<li><strong>save</strong> (bool, optional):\nWhether to save the final model.</li>\n<li><strong>save_path</strong> (None or str):\nPath for saving the final model, running time and test score. If \nNone, then <code>'./'</code> is used. The SavedModel format is a \ndirectory containing a protobuf binary and a TensorFlow checkpoint.</li>\n<li><strong>save_bestmodel</strong> (None or str):\nIf True, the model with the best validation loss is saved during \ntraining.</li>\n<li><strong>device</strong> (str):\nChoice of 'GPU' or 'CPU' for the training of the Tensorflow models.</li>\n<li><strong>gpu_memory_growth</strong> (bool, optional):\nBy default, TensorFlow maps nearly all of the GPU memory of all GPUs.\nIf True, we request to only grow the memory usage as is needed by \nthe process.</li>\n<li><strong>use_multiprocessing</strong> (bool, optional):\nUsed for data generator. If True, use process-based threading.</li>\n<li><strong>show_plot</strong> (bool, optional):\nIf True the static plot is shown after training.</li>\n<li><strong>save_plot</strong> (bool, optional):\nIf True the static plot is saved to disk after training.</li>\n<li><strong>verbose</strong> (bool, optional):\nVerbosity mode. False or 0 = silent. True or 1, max amount of \ninformation is printed out. When equal 2, then less info is shown.</li>\n<li><strong>**architecture_params</strong> (dict):\nDictionary with additional parameters passed to the neural network \nmodel.</li>\n</ul>\n", "signature": "(\n    self,\n    backbone,\n    upsampling,\n    data_train,\n    data_val,\n    data_test,\n    data_train_lr=None,\n    data_val_lr=None,\n    data_test_lr=None,\n    predictors_train=None,\n    predictors_val=None,\n    predictors_test=None,\n    static_vars=None,\n    scale=5,\n    interpolation='inter_area',\n    patch_size=None,\n    time_window=None,\n    batch_size=64,\n    loss='mae',\n    epochs=60,\n    steps_per_epoch=None,\n    test_steps=None,\n    validation_steps=None,\n    device='GPU',\n    gpu_memory_growth=True,\n    use_multiprocessing=False,\n    model_list=None,\n    learning_rate=(0.001, 0.0001),\n    lr_decay_after=100000.0,\n    early_stopping=False,\n    patience=6,\n    min_delta=0,\n    show_plot=True,\n    save=False,\n    save_path=None,\n    save_bestmodel=False,\n    trained_model=None,\n    trained_epochs=0,\n    verbose=True,\n    **architecture_params\n)", "funcdef": "def"}, {"fullname": "dl4ds.training.supervised.SupervisedTrainer.setup_datagen", "modulename": "dl4ds.training.supervised", "qualname": "SupervisedTrainer.setup_datagen", "type": "function", "doc": "<p>Setting up the data generators</p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dl4ds.training.supervised.SupervisedTrainer.setup_model", "modulename": "dl4ds.training.supervised", "qualname": "SupervisedTrainer.setup_model", "type": "function", "doc": "<p>Setting up the model</p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dl4ds.training.supervised.SupervisedTrainer.run", "modulename": "dl4ds.training.supervised", "qualname": "SupervisedTrainer.run", "type": "function", "doc": "<p>Compiling, training and saving the model</p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dl4ds.utils", "modulename": "dl4ds.utils", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dl4ds.utils.spatial_to_spatiotemporal_samples", "modulename": "dl4ds.utils", "qualname": "spatial_to_spatiotemporal_samples", "type": "function", "doc": "<p>Add one dimension to spatial array [n_samples or time, lat, lon, vars] \nin order to have [n_samples, time_window, lat, lon, vars].</p>\n", "signature": "(array, time_window)", "funcdef": "def"}, {"fullname": "dl4ds.utils.spatiotemporal_to_spatial_samples", "modulename": "dl4ds.utils", "qualname": "spatiotemporal_to_spatial_samples", "type": "function", "doc": "<p>Remove dimension <code>time_window</code> from <code>array</code>, resulting in a sequence of\nspatial samples/grids. <code>time_window</code> is a dimension assumed to be in the \nsecond place.</p>\n\n<h3 id=\"to-do-other-ways-to-collapse-the-time_window-dimension\">TO-DO : other ways to collapse the time_window dimension</h3>\n", "signature": "(array, time_window)", "funcdef": "def"}, {"fullname": "dl4ds.utils.checkarray_ndim", "modulename": "dl4ds.utils", "qualname": "checkarray_ndim", "type": "function", "doc": "<p>Check the np.ndarray has at least <code>ndim</code> dimensions. If needed a new\ndimension (of lenght 1) is added at the position given by <code>add_axis_position</code>.</p>\n", "signature": "(array, ndim=3, add_axis_position=-1)", "funcdef": "def"}, {"fullname": "dl4ds.utils.check_compatibility_upsbackb", "modulename": "dl4ds.utils", "qualname": "check_compatibility_upsbackb", "type": "function", "doc": "<p>Check that the upsampling and backbone arguments are compatible. </p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>backbone</strong> (str):\nBackbone block.</li>\n<li><strong>upsampling</strong> (str):\nUpsampling method.</li>\n<li><strong>time_window</strong> (int):\nTime window for spatio-temporal samples.</li>\n</ul>\n", "signature": "(backbone, upsampling, time_window)", "funcdef": "def"}, {"fullname": "dl4ds.utils.checkarg_upsampling", "modulename": "dl4ds.utils", "qualname": "checkarg_upsampling", "type": "function", "doc": "<p>Check the argument <code>upsampling</code>.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>upsampling</strong> (str):\nUpsampling method.</li>\n</ul>\n", "signature": "(upsampling)", "funcdef": "def"}, {"fullname": "dl4ds.utils.checkarg_backbone", "modulename": "dl4ds.utils", "qualname": "checkarg_backbone", "type": "function", "doc": "<p>Check the argument <code>backbone</code>.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>backbone</strong> (str):\nBackbone block.</li>\n</ul>\n", "signature": "(backbone)", "funcdef": "def"}, {"fullname": "dl4ds.utils.checkarg_dropout_variant", "modulename": "dl4ds.utils", "qualname": "checkarg_dropout_variant", "type": "function", "doc": "<p>Check the argument <code>dropout_variant</code>.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dropout_variant</strong> (str):\nDesired dropout variant.</li>\n</ul>\n", "signature": "(dropout_variant)", "funcdef": "def"}, {"fullname": "dl4ds.utils.checkarg_loss", "modulename": "dl4ds.utils", "qualname": "checkarg_loss", "type": "function", "doc": "<p>Check the argument <code>loss</code>.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>loss</strong> (str):\nLoss/cost function.</li>\n</ul>\n", "signature": "(loss)", "funcdef": "def"}, {"fullname": "dl4ds.utils.set_gpu_memory_growth", "modulename": "dl4ds.utils", "qualname": "set_gpu_memory_growth", "type": "function", "doc": "<p></p>\n", "signature": "()", "funcdef": "def"}, {"fullname": "dl4ds.utils.list_devices", "modulename": "dl4ds.utils", "qualname": "list_devices", "type": "function", "doc": "<p></p>\n", "signature": "(which='physical', gpu=True, verbose=True)", "funcdef": "def"}, {"fullname": "dl4ds.utils.set_visible_gpus", "modulename": "dl4ds.utils", "qualname": "set_visible_gpus", "type": "function", "doc": "<p></p>\n", "signature": "(*gpu_indices)", "funcdef": "def"}, {"fullname": "dl4ds.utils.rank", "modulename": "dl4ds.utils", "qualname": "rank", "type": "function", "doc": "<p></p>\n", "signature": "(x)", "funcdef": "def"}, {"fullname": "dl4ds.utils.Timing", "modulename": "dl4ds.utils", "qualname": "Timing", "type": "class", "doc": "<p></p>\n"}, {"fullname": "dl4ds.utils.Timing.__init__", "modulename": "dl4ds.utils", "qualname": "Timing.__init__", "type": "function", "doc": "<p>Timing utility class.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>verbose</strong> (bool):\nVerbosity.</li>\n</ul>\n", "signature": "(self, verbose=True)", "funcdef": "def"}, {"fullname": "dl4ds.utils.Timing.sep", "modulename": "dl4ds.utils", "qualname": "Timing.sep", "type": "variable", "doc": "<p></p>\n", "default_value": " = '--------------------------------------------------------------------------------'"}, {"fullname": "dl4ds.utils.Timing.runtime", "modulename": "dl4ds.utils", "qualname": "Timing.runtime", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dl4ds.utils.Timing.checktime", "modulename": "dl4ds.utils", "qualname": "Timing.checktime", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dl4ds.utils.crop_array", "modulename": "dl4ds.utils", "qualname": "crop_array", "type": "function", "doc": "<p>Return a square cropped version of a 2D, 3D or 4D or 5D ndarray.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>array</strong> (numpy ndarray):\nInput image (2D ndarray) or cube (3D, 4D or 5D ndarray).</li>\n<li><strong>size</strong> (int):\nSize of the cropped image.</li>\n<li><strong>yx</strong> (tuple of int or None, optional):\nY,X coordinate of the bottom-left corner. If None then a random\nposition will be chosen.</li>\n<li><strong>position</strong> (bool, optional):\nIf set to True return also the coordinates of the bottom-left corner.</li>\n<li><strong>get_copy</strong> (bool, optional):\nIf True a cropped copy of the intial array is returned. By default a\nsliced view of the array is returned.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>cropped_array</strong> (numpy ndarray):\nCropped ndarray. By default a view is returned, unless <code>get_copy</code>\nis True.</li>\n<li><strong>y, x</strong> (int):\n[position=True] Y,X coordinates.</li>\n</ul>\n", "signature": "(\n    array,\n    size,\n    yx=None,\n    position=False,\n    exclude_borders=False,\n    get_copy=False\n)", "funcdef": "def"}, {"fullname": "dl4ds.utils.resize_array", "modulename": "dl4ds.utils", "qualname": "resize_array", "type": "function", "doc": "<p>Return a resized version of a 2D or [y,x] 3D ndarray [y,x,channels] or\n4D ndarray [time,y,x,channels] via interpolation.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>array</strong> (numpy ndarray):\nInput ndarray.</li>\n<li><strong>newsize</strong> (tuple of int):\nNew size in X,Y.</li>\n<li><strong>interpolation</strong> (str, optional):\nInterpolation mode.</li>\n<li><strong>squeezed</strong> (bool, optional):\nIf True, the output will be squeezed (any dimension with lenght 1 will\nbe removed).</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>resized_arr</strong> (numpy ndarray):\nInterpolated array with size <code>newsize</code>.</li>\n</ul>\n", "signature": "(\n    array,\n    newsize,\n    interpolation='inter_area',\n    squeezed=True,\n    keep_dynamic_range=False\n)", "funcdef": "def"}, {"fullname": "dl4ds.utils.plot_history", "modulename": "dl4ds.utils", "qualname": "plot_history", "type": "function", "doc": "<p>Plot given training histories.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>histories,</strong>: the histories to plot.\nThis parameter can either be a single or multiple dataframes\nor one or more paths to the stored CSVs or JSON of the history.</li>\n<li><strong>style</strong> (str=\"-\",):\nthe style to use when plotting the graphs.</li>\n<li><strong>side</strong> (int=5,):\nthe side of every sub-graph.</li>\n<li><strong>graphs_per_row</strong> (int=4,):\nnumber of graphs per row.</li>\n<li><strong>customization_callback</strong> (Callable=None,):\ncallback for customising axis.</li>\n<li><strong>path</strong> (str=None,):\nwhere to save the graphs, by defalut nowhere.</li>\n<li><strong>single_graphs</strong> (bool=False,):\nwhetever to create the graphs one by one.</li>\n<li><strong>max_epochs</strong> (Union[int, str] = \"max\",):\nNumber of epochs to plot. Can either be \"max\", \"min\" or a positive integer value.</li>\n<li><strong>log_scale_metrics</strong> (bool = False,):\nWether to use log scale for the metrics.</li>\n<li><strong>title</strong> (str = None,):\nTitle to put on top of the subplots.</li>\n</ul>\n\n<h6 id=\"raises\">Raises</h6>\n\n<ul>\n<li><strong>ValueError,</strong>: If monitor_mode is not either \"min\" or \"max\".</li>\n<li><strong>ValueError,</strong>: If max_epochs is not either \"min\", \"max\" or a numeric integer.</li>\n</ul>\n", "signature": "(\n    histories: Union[keras.callbacks.History, List[keras.callbacks.History], Dict[str, List[float]], pandas.core.frame.DataFrame, List[pandas.core.frame.DataFrame], str, List[str]],\n    style: str = '-',\n    side: float = 5,\n    graphs_per_row: int = 4,\n    customization_callback: Callable = None,\n    path: str = None,\n    single_graphs: bool = False,\n    max_epochs: Union[int, str] = 'max',\n    monitor: str = None,\n    monitor_mode: str = 'max',\n    log_scale_metrics: bool = False,\n    title: str = None\n) -> Tuple[Union[matplotlib.figure.Figure, List[matplotlib.figure.Figure]], Union[matplotlib.axes._axes.Axes, List[matplotlib.axes._axes.Axes]]]", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();