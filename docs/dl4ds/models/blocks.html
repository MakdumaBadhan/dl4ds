<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 11.2.0"/>
    <title>dl4ds.models.blocks API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent }nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .pdoc-alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:1rem center;margin-bottom:1rem;}.pdoc .pdoc-alert > *:last-child{margin-bottom:0;}.pdoc .pdoc-alert-note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--code);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.pdoc details{filter:opacity(1);}.pdoc details:not([open]){height:0;}.pdoc details > summary{position:absolute;top:-35px;right:0;font-size:.75rem;color:var(--muted);padding:0 .7em;user-select:none;cursor:pointer;}.pdoc details > summary:focus{outline:0;}.pdoc > section.module-info details > summary{top:-20px;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc .headerlink{position:absolute;width:0;margin-left:-1.5rem;line-height:1.4rem;font-size:1.5rem;font-weight:normal;transition:all 100ms ease-in-out;opacity:0;user-select:none;}.pdoc .attr > .headerlink{margin-left:-2.5rem;}.pdoc *:hover > .headerlink,.pdoc *:target > .attr > .headerlink{opacity:1;}.pdoc .attr{display:block;color:var(--text);margin:.5rem 0 .5rem;padding:.4rem 5rem .4rem 1rem;background-color:var(--accent);}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{white-space:pre-wrap;}.pdoc .annotation{color:var(--annotation);}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../models.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;dl4ds.models</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



        <h2>API Documentation</h2>
            <ul class="memberlist">
            <li>
                    <a class="class" href="#ConvBlock">ConvBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ConvBlock.__init__">ConvBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#ConvBlock.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#DropPath">DropPath</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#DropPath.__init__">DropPath</a>
                        </li>
                        <li>
                                <a class="function" href="#DropPath.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ConvNextBlock">ConvNextBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ConvNextBlock.__init__">ConvNextBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#ConvNextBlock.build">build</a>
                        </li>
                        <li>
                                <a class="function" href="#ConvNextBlock.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ResidualBlock">ResidualBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ResidualBlock.__init__">ResidualBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#ResidualBlock.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#DenseBlock">DenseBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#DenseBlock.__init__">DenseBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#DenseBlock.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#TransitionBlock">TransitionBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#TransitionBlock.__init__">TransitionBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#TransitionBlock.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#LocalizedConvBlock">LocalizedConvBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#LocalizedConvBlock.__init__">LocalizedConvBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#LocalizedConvBlock.call">call</a>
                        </li>
                        <li>
                                <a class="function" href="#LocalizedConvBlock.compute_output_shape">compute_output_shape</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#RecurrentConvBlock">RecurrentConvBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#RecurrentConvBlock.__init__">RecurrentConvBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#RecurrentConvBlock.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#SubpixelConvolutionBlock">SubpixelConvolutionBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#SubpixelConvolutionBlock.__init__">SubpixelConvolutionBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#SubpixelConvolutionBlock.upsample_conv">upsample_conv</a>
                        </li>
                        <li>
                                <a class="function" href="#SubpixelConvolutionBlock.compute_output_shape">compute_output_shape</a>
                        </li>
                        <li>
                                <a class="function" href="#SubpixelConvolutionBlock.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ResizeConvolutionBlock">ResizeConvolutionBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ResizeConvolutionBlock.__init__">ResizeConvolutionBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#ResizeConvolutionBlock.compute_output_shape">compute_output_shape</a>
                        </li>
                        <li>
                                <a class="function" href="#ResizeConvolutionBlock.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#DeconvolutionBlock">DeconvolutionBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#DeconvolutionBlock.__init__">DeconvolutionBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#DeconvolutionBlock.compute_output_shape">compute_output_shape</a>
                        </li>
                        <li>
                                <a class="function" href="#DeconvolutionBlock.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ChannelAttention2D">ChannelAttention2D</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ChannelAttention2D.__init__">ChannelAttention2D</a>
                        </li>
                        <li>
                                <a class="function" href="#ChannelAttention2D.call">call</a>
                        </li>
                        <li>
                                <a class="function" href="#ChannelAttention2D.get_config">get_config</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#EncoderBlock">EncoderBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#EncoderBlock.__init__">EncoderBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#EncoderBlock.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#PadConcat">PadConcat</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#PadConcat.__init__">PadConcat</a>
                        </li>
                        <li>
                                <a class="function" href="#PadConcat.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MCDropout">MCDropout</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MCDropout.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MCGaussianDropout">MCGaussianDropout</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MCGaussianDropout.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MCSpatialDropout2D">MCSpatialDropout2D</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MCSpatialDropout2D.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MCSpatialDropout3D">MCSpatialDropout3D</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MCSpatialDropout3D.call">call</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="function" href="#get_dropout_layer">get_dropout_layer</a>
            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../dl4ds.html">dl4ds</a><wbr>.<a href="./../models.html">models</a><wbr>.blocks    </h1>

                
                        <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="L-0"><a href="#L-0"><span class="linenos">  0</span></a><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="p">(</span><span class="n">Add</span><span class="p">,</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">ConvLSTM2D</span><span class="p">,</span> <span class="n">Concatenate</span><span class="p">,</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a>                                     <span class="n">SeparableConv2D</span><span class="p">,</span> <span class="n">BatchNormalization</span><span class="p">,</span> 
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a>                                     <span class="n">LayerNormalization</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> 
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a>                                     <span class="n">Dropout</span><span class="p">,</span> <span class="n">GaussianDropout</span><span class="p">,</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a>                                     <span class="n">SpatialDropout2D</span><span class="p">,</span> <span class="n">Conv2DTranspose</span><span class="p">,</span> 
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a>                                     <span class="n">SpatialDropout3D</span><span class="p">,</span> <span class="n">LocallyConnected2D</span><span class="p">,</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a>                                     <span class="n">ZeroPadding2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span><span class="p">,</span> <span class="n">Resizing</span><span class="p">,</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a>                                     <span class="n">DepthwiseConv2D</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">)</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a><span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">checkarg_dropout_variant</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a><span class="k">class</span> <span class="nc">ConvBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span> 
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="sd">    Convolutional block.</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a><span class="sd">    References</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a><span class="sd">    ----------</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a><span class="sd">    [1] Effective and Efficient Dropout for Deep Convolutional Neural Networks: </span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a><span class="sd">    https://arxiv.org/abs/1904.03392</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a><span class="sd">    [2] Rethinking the Usage of Batch Normalization and Dropout: </span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a><span class="sd">    https://arxiv.org/abs/1905.05928</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">ks_cl2</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> 
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a>                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a>                 <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a>                 <span class="n">depthwise_separable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span> <span class="o">=</span> <span class="n">dropout_variant</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_separable</span> <span class="o">=</span> <span class="n">depthwise_separable</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_separable</span><span class="p">:</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a>                <span class="n">filters</span><span class="p">,</span> 
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl1</span><span class="p">,</span> 
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a>                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a>                <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> 
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a>                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a>                <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a>                <span class="n">filters</span><span class="p">,</span> 
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl2</span><span class="p">,</span> 
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a>                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a>                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a>                <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a>                <span class="n">filters</span><span class="p">,</span> 
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl1</span><span class="p">,</span> 
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a>                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a>                <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> 
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a>                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a>                <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a>                <span class="n">filters</span><span class="p">,</span> 
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl2</span><span class="p">,</span> 
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a>                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a>                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a>                <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bn&#39;</span><span class="p">,</span> <span class="s1">&#39;ln&#39;</span><span class="p">]:</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Normalization not supported, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;bn&#39;</span><span class="p">:</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;ln&#39;</span><span class="p">:</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">()</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">()</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">:</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">att</span> <span class="o">=</span> <span class="n">ChannelAttention2D</span><span class="p">(</span><span class="n">filters</span><span class="p">)</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>        
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">get_dropout_layer</span><span class="p">(</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">get_dropout_layer</span><span class="p">(</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a>        <span class="sd">&quot;&quot;&quot;Model&#39;s forward pass.</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="k">else</span> <span class="n">X</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">:</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a><span class="k">class</span> <span class="nc">DropPath</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a><span class="sd">    Drop path layer. </span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a><span class="sd">    Adapted from </span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a><span class="sd">    https://github.com/rishigami/Swin-Transformer-TF/blob/main/swintransformer/model.py</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_prob</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span> <span class="o">=</span> <span class="n">drop_prob</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">training</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">):</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>            <span class="k">return</span> <span class="n">x</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a>        <span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span>    <span class="c1"># Compute keep_prob</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a>        <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">keep_prob</span>   <span class="c1"># Compute drop_connect tensor</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a>        <span class="n">random_tensor</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a>        <span class="n">binary_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">random_tensor</span><span class="p">)</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span> <span class="o">*</span> <span class="n">binary_tensor</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a>        <span class="k">return</span> <span class="n">output</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a><span class="k">class</span> <span class="nc">ConvNextBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span> 
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a><span class="sd">    ConvNext block.</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a><span class="sd">    References</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a><span class="sd">    ----------</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a><span class="sd">    [1] A ConvNet for the 2020s: https://arxiv.org/abs/2201.03545 </span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">drop_path</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">layer_scale_init_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1">#1e-6 </span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>                 <span class="n">use_1x1conv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span> 
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">filters</span> <span class="o">=</span> <span class="n">filters</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="n">drop_path</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_scale_init_value</span> <span class="o">=</span> <span class="n">layer_scale_init_value</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dwconv</span> <span class="o">=</span> <span class="n">DepthwiseConv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>                                      <span class="n">depth_multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pwconv1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">)</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="n">DropPath</span><span class="p">(</span><span class="n">drop_path</span><span class="p">)</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pwconv2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">)</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span> <span class="o">=</span> <span class="n">use_1x1conv</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bn&#39;</span><span class="p">,</span> <span class="s1">&#39;ln&#39;</span><span class="p">]:</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Normalization not supported, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;bn&#39;</span><span class="p">:</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;ln&#39;</span><span class="p">:</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span><span class="p">:</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv1x1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>            <span class="n">initial_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_scale_init_value</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">)),</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>            <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;gamma&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_scale_init_value</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a>        <span class="nb">input</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dwconv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pwconv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pwconv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">x</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span><span class="p">:</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1x1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a><span class="k">class</span> <span class="nc">ResidualBlock</span><span class="p">(</span><span class="n">ConvBlock</span><span class="p">):</span> 
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a><span class="sd">    Residual block. </span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a><span class="sd">    </span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a><span class="sd">    Two examples:</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a><span class="sd">    * Standard residual block [1]: Conv2D -&gt; BN -&gt; ReLU -&gt; Conv2D -&gt; BN -&gt; Add _&gt; ReLU</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a><span class="sd">    * EDSR-style block: Conv2D -&gt; ReLU -&gt; Conv2D -&gt; Add -&gt; ReLU</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a><span class="sd">    References</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a><span class="sd">    ----------</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a><span class="sd">    [1] Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">ks_cl2</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> 
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a>                 <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_1x1conv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="p">,</span> <span class="n">ks_cl2</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> 
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a>                         <span class="n">normalization</span><span class="p">,</span> <span class="n">attention</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> 
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>                         <span class="n">dropout_variant</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span> <span class="o">=</span> <span class="n">use_1x1conv</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span><span class="p">:</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv1x1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> 
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">:</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span><span class="p">:</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1x1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a>        <span class="n">Y</span> <span class="o">+=</span> <span class="n">X</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a><span class="k">class</span> <span class="nc">DenseBlock</span><span class="p">(</span><span class="n">ConvBlock</span><span class="p">):</span> 
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a><span class="sd">    Dense block.</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a><span class="sd">    References</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a><span class="sd">    ----------</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a><span class="sd">    [1] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a><span class="sd">        Densely Connected Convolutional Networks: </span>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a><span class="sd">        https://arxiv.org/abs/1608.06993</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">ks_cl2</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> 
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a>                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a>                 <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="p">,</span> <span class="n">ks_cl2</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> 
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a>                         <span class="n">normalization</span><span class="p">,</span> <span class="n">attention</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> 
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a>                         <span class="n">dropout_variant</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a>            <span class="mi">4</span> <span class="o">*</span> <span class="n">filters</span><span class="p">,</span> 
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a>            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl1</span><span class="p">,</span> 
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a>            <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> 
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a>            <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a>            <span class="n">filters</span><span class="p">,</span> 
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl2</span><span class="p">,</span> 
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a>            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a>            <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">()</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">X</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>       
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">:</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">Y</span><span class="p">,</span> <span class="n">X</span><span class="p">])</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a><span class="k">class</span> <span class="nc">TransitionBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a>    <span class="sd">&quot;&quot;&quot; </span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a><span class="sd">    Transition layer to control the complexity of the model by using 1x1 </span>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a><span class="sd">    convolutions. Used in architectures, such as the Densenet.</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a><span class="sd">    References</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a><span class="sd">    ----------</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a><span class="sd">    [1] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos">287</span></a><span class="sd">        Densely Connected Convolutional Networks: </span>
</span><span id="L-288"><a href="#L-288"><span class="linenos">288</span></a><span class="sd">        https://arxiv.org/abs/1608.06993</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos">289</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos">290</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="L-291"><a href="#L-291"><span class="linenos">291</span></a>                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos">292</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="L-293"><a href="#L-293"><span class="linenos">293</span></a>        <span class="k">if</span> <span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;bn&#39;</span><span class="p">:</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos">294</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos">295</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos">296</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos">297</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos">298</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos">299</span></a>
</span><span id="L-300"><a href="#L-300"><span class="linenos">300</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos">301</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-302"><a href="#L-302"><span class="linenos">302</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos">303</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos">304</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos">305</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos">306</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="L-307"><a href="#L-307"><span class="linenos">307</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos">308</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos">309</span></a>
</span><span id="L-310"><a href="#L-310"><span class="linenos">310</span></a>
</span><span id="L-311"><a href="#L-311"><span class="linenos">311</span></a><span class="k">class</span> <span class="nc">LocalizedConvBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos">312</span></a>    <span class="sd">&quot;&quot;&quot; </span>
</span><span id="L-313"><a href="#L-313"><span class="linenos">313</span></a><span class="sd">    Localized convolutional block through a locally connected layer (1x1 kernel) </span>
</span><span id="L-314"><a href="#L-314"><span class="linenos">314</span></a><span class="sd">    with biases.</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos">315</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-316"><a href="#L-316"><span class="linenos">316</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
</span><span id="L-317"><a href="#L-317"><span class="linenos">317</span></a>                 <span class="n">name_sufix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos">318</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;LocalizedConvBlock&#39;</span> <span class="o">+</span> <span class="n">name_sufix</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos">319</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">filters</span> <span class="o">=</span> <span class="n">filters</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos">320</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">transition</span> <span class="o">=</span> <span class="n">TransitionBlock</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">)</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos">321</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">localconv</span> <span class="o">=</span> <span class="n">LocallyConnected2D</span><span class="p">(</span>
</span><span id="L-322"><a href="#L-322"><span class="linenos">322</span></a>            <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos">323</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-324"><a href="#L-324"><span class="linenos">324</span></a>            <span class="n">implementation</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="L-325"><a href="#L-325"><span class="linenos">325</span></a>            <span class="n">bias_initializer</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
</span><span id="L-326"><a href="#L-326"><span class="linenos">326</span></a>            <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
</span><span id="L-327"><a href="#L-327"><span class="linenos">327</span></a>            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos">328</span></a>
</span><span id="L-329"><a href="#L-329"><span class="linenos">329</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="L-330"><a href="#L-330"><span class="linenos">330</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transition</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos">331</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">localconv</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos">332</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span><span id="L-333"><a href="#L-333"><span class="linenos">333</span></a>
</span><span id="L-334"><a href="#L-334"><span class="linenos">334</span></a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos">335</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">)</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos">336</span></a>
</span><span id="L-337"><a href="#L-337"><span class="linenos">337</span></a>
</span><span id="L-338"><a href="#L-338"><span class="linenos">338</span></a><span class="k">class</span> <span class="nc">RecurrentConvBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span> 
</span><span id="L-339"><a href="#L-339"><span class="linenos">339</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-340"><a href="#L-340"><span class="linenos">340</span></a><span class="sd">    Recurrent convolutional block.</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos">341</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-342"><a href="#L-342"><span class="linenos">342</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">ks_cl2</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> 
</span><span id="L-343"><a href="#L-343"><span class="linenos">343</span></a>                 <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="L-344"><a href="#L-344"><span class="linenos">344</span></a>                 <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos">345</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;RecurrentConvBlock&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="L-346"><a href="#L-346"><span class="linenos">346</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
</span><span id="L-347"><a href="#L-347"><span class="linenos">347</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>
</span><span id="L-348"><a href="#L-348"><span class="linenos">348</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span> <span class="o">=</span> <span class="n">dropout_variant</span>
</span><span id="L-349"><a href="#L-349"><span class="linenos">349</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">convlstm1</span> <span class="o">=</span> <span class="n">ConvLSTM2D</span><span class="p">(</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos">350</span></a>            <span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl1</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="L-351"><a href="#L-351"><span class="linenos">351</span></a>            <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="L-352"><a href="#L-352"><span class="linenos">352</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">convlstm2</span> <span class="o">=</span> <span class="n">ConvLSTM2D</span><span class="p">(</span>
</span><span id="L-353"><a href="#L-353"><span class="linenos">353</span></a>            <span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl2</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="L-354"><a href="#L-354"><span class="linenos">354</span></a>            <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="L-355"><a href="#L-355"><span class="linenos">355</span></a>
</span><span id="L-356"><a href="#L-356"><span class="linenos">356</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos">357</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bn&#39;</span><span class="p">,</span> <span class="s1">&#39;ln&#39;</span><span class="p">]:</span>
</span><span id="L-358"><a href="#L-358"><span class="linenos">358</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Normalization not supported, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos">359</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;bn&#39;</span><span class="p">:</span>
</span><span id="L-360"><a href="#L-360"><span class="linenos">360</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos">361</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="L-362"><a href="#L-362"><span class="linenos">362</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;ln&#39;</span><span class="p">:</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos">363</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">()</span>
</span><span id="L-364"><a href="#L-364"><span class="linenos">364</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">()</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos">365</span></a>
</span><span id="L-366"><a href="#L-366"><span class="linenos">366</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</span><span id="L-367"><a href="#L-367"><span class="linenos">367</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">skipconnection</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()</span>
</span><span id="L-368"><a href="#L-368"><span class="linenos">368</span></a>
</span><span id="L-369"><a href="#L-369"><span class="linenos">369</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-370"><a href="#L-370"><span class="linenos">370</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos">371</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">get_dropout_layer</span><span class="p">(</span>
</span><span id="L-372"><a href="#L-372"><span class="linenos">372</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span><span id="L-373"><a href="#L-373"><span class="linenos">373</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">get_dropout_layer</span><span class="p">(</span>
</span><span id="L-374"><a href="#L-374"><span class="linenos">374</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>            
</span><span id="L-375"><a href="#L-375"><span class="linenos">375</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-376"><a href="#L-376"><span class="linenos">376</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos">377</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-378"><a href="#L-378"><span class="linenos">378</span></a>
</span><span id="L-379"><a href="#L-379"><span class="linenos">379</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="L-380"><a href="#L-380"><span class="linenos">380</span></a>        <span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-381"><a href="#L-381"><span class="linenos">381</span></a><span class="sd">        Forward pass. </span>
</span><span id="L-382"><a href="#L-382"><span class="linenos">382</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos">383</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="L-384"><a href="#L-384"><span class="linenos">384</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  
</span><span id="L-385"><a href="#L-385"><span class="linenos">385</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-386"><a href="#L-386"><span class="linenos">386</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span>
</span><span id="L-387"><a href="#L-387"><span class="linenos">387</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convlstm1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos">388</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos">389</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos">390</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos">391</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="L-392"><a href="#L-392"><span class="linenos">392</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-393"><a href="#L-393"><span class="linenos">393</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convlstm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-394"><a href="#L-394"><span class="linenos">394</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-395"><a href="#L-395"><span class="linenos">395</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos">396</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos">397</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span><span id="L-398"><a href="#L-398"><span class="linenos">398</span></a>
</span><span id="L-399"><a href="#L-399"><span class="linenos">399</span></a>
</span><span id="L-400"><a href="#L-400"><span class="linenos">400</span></a><span class="k">class</span> <span class="nc">SubpixelConvolutionBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="L-401"><a href="#L-401"><span class="linenos">401</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-402"><a href="#L-402"><span class="linenos">402</span></a><span class="sd">    Subpixel convolution (pixel shuffle) block.</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos">403</span></a>
</span><span id="L-404"><a href="#L-404"><span class="linenos">404</span></a><span class="sd">    References</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos">405</span></a><span class="sd">    ----------</span>
</span><span id="L-406"><a href="#L-406"><span class="linenos">406</span></a><span class="sd">    [1] Real-Time Single Image and Video Super-Resolution Using an Efficient </span>
</span><span id="L-407"><a href="#L-407"><span class="linenos">407</span></a><span class="sd">    Sub-Pixel Convolutional Neural Network: https://arxiv.org/abs/1609.05158</span>
</span><span id="L-408"><a href="#L-408"><span class="linenos">408</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos">409</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-410"><a href="#L-410"><span class="linenos">410</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;SubpixelConvolution&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="L-411"><a href="#L-411"><span class="linenos">411</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
</span><span id="L-412"><a href="#L-412"><span class="linenos">412</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">=</span> <span class="n">n_filters</span>
</span><span id="L-413"><a href="#L-413"><span class="linenos">413</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos">414</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="L-415"><a href="#L-415"><span class="linenos">415</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">*</span> <span class="p">(</span><span class="mi">5</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos">416</span></a>
</span><span id="L-417"><a href="#L-417"><span class="linenos">417</span></a>    <span class="k">def</span> <span class="nf">upsample_conv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">factor</span><span class="p">):</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos">418</span></a>        <span class="sd">&quot;&quot;&quot;Sub-pixel convolution (pixel shuffle)</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos">419</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-420"><a href="#L-420"><span class="linenos">420</span></a>        <span class="k">if</span> <span class="n">factor</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="L-421"><a href="#L-421"><span class="linenos">421</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-422"><a href="#L-422"><span class="linenos">422</span></a>        <span class="k">elif</span> <span class="n">factor</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
</span><span id="L-423"><a href="#L-423"><span class="linenos">423</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-424"><a href="#L-424"><span class="linenos">424</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-425"><a href="#L-425"><span class="linenos">425</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-426"><a href="#L-426"><span class="linenos">426</span></a>        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">depth_to_space</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">factor</span><span class="p">)</span>
</span><span id="L-427"><a href="#L-427"><span class="linenos">427</span></a>
</span><span id="L-428"><a href="#L-428"><span class="linenos">428</span></a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="L-429"><a href="#L-429"><span class="linenos">429</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> 
</span><span id="L-430"><a href="#L-430"><span class="linenos">430</span></a>                <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</span><span id="L-431"><a href="#L-431"><span class="linenos">431</span></a>
</span><span id="L-432"><a href="#L-432"><span class="linenos">432</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-433"><a href="#L-433"><span class="linenos">433</span></a>        <span class="sd">&quot;&quot;&quot;  Forward pass.</span>
</span><span id="L-434"><a href="#L-434"><span class="linenos">434</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-435"><a href="#L-435"><span class="linenos">435</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="L-436"><a href="#L-436"><span class="linenos">436</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-437"><a href="#L-437"><span class="linenos">437</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
</span><span id="L-438"><a href="#L-438"><span class="linenos">438</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-439"><a href="#L-439"><span class="linenos">439</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-440"><a href="#L-440"><span class="linenos">440</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
</span><span id="L-441"><a href="#L-441"><span class="linenos">441</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-442"><a href="#L-442"><span class="linenos">442</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-443"><a href="#L-443"><span class="linenos">443</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-444"><a href="#L-444"><span class="linenos">444</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
</span><span id="L-445"><a href="#L-445"><span class="linenos">445</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-446"><a href="#L-446"><span class="linenos">446</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span><span id="L-447"><a href="#L-447"><span class="linenos">447</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">20</span><span class="p">:</span>
</span><span id="L-448"><a href="#L-448"><span class="linenos">448</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-449"><a href="#L-449"><span class="linenos">449</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-450"><a href="#L-450"><span class="linenos">450</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span><span id="L-451"><a href="#L-451"><span class="linenos">451</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-452"><a href="#L-452"><span class="linenos">452</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span><span id="L-453"><a href="#L-453"><span class="linenos">453</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="L-454"><a href="#L-454"><span class="linenos">454</span></a>
</span><span id="L-455"><a href="#L-455"><span class="linenos">455</span></a>
</span><span id="L-456"><a href="#L-456"><span class="linenos">456</span></a><span class="k">class</span> <span class="nc">ResizeConvolutionBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="L-457"><a href="#L-457"><span class="linenos">457</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-458"><a href="#L-458"><span class="linenos">458</span></a><span class="sd">    Upsampling via bilinear interpolation followed by a 2D Convolution. </span>
</span><span id="L-459"><a href="#L-459"><span class="linenos">459</span></a>
</span><span id="L-460"><a href="#L-460"><span class="linenos">460</span></a><span class="sd">    Parameters</span>
</span><span id="L-461"><a href="#L-461"><span class="linenos">461</span></a><span class="sd">    ----------</span>
</span><span id="L-462"><a href="#L-462"><span class="linenos">462</span></a><span class="sd">    interpolation : str</span>
</span><span id="L-463"><a href="#L-463"><span class="linenos">463</span></a><span class="sd">        The interpolation method. Defaults to &quot;bilinear&quot;. Supports &quot;bilinear&quot;, </span>
</span><span id="L-464"><a href="#L-464"><span class="linenos">464</span></a><span class="sd">        &quot;nearest&quot;, &quot;bicubic&quot;, &quot;area&quot;, &quot;lanczos3&quot;, &quot;lanczos5&quot;, &quot;gaussian&quot;, </span>
</span><span id="L-465"><a href="#L-465"><span class="linenos">465</span></a><span class="sd">        &quot;mitchellcubic&quot;.</span>
</span><span id="L-466"><a href="#L-466"><span class="linenos">466</span></a>
</span><span id="L-467"><a href="#L-467"><span class="linenos">467</span></a><span class="sd">    References</span>
</span><span id="L-468"><a href="#L-468"><span class="linenos">468</span></a><span class="sd">    ----------</span>
</span><span id="L-469"><a href="#L-469"><span class="linenos">469</span></a><span class="sd">    [1] Deconvolution and Checkerboard Artifacts: </span>
</span><span id="L-470"><a href="#L-470"><span class="linenos">470</span></a><span class="sd">    https://distill.pub/2016/deconv-checkerboard/</span>
</span><span id="L-471"><a href="#L-471"><span class="linenos">471</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-472"><a href="#L-472"><span class="linenos">472</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> 
</span><span id="L-473"><a href="#L-473"><span class="linenos">473</span></a>                 <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-474"><a href="#L-474"><span class="linenos">474</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;ResizeConvolution&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="L-475"><a href="#L-475"><span class="linenos">475</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
</span><span id="L-476"><a href="#L-476"><span class="linenos">476</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">=</span> <span class="n">n_filters</span>
</span><span id="L-477"><a href="#L-477"><span class="linenos">477</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span> <span class="o">=</span> <span class="n">interpolation</span>
</span><span id="L-478"><a href="#L-478"><span class="linenos">478</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="L-479"><a href="#L-479"><span class="linenos">479</span></a>
</span><span id="L-480"><a href="#L-480"><span class="linenos">480</span></a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="L-481"><a href="#L-481"><span class="linenos">481</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> 
</span><span id="L-482"><a href="#L-482"><span class="linenos">482</span></a>                <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</span><span id="L-483"><a href="#L-483"><span class="linenos">483</span></a>
</span><span id="L-484"><a href="#L-484"><span class="linenos">484</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-485"><a href="#L-485"><span class="linenos">485</span></a>        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-486"><a href="#L-486"><span class="linenos">486</span></a>        <span class="n">height</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span><span id="L-487"><a href="#L-487"><span class="linenos">487</span></a>        <span class="n">width</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span><span id="L-488"><a href="#L-488"><span class="linenos">488</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">Resizing</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-489"><a href="#L-489"><span class="linenos">489</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="L-490"><a href="#L-490"><span class="linenos">490</span></a>        <span class="k">return</span> <span class="n">y</span>
</span><span id="L-491"><a href="#L-491"><span class="linenos">491</span></a>
</span><span id="L-492"><a href="#L-492"><span class="linenos">492</span></a>
</span><span id="L-493"><a href="#L-493"><span class="linenos">493</span></a><span class="k">class</span> <span class="nc">DeconvolutionBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="L-494"><a href="#L-494"><span class="linenos">494</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-495"><a href="#L-495"><span class="linenos">495</span></a><span class="sd">    Deconvolution or transposed convolution block.</span>
</span><span id="L-496"><a href="#L-496"><span class="linenos">496</span></a>
</span><span id="L-497"><a href="#L-497"><span class="linenos">497</span></a><span class="sd">    References</span>
</span><span id="L-498"><a href="#L-498"><span class="linenos">498</span></a><span class="sd">    ----------</span>
</span><span id="L-499"><a href="#L-499"><span class="linenos">499</span></a><span class="sd">    [1] FSRCNN - Accelerating the Super-Resolution Convolutional Neural Network: </span>
</span><span id="L-500"><a href="#L-500"><span class="linenos">500</span></a><span class="sd">    https://arxiv.org/abs/1608.00367</span>
</span><span id="L-501"><a href="#L-501"><span class="linenos">501</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-502"><a href="#L-502"><span class="linenos">502</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">output_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="L-503"><a href="#L-503"><span class="linenos">503</span></a>                 <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-504"><a href="#L-504"><span class="linenos">504</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Deconvolution&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="L-505"><a href="#L-505"><span class="linenos">505</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
</span><span id="L-506"><a href="#L-506"><span class="linenos">506</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_activation</span> <span class="o">=</span> <span class="n">output_activation</span>
</span><span id="L-507"><a href="#L-507"><span class="linenos">507</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose1</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> 
</span><span id="L-508"><a href="#L-508"><span class="linenos">508</span></a>            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;deconv_1of2_scale_x2&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-509"><a href="#L-509"><span class="linenos">509</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose2</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> 
</span><span id="L-510"><a href="#L-510"><span class="linenos">510</span></a>            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;deconv_2of2_scale_x2&#39;</span><span class="p">,</span> 
</span><span id="L-511"><a href="#L-511"><span class="linenos">511</span></a>            <span class="n">activation</span><span class="o">=</span><span class="n">output_activation</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-512"><a href="#L-512"><span class="linenos">512</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> 
</span><span id="L-513"><a href="#L-513"><span class="linenos">513</span></a>            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="L-514"><a href="#L-514"><span class="linenos">514</span></a>            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;deconv_scale_x&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> 
</span><span id="L-515"><a href="#L-515"><span class="linenos">515</span></a>            <span class="n">activation</span><span class="o">=</span><span class="n">output_activation</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-516"><a href="#L-516"><span class="linenos">516</span></a>
</span><span id="L-517"><a href="#L-517"><span class="linenos">517</span></a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="L-518"><a href="#L-518"><span class="linenos">518</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> 
</span><span id="L-519"><a href="#L-519"><span class="linenos">519</span></a>                <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</span><span id="L-520"><a href="#L-520"><span class="linenos">520</span></a>
</span><span id="L-521"><a href="#L-521"><span class="linenos">521</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-522"><a href="#L-522"><span class="linenos">522</span></a>        <span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-523"><a href="#L-523"><span class="linenos">523</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-524"><a href="#L-524"><span class="linenos">524</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
</span><span id="L-525"><a href="#L-525"><span class="linenos">525</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-526"><a href="#L-526"><span class="linenos">526</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-527"><a href="#L-527"><span class="linenos">527</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
</span><span id="L-528"><a href="#L-528"><span class="linenos">528</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-529"><a href="#L-529"><span class="linenos">529</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-530"><a href="#L-530"><span class="linenos">530</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-531"><a href="#L-531"><span class="linenos">531</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-532"><a href="#L-532"><span class="linenos">532</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-533"><a href="#L-533"><span class="linenos">533</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="L-534"><a href="#L-534"><span class="linenos">534</span></a>
</span><span id="L-535"><a href="#L-535"><span class="linenos">535</span></a>
</span><span id="L-536"><a href="#L-536"><span class="linenos">536</span></a><span class="k">class</span> <span class="nc">ChannelAttention2D</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="L-537"><a href="#L-537"><span class="linenos">537</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-538"><a href="#L-538"><span class="linenos">538</span></a><span class="sd">    Channel Attention for CNNs. Inputs need to be Conv2D feature maps.</span>
</span><span id="L-539"><a href="#L-539"><span class="linenos">539</span></a><span class="sd">    The layer implements the following:</span>
</span><span id="L-540"><a href="#L-540"><span class="linenos">540</span></a><span class="sd">        1. Average Pooling to create `[1,1,C]` vectors</span>
</span><span id="L-541"><a href="#L-541"><span class="linenos">541</span></a><span class="sd">        2. Conv2D with k=1 for fully connected features and relu ac</span>
</span><span id="L-542"><a href="#L-542"><span class="linenos">542</span></a><span class="sd">        3. Sigmoid activation to create attention maps</span>
</span><span id="L-543"><a href="#L-543"><span class="linenos">543</span></a>
</span><span id="L-544"><a href="#L-544"><span class="linenos">544</span></a><span class="sd">    Adapted from visual_attention_tf: </span>
</span><span id="L-545"><a href="#L-545"><span class="linenos">545</span></a><span class="sd">    https://github.com/vinayak19th/Visual_attention_tf/blob/main/src/visual_attention/channel_attention.py</span>
</span><span id="L-546"><a href="#L-546"><span class="linenos">546</span></a><span class="sd">    </span>
</span><span id="L-547"><a href="#L-547"><span class="linenos">547</span></a><span class="sd">    Parameters</span>
</span><span id="L-548"><a href="#L-548"><span class="linenos">548</span></a><span class="sd">    ----------</span>
</span><span id="L-549"><a href="#L-549"><span class="linenos">549</span></a><span class="sd">    nf [int]: number of filters or channels</span>
</span><span id="L-550"><a href="#L-550"><span class="linenos">550</span></a><span class="sd">    r[int] : Reduction factor</span>
</span><span id="L-551"><a href="#L-551"><span class="linenos">551</span></a><span class="sd">    </span>
</span><span id="L-552"><a href="#L-552"><span class="linenos">552</span></a><span class="sd">    Input</span>
</span><span id="L-553"><a href="#L-553"><span class="linenos">553</span></a><span class="sd">    -----</span>
</span><span id="L-554"><a href="#L-554"><span class="linenos">554</span></a><span class="sd">    Feature maps : Conv2D feature maps of the shape `[batch,W,H,C]`.</span>
</span><span id="L-555"><a href="#L-555"><span class="linenos">555</span></a><span class="sd">    </span>
</span><span id="L-556"><a href="#L-556"><span class="linenos">556</span></a><span class="sd">    Output</span>
</span><span id="L-557"><a href="#L-557"><span class="linenos">557</span></a><span class="sd">    ------</span>
</span><span id="L-558"><a href="#L-558"><span class="linenos">558</span></a><span class="sd">    Attention activated Conv2D features of shape `[batch,W,H,C]`.</span>
</span><span id="L-559"><a href="#L-559"><span class="linenos">559</span></a><span class="sd">    </span>
</span><span id="L-560"><a href="#L-560"><span class="linenos">560</span></a><span class="sd">    Reference</span>
</span><span id="L-561"><a href="#L-561"><span class="linenos">561</span></a><span class="sd">    ---------</span>
</span><span id="L-562"><a href="#L-562"><span class="linenos">562</span></a><span class="sd">    CBAM: Convolutional Block Attention Module (Sanghyun Woo et al 2018): </span>
</span><span id="L-563"><a href="#L-563"><span class="linenos">563</span></a><span class="sd">    https://arxiv.org/abs/1807.06521</span>
</span><span id="L-564"><a href="#L-564"><span class="linenos">564</span></a>
</span><span id="L-565"><a href="#L-565"><span class="linenos">565</span></a><span class="sd">    Notes</span>
</span><span id="L-566"><a href="#L-566"><span class="linenos">566</span></a><span class="sd">    -----</span>
</span><span id="L-567"><a href="#L-567"><span class="linenos">567</span></a><span class="sd">    Here is a code example for using `ChannelAttention2D` in a CNN:</span>
</span><span id="L-568"><a href="#L-568"><span class="linenos">568</span></a><span class="sd">    ```python</span>
</span><span id="L-569"><a href="#L-569"><span class="linenos">569</span></a><span class="sd">    inp = Input(shape=(1920,1080,3))</span>
</span><span id="L-570"><a href="#L-570"><span class="linenos">570</span></a><span class="sd">    cnn_layer = Conv2D(32,3,,activation=&#39;relu&#39;, padding=&#39;same&#39;)(inp)</span>
</span><span id="L-571"><a href="#L-571"><span class="linenos">571</span></a><span class="sd">    # Using the .shape[-1] to simplify network modifications. Can directly input number of channels as well</span>
</span><span id="L-572"><a href="#L-572"><span class="linenos">572</span></a><span class="sd">    attention_cnn = ChannelAttention2D(cnn_layer.shape[-1],cnn_layer.shape[1:-1])(cnn_layer)</span>
</span><span id="L-573"><a href="#L-573"><span class="linenos">573</span></a><span class="sd">    #ADD DNN layers .....</span>
</span><span id="L-574"><a href="#L-574"><span class="linenos">574</span></a><span class="sd">    ```</span>
</span><span id="L-575"><a href="#L-575"><span class="linenos">575</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-576"><a href="#L-576"><span class="linenos">576</span></a>
</span><span id="L-577"><a href="#L-577"><span class="linenos">577</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="L-578"><a href="#L-578"><span class="linenos">578</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="L-579"><a href="#L-579"><span class="linenos">579</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">nf</span> <span class="o">=</span> <span class="n">nf</span>
</span><span id="L-580"><a href="#L-580"><span class="linenos">580</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">r</span>
</span><span id="L-581"><a href="#L-581"><span class="linenos">581</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">nf</span><span class="o">/</span><span class="n">r</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-582"><a href="#L-582"><span class="linenos">582</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">nf</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-583"><a href="#L-583"><span class="linenos">583</span></a>
</span><span id="L-584"><a href="#L-584"><span class="linenos">584</span></a>    <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
</span><span id="L-585"><a href="#L-585"><span class="linenos">585</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-586"><a href="#L-586"><span class="linenos">586</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-587"><a href="#L-587"><span class="linenos">587</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="L-588"><a href="#L-588"><span class="linenos">588</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="L-589"><a href="#L-589"><span class="linenos">589</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="L-590"><a href="#L-590"><span class="linenos">590</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="L-591"><a href="#L-591"><span class="linenos">591</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="L-592"><a href="#L-592"><span class="linenos">592</span></a>        <span class="k">return</span> <span class="n">y</span>
</span><span id="L-593"><a href="#L-593"><span class="linenos">593</span></a>
</span><span id="L-594"><a href="#L-594"><span class="linenos">594</span></a>    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="L-595"><a href="#L-595"><span class="linenos">595</span></a>        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
</span><span id="L-596"><a href="#L-596"><span class="linenos">596</span></a>        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;Att_filters&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">nf</span><span class="p">})</span>
</span><span id="L-597"><a href="#L-597"><span class="linenos">597</span></a>        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;Red_factor&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">})</span>
</span><span id="L-598"><a href="#L-598"><span class="linenos">598</span></a>        <span class="k">return</span> <span class="n">config</span>
</span><span id="L-599"><a href="#L-599"><span class="linenos">599</span></a>
</span><span id="L-600"><a href="#L-600"><span class="linenos">600</span></a>
</span><span id="L-601"><a href="#L-601"><span class="linenos">601</span></a><span class="k">class</span> <span class="nc">EncoderBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="L-602"><a href="#L-602"><span class="linenos">602</span></a>    <span class="sd">&quot;&quot;&quot;Encoder block for a decoder-encoder architecture, such as the UNET.</span>
</span><span id="L-603"><a href="#L-603"><span class="linenos">603</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-604"><a href="#L-604"><span class="linenos">604</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="L-605"><a href="#L-605"><span class="linenos">605</span></a>                 <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-606"><a href="#L-606"><span class="linenos">606</span></a>                 <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
</span><span id="L-607"><a href="#L-607"><span class="linenos">607</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;EncoderBlock&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="L-608"><a href="#L-608"><span class="linenos">608</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">ConvBlock</span><span class="p">(</span>
</span><span id="L-609"><a href="#L-609"><span class="linenos">609</span></a>            <span class="n">n_filters</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> 
</span><span id="L-610"><a href="#L-610"><span class="linenos">610</span></a>            <span class="n">dropout_variant</span><span class="o">=</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="n">normalization</span><span class="p">,</span> 
</span><span id="L-611"><a href="#L-611"><span class="linenos">611</span></a>            <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">)</span>
</span><span id="L-612"><a href="#L-612"><span class="linenos">612</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span><span id="L-613"><a href="#L-613"><span class="linenos">613</span></a>
</span><span id="L-614"><a href="#L-614"><span class="linenos">614</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="L-615"><a href="#L-615"><span class="linenos">615</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="L-616"><a href="#L-616"><span class="linenos">616</span></a>        <span class="n">Y_downsampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="L-617"><a href="#L-617"><span class="linenos">617</span></a>        <span class="k">return</span> <span class="p">[</span><span class="n">Y_downsampled</span><span class="p">,</span> <span class="n">Y</span><span class="p">]</span>
</span><span id="L-618"><a href="#L-618"><span class="linenos">618</span></a>
</span><span id="L-619"><a href="#L-619"><span class="linenos">619</span></a>
</span><span id="L-620"><a href="#L-620"><span class="linenos">620</span></a><span class="k">class</span> <span class="nc">PadConcat</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="L-621"><a href="#L-621"><span class="linenos">621</span></a>    <span class="sd">&quot;&quot;&quot;Concatenate layer that takes two tensors, if needed it pads to match </span>
</span><span id="L-622"><a href="#L-622"><span class="linenos">622</span></a><span class="sd">    height and width. </span>
</span><span id="L-623"><a href="#L-623"><span class="linenos">623</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-624"><a href="#L-624"><span class="linenos">624</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
</span><span id="L-625"><a href="#L-625"><span class="linenos">625</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Concatenate&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="L-626"><a href="#L-626"><span class="linenos">626</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">debug</span> <span class="o">=</span> <span class="n">debug</span>
</span><span id="L-627"><a href="#L-627"><span class="linenos">627</span></a>
</span><span id="L-628"><a href="#L-628"><span class="linenos">628</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="L-629"><a href="#L-629"><span class="linenos">629</span></a>        <span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">)</span> <span class="o">=</span> <span class="n">X</span>
</span><span id="L-630"><a href="#L-630"><span class="linenos">630</span></a>        <span class="n">y1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-631"><a href="#L-631"><span class="linenos">631</span></a>        <span class="n">x1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="L-632"><a href="#L-632"><span class="linenos">632</span></a>        <span class="n">y2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-633"><a href="#L-633"><span class="linenos">633</span></a>        <span class="n">x2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="L-634"><a href="#L-634"><span class="linenos">634</span></a>
</span><span id="L-635"><a href="#L-635"><span class="linenos">635</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span>
</span><span id="L-636"><a href="#L-636"><span class="linenos">636</span></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;input1 (</span><span class="si">{</span><span class="n">y1</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">x1</span><span class="si">}</span><span class="s1">) input2 (</span><span class="si">{</span><span class="n">y2</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">x2</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
</span><span id="L-637"><a href="#L-637"><span class="linenos">637</span></a>
</span><span id="L-638"><a href="#L-638"><span class="linenos">638</span></a>        <span class="k">if</span> <span class="n">y2</span> <span class="o">&lt;</span> <span class="n">y1</span><span class="p">:</span>
</span><span id="L-639"><a href="#L-639"><span class="linenos">639</span></a>            <span class="n">t2</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">y1</span> <span class="o">-</span> <span class="n">y2</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))(</span><span class="n">t2</span><span class="p">)</span>
</span><span id="L-640"><a href="#L-640"><span class="linenos">640</span></a>        <span class="k">elif</span> <span class="n">y2</span> <span class="o">&gt;</span> <span class="n">y1</span><span class="p">:</span>
</span><span id="L-641"><a href="#L-641"><span class="linenos">641</span></a>            <span class="n">t1</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">y2</span> <span class="o">-</span> <span class="n">y1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))(</span><span class="n">t1</span><span class="p">)</span>
</span><span id="L-642"><a href="#L-642"><span class="linenos">642</span></a>
</span><span id="L-643"><a href="#L-643"><span class="linenos">643</span></a>        <span class="k">if</span> <span class="n">x2</span> <span class="o">&lt;</span> <span class="n">x1</span><span class="p">:</span>
</span><span id="L-644"><a href="#L-644"><span class="linenos">644</span></a>            <span class="n">t2</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)))(</span><span class="n">t2</span><span class="p">)</span>
</span><span id="L-645"><a href="#L-645"><span class="linenos">645</span></a>        <span class="k">elif</span> <span class="n">x2</span> <span class="o">&gt;</span> <span class="n">x1</span><span class="p">:</span>
</span><span id="L-646"><a href="#L-646"><span class="linenos">646</span></a>            <span class="n">t1</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="p">)))(</span><span class="n">t1</span><span class="p">)</span>
</span><span id="L-647"><a href="#L-647"><span class="linenos">647</span></a>
</span><span id="L-648"><a href="#L-648"><span class="linenos">648</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span>
</span><span id="L-649"><a href="#L-649"><span class="linenos">649</span></a>            <span class="n">y1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-650"><a href="#L-650"><span class="linenos">650</span></a>            <span class="n">x1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="L-651"><a href="#L-651"><span class="linenos">651</span></a>            <span class="n">y2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-652"><a href="#L-652"><span class="linenos">652</span></a>            <span class="n">x2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="L-653"><a href="#L-653"><span class="linenos">653</span></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;output1 (</span><span class="si">{</span><span class="n">y1</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">x1</span><span class="si">}</span><span class="s1">) output2 (</span><span class="si">{</span><span class="n">y2</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">x2</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
</span><span id="L-654"><a href="#L-654"><span class="linenos">654</span></a>
</span><span id="L-655"><a href="#L-655"><span class="linenos">655</span></a>        <span class="k">return</span> <span class="n">Concatenate</span><span class="p">()([</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">])</span>
</span><span id="L-656"><a href="#L-656"><span class="linenos">656</span></a>
</span><span id="L-657"><a href="#L-657"><span class="linenos">657</span></a>
</span><span id="L-658"><a href="#L-658"><span class="linenos">658</span></a><span class="k">class</span> <span class="nc">MCDropout</span><span class="p">(</span><span class="n">Dropout</span><span class="p">):</span>
</span><span id="L-659"><a href="#L-659"><span class="linenos">659</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="L-660"><a href="#L-660"><span class="linenos">660</span></a>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-661"><a href="#L-661"><span class="linenos">661</span></a>
</span><span id="L-662"><a href="#L-662"><span class="linenos">662</span></a>
</span><span id="L-663"><a href="#L-663"><span class="linenos">663</span></a><span class="k">class</span> <span class="nc">MCGaussianDropout</span><span class="p">(</span><span class="n">GaussianDropout</span><span class="p">):</span>
</span><span id="L-664"><a href="#L-664"><span class="linenos">664</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="L-665"><a href="#L-665"><span class="linenos">665</span></a>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-666"><a href="#L-666"><span class="linenos">666</span></a>
</span><span id="L-667"><a href="#L-667"><span class="linenos">667</span></a>
</span><span id="L-668"><a href="#L-668"><span class="linenos">668</span></a><span class="k">class</span> <span class="nc">MCSpatialDropout2D</span><span class="p">(</span><span class="n">SpatialDropout2D</span><span class="p">):</span>
</span><span id="L-669"><a href="#L-669"><span class="linenos">669</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="L-670"><a href="#L-670"><span class="linenos">670</span></a>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-671"><a href="#L-671"><span class="linenos">671</span></a>
</span><span id="L-672"><a href="#L-672"><span class="linenos">672</span></a>
</span><span id="L-673"><a href="#L-673"><span class="linenos">673</span></a><span class="k">class</span> <span class="nc">MCSpatialDropout3D</span><span class="p">(</span><span class="n">SpatialDropout3D</span><span class="p">):</span>
</span><span id="L-674"><a href="#L-674"><span class="linenos">674</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="L-675"><a href="#L-675"><span class="linenos">675</span></a>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-676"><a href="#L-676"><span class="linenos">676</span></a>
</span><span id="L-677"><a href="#L-677"><span class="linenos">677</span></a>
</span><span id="L-678"><a href="#L-678"><span class="linenos">678</span></a><span class="k">def</span> <span class="nf">get_dropout_layer</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
</span><span id="L-679"><a href="#L-679"><span class="linenos">679</span></a>    <span class="sd">&quot;&quot;&quot;Choose an return a dropout layer depending on the input arguments. If</span>
</span><span id="L-680"><a href="#L-680"><span class="linenos">680</span></a><span class="sd">    ``dropout_rate=0`` then an identity layer is returned (the input tensor </span>
</span><span id="L-681"><a href="#L-681"><span class="linenos">681</span></a><span class="sd">    is returned without any modification). </span>
</span><span id="L-682"><a href="#L-682"><span class="linenos">682</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-683"><a href="#L-683"><span class="linenos">683</span></a>    <span class="n">dropout_variant</span> <span class="o">=</span> <span class="n">checkarg_dropout_variant</span><span class="p">(</span><span class="n">dropout_variant</span><span class="p">)</span>
</span><span id="L-684"><a href="#L-684"><span class="linenos">684</span></a>    <span class="k">if</span> <span class="n">dropout_rate</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-685"><a href="#L-685"><span class="linenos">685</span></a>        <span class="k">if</span> <span class="n">dropout_variant</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">dropout_variant</span> <span class="o">==</span> <span class="s1">&#39;vanilla&#39;</span><span class="p">:</span>
</span><span id="L-686"><a href="#L-686"><span class="linenos">686</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="L-687"><a href="#L-687"><span class="linenos">687</span></a>        <span class="k">elif</span> <span class="n">dropout_variant</span> <span class="o">==</span> <span class="s1">&#39;gaussian&#39;</span><span class="p">:</span>
</span><span id="L-688"><a href="#L-688"><span class="linenos">688</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="n">GaussianDropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="L-689"><a href="#L-689"><span class="linenos">689</span></a>        <span class="k">elif</span> <span class="n">dropout_variant</span> <span class="o">==</span> <span class="s1">&#39;spatial&#39;</span><span class="p">:</span>
</span><span id="L-690"><a href="#L-690"><span class="linenos">690</span></a>            <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="L-691"><a href="#L-691"><span class="linenos">691</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="n">SpatialDropout2D</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="L-692"><a href="#L-692"><span class="linenos">692</span></a>            <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
</span><span id="L-693"><a href="#L-693"><span class="linenos">693</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="n">SpatialDropout3D</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="L-694"><a href="#L-694"><span class="linenos">694</span></a>        <span class="k">elif</span> <span class="n">dropout_variant</span> <span class="o">==</span> <span class="s1">&#39;mcdrop&#39;</span><span class="p">:</span>
</span><span id="L-695"><a href="#L-695"><span class="linenos">695</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="n">MCDropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="L-696"><a href="#L-696"><span class="linenos">696</span></a>        <span class="k">elif</span> <span class="n">dropout_variant</span> <span class="o">==</span> <span class="s1">&#39;mcgaussiandrop&#39;</span><span class="p">:</span>
</span><span id="L-697"><a href="#L-697"><span class="linenos">697</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="n">MCGaussianDropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="L-698"><a href="#L-698"><span class="linenos">698</span></a>        <span class="k">elif</span> <span class="n">dropout_variant</span> <span class="o">==</span> <span class="s1">&#39;mcspatialdrop&#39;</span><span class="p">:</span>
</span><span id="L-699"><a href="#L-699"><span class="linenos">699</span></a>            <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="L-700"><a href="#L-700"><span class="linenos">700</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="n">MCSpatialDropout2D</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="L-701"><a href="#L-701"><span class="linenos">701</span></a>            <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
</span><span id="L-702"><a href="#L-702"><span class="linenos">702</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="n">MCSpatialDropout3D</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="L-703"><a href="#L-703"><span class="linenos">703</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="L-704"><a href="#L-704"><span class="linenos">704</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="L-705"><a href="#L-705"><span class="linenos">705</span></a>    <span class="k">return</span> <span class="n">layer</span>
</span></pre></div>

        </details>

            </section>
                <section id="ConvBlock">
                                <div class="attr class">
        <a class="headerlink" href="#ConvBlock">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">ConvBlock</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConvBlock-13"><a href="#ConvBlock-13"><span class="linenos"> 13</span></a><span class="k">class</span> <span class="nc">ConvBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span> 
</span><span id="ConvBlock-14"><a href="#ConvBlock-14"><span class="linenos"> 14</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="ConvBlock-15"><a href="#ConvBlock-15"><span class="linenos"> 15</span></a><span class="sd">    Convolutional block.</span>
</span><span id="ConvBlock-16"><a href="#ConvBlock-16"><span class="linenos"> 16</span></a>
</span><span id="ConvBlock-17"><a href="#ConvBlock-17"><span class="linenos"> 17</span></a><span class="sd">    References</span>
</span><span id="ConvBlock-18"><a href="#ConvBlock-18"><span class="linenos"> 18</span></a><span class="sd">    ----------</span>
</span><span id="ConvBlock-19"><a href="#ConvBlock-19"><span class="linenos"> 19</span></a><span class="sd">    [1] Effective and Efficient Dropout for Deep Convolutional Neural Networks: </span>
</span><span id="ConvBlock-20"><a href="#ConvBlock-20"><span class="linenos"> 20</span></a><span class="sd">    https://arxiv.org/abs/1904.03392</span>
</span><span id="ConvBlock-21"><a href="#ConvBlock-21"><span class="linenos"> 21</span></a><span class="sd">    [2] Rethinking the Usage of Batch Normalization and Dropout: </span>
</span><span id="ConvBlock-22"><a href="#ConvBlock-22"><span class="linenos"> 22</span></a><span class="sd">    https://arxiv.org/abs/1905.05928</span>
</span><span id="ConvBlock-23"><a href="#ConvBlock-23"><span class="linenos"> 23</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ConvBlock-24"><a href="#ConvBlock-24"><span class="linenos"> 24</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">ks_cl2</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> 
</span><span id="ConvBlock-25"><a href="#ConvBlock-25"><span class="linenos"> 25</span></a>                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span><span id="ConvBlock-26"><a href="#ConvBlock-26"><span class="linenos"> 26</span></a>                 <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="ConvBlock-27"><a href="#ConvBlock-27"><span class="linenos"> 27</span></a>                 <span class="n">depthwise_separable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="ConvBlock-28"><a href="#ConvBlock-28"><span class="linenos"> 28</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</span><span id="ConvBlock-29"><a href="#ConvBlock-29"><span class="linenos"> 29</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
</span><span id="ConvBlock-30"><a href="#ConvBlock-30"><span class="linenos"> 30</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span>
</span><span id="ConvBlock-31"><a href="#ConvBlock-31"><span class="linenos"> 31</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span> <span class="o">=</span> <span class="n">dropout_variant</span>
</span><span id="ConvBlock-32"><a href="#ConvBlock-32"><span class="linenos"> 32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>
</span><span id="ConvBlock-33"><a href="#ConvBlock-33"><span class="linenos"> 33</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_separable</span> <span class="o">=</span> <span class="n">depthwise_separable</span>
</span><span id="ConvBlock-34"><a href="#ConvBlock-34"><span class="linenos"> 34</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_separable</span><span class="p">:</span>
</span><span id="ConvBlock-35"><a href="#ConvBlock-35"><span class="linenos"> 35</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span>
</span><span id="ConvBlock-36"><a href="#ConvBlock-36"><span class="linenos"> 36</span></a>                <span class="n">filters</span><span class="p">,</span> 
</span><span id="ConvBlock-37"><a href="#ConvBlock-37"><span class="linenos"> 37</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl1</span><span class="p">,</span> 
</span><span id="ConvBlock-38"><a href="#ConvBlock-38"><span class="linenos"> 38</span></a>                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="ConvBlock-39"><a href="#ConvBlock-39"><span class="linenos"> 39</span></a>                <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> 
</span><span id="ConvBlock-40"><a href="#ConvBlock-40"><span class="linenos"> 40</span></a>                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConvBlock-41"><a href="#ConvBlock-41"><span class="linenos"> 41</span></a>                <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="ConvBlock-42"><a href="#ConvBlock-42"><span class="linenos"> 42</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span>
</span><span id="ConvBlock-43"><a href="#ConvBlock-43"><span class="linenos"> 43</span></a>                <span class="n">filters</span><span class="p">,</span> 
</span><span id="ConvBlock-44"><a href="#ConvBlock-44"><span class="linenos"> 44</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl2</span><span class="p">,</span> 
</span><span id="ConvBlock-45"><a href="#ConvBlock-45"><span class="linenos"> 45</span></a>                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="ConvBlock-46"><a href="#ConvBlock-46"><span class="linenos"> 46</span></a>                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConvBlock-47"><a href="#ConvBlock-47"><span class="linenos"> 47</span></a>                <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="ConvBlock-48"><a href="#ConvBlock-48"><span class="linenos"> 48</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConvBlock-49"><a href="#ConvBlock-49"><span class="linenos"> 49</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span>
</span><span id="ConvBlock-50"><a href="#ConvBlock-50"><span class="linenos"> 50</span></a>                <span class="n">filters</span><span class="p">,</span> 
</span><span id="ConvBlock-51"><a href="#ConvBlock-51"><span class="linenos"> 51</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl1</span><span class="p">,</span> 
</span><span id="ConvBlock-52"><a href="#ConvBlock-52"><span class="linenos"> 52</span></a>                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="ConvBlock-53"><a href="#ConvBlock-53"><span class="linenos"> 53</span></a>                <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> 
</span><span id="ConvBlock-54"><a href="#ConvBlock-54"><span class="linenos"> 54</span></a>                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConvBlock-55"><a href="#ConvBlock-55"><span class="linenos"> 55</span></a>                <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="ConvBlock-56"><a href="#ConvBlock-56"><span class="linenos"> 56</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span>
</span><span id="ConvBlock-57"><a href="#ConvBlock-57"><span class="linenos"> 57</span></a>                <span class="n">filters</span><span class="p">,</span> 
</span><span id="ConvBlock-58"><a href="#ConvBlock-58"><span class="linenos"> 58</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl2</span><span class="p">,</span> 
</span><span id="ConvBlock-59"><a href="#ConvBlock-59"><span class="linenos"> 59</span></a>                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="ConvBlock-60"><a href="#ConvBlock-60"><span class="linenos"> 60</span></a>                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConvBlock-61"><a href="#ConvBlock-61"><span class="linenos"> 61</span></a>                <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="ConvBlock-62"><a href="#ConvBlock-62"><span class="linenos"> 62</span></a>
</span><span id="ConvBlock-63"><a href="#ConvBlock-63"><span class="linenos"> 63</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConvBlock-64"><a href="#ConvBlock-64"><span class="linenos"> 64</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bn&#39;</span><span class="p">,</span> <span class="s1">&#39;ln&#39;</span><span class="p">]:</span>
</span><span id="ConvBlock-65"><a href="#ConvBlock-65"><span class="linenos"> 65</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Normalization not supported, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span><span id="ConvBlock-66"><a href="#ConvBlock-66"><span class="linenos"> 66</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;bn&#39;</span><span class="p">:</span>
</span><span id="ConvBlock-67"><a href="#ConvBlock-67"><span class="linenos"> 67</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="ConvBlock-68"><a href="#ConvBlock-68"><span class="linenos"> 68</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="ConvBlock-69"><a href="#ConvBlock-69"><span class="linenos"> 69</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;ln&#39;</span><span class="p">:</span>
</span><span id="ConvBlock-70"><a href="#ConvBlock-70"><span class="linenos"> 70</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">()</span>
</span><span id="ConvBlock-71"><a href="#ConvBlock-71"><span class="linenos"> 71</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">()</span>
</span><span id="ConvBlock-72"><a href="#ConvBlock-72"><span class="linenos"> 72</span></a>
</span><span id="ConvBlock-73"><a href="#ConvBlock-73"><span class="linenos"> 73</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">:</span>
</span><span id="ConvBlock-74"><a href="#ConvBlock-74"><span class="linenos"> 74</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">att</span> <span class="o">=</span> <span class="n">ChannelAttention2D</span><span class="p">(</span><span class="n">filters</span><span class="p">)</span>
</span><span id="ConvBlock-75"><a href="#ConvBlock-75"><span class="linenos"> 75</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</span><span id="ConvBlock-76"><a href="#ConvBlock-76"><span class="linenos"> 76</span></a>        
</span><span id="ConvBlock-77"><a href="#ConvBlock-77"><span class="linenos"> 77</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="ConvBlock-78"><a href="#ConvBlock-78"><span class="linenos"> 78</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="ConvBlock-79"><a href="#ConvBlock-79"><span class="linenos"> 79</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">get_dropout_layer</span><span class="p">(</span>
</span><span id="ConvBlock-80"><a href="#ConvBlock-80"><span class="linenos"> 80</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="ConvBlock-81"><a href="#ConvBlock-81"><span class="linenos"> 81</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">get_dropout_layer</span><span class="p">(</span>
</span><span id="ConvBlock-82"><a href="#ConvBlock-82"><span class="linenos"> 82</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="ConvBlock-83"><a href="#ConvBlock-83"><span class="linenos"> 83</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="ConvBlock-84"><a href="#ConvBlock-84"><span class="linenos"> 84</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConvBlock-85"><a href="#ConvBlock-85"><span class="linenos"> 85</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="ConvBlock-86"><a href="#ConvBlock-86"><span class="linenos"> 86</span></a>
</span><span id="ConvBlock-87"><a href="#ConvBlock-87"><span class="linenos"> 87</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="ConvBlock-88"><a href="#ConvBlock-88"><span class="linenos"> 88</span></a>        <span class="sd">&quot;&quot;&quot;Model&#39;s forward pass.</span>
</span><span id="ConvBlock-89"><a href="#ConvBlock-89"><span class="linenos"> 89</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ConvBlock-90"><a href="#ConvBlock-90"><span class="linenos"> 90</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="k">else</span> <span class="n">X</span>
</span><span id="ConvBlock-91"><a href="#ConvBlock-91"><span class="linenos"> 91</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock-92"><a href="#ConvBlock-92"><span class="linenos"> 92</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConvBlock-93"><a href="#ConvBlock-93"><span class="linenos"> 93</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock-94"><a href="#ConvBlock-94"><span class="linenos"> 94</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock-95"><a href="#ConvBlock-95"><span class="linenos"> 95</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="ConvBlock-96"><a href="#ConvBlock-96"><span class="linenos"> 96</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock-97"><a href="#ConvBlock-97"><span class="linenos"> 97</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock-98"><a href="#ConvBlock-98"><span class="linenos"> 98</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConvBlock-99"><a href="#ConvBlock-99"><span class="linenos"> 99</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock-100"><a href="#ConvBlock-100"><span class="linenos">100</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock-101"><a href="#ConvBlock-101"><span class="linenos">101</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">:</span>
</span><span id="ConvBlock-102"><a href="#ConvBlock-102"><span class="linenos">102</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock-103"><a href="#ConvBlock-103"><span class="linenos">103</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Convolutional block.</p>

<h6 id="references">References</h6>

<p>[1] Effective and Efficient Dropout for Deep Convolutional Neural Networks: 
https://arxiv.org/abs/1904.03392
[2] Rethinking the Usage of Batch Normalization and Dropout: 
https://arxiv.org/abs/1905.05928</p>
</div>


                            <div id="ConvBlock.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ConvBlock.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">ConvBlock</span><span class="signature">(
    filters,
    strides=1,
    ks_cl1=(3, 3),
    ks_cl2=(3, 3),
    activation=&#39;relu&#39;,
    normalization=None,
    attention=False,
    dropout_rate=0,
    dropout_variant=None,
    depthwise_separable=False,
    name=None,
    **conv_kwargs
)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConvBlock.__init__-24"><a href="#ConvBlock.__init__-24"><span class="linenos">24</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">ks_cl2</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> 
</span><span id="ConvBlock.__init__-25"><a href="#ConvBlock.__init__-25"><span class="linenos">25</span></a>                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-26"><a href="#ConvBlock.__init__-26"><span class="linenos">26</span></a>                 <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-27"><a href="#ConvBlock.__init__-27"><span class="linenos">27</span></a>                 <span class="n">depthwise_separable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="ConvBlock.__init__-28"><a href="#ConvBlock.__init__-28"><span class="linenos">28</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</span><span id="ConvBlock.__init__-29"><a href="#ConvBlock.__init__-29"><span class="linenos">29</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
</span><span id="ConvBlock.__init__-30"><a href="#ConvBlock.__init__-30"><span class="linenos">30</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span>
</span><span id="ConvBlock.__init__-31"><a href="#ConvBlock.__init__-31"><span class="linenos">31</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span> <span class="o">=</span> <span class="n">dropout_variant</span>
</span><span id="ConvBlock.__init__-32"><a href="#ConvBlock.__init__-32"><span class="linenos">32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>
</span><span id="ConvBlock.__init__-33"><a href="#ConvBlock.__init__-33"><span class="linenos">33</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_separable</span> <span class="o">=</span> <span class="n">depthwise_separable</span>
</span><span id="ConvBlock.__init__-34"><a href="#ConvBlock.__init__-34"><span class="linenos">34</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_separable</span><span class="p">:</span>
</span><span id="ConvBlock.__init__-35"><a href="#ConvBlock.__init__-35"><span class="linenos">35</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span>
</span><span id="ConvBlock.__init__-36"><a href="#ConvBlock.__init__-36"><span class="linenos">36</span></a>                <span class="n">filters</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-37"><a href="#ConvBlock.__init__-37"><span class="linenos">37</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl1</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-38"><a href="#ConvBlock.__init__-38"><span class="linenos">38</span></a>                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-39"><a href="#ConvBlock.__init__-39"><span class="linenos">39</span></a>                <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-40"><a href="#ConvBlock.__init__-40"><span class="linenos">40</span></a>                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConvBlock.__init__-41"><a href="#ConvBlock.__init__-41"><span class="linenos">41</span></a>                <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="ConvBlock.__init__-42"><a href="#ConvBlock.__init__-42"><span class="linenos">42</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span>
</span><span id="ConvBlock.__init__-43"><a href="#ConvBlock.__init__-43"><span class="linenos">43</span></a>                <span class="n">filters</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-44"><a href="#ConvBlock.__init__-44"><span class="linenos">44</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl2</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-45"><a href="#ConvBlock.__init__-45"><span class="linenos">45</span></a>                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-46"><a href="#ConvBlock.__init__-46"><span class="linenos">46</span></a>                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConvBlock.__init__-47"><a href="#ConvBlock.__init__-47"><span class="linenos">47</span></a>                <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="ConvBlock.__init__-48"><a href="#ConvBlock.__init__-48"><span class="linenos">48</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConvBlock.__init__-49"><a href="#ConvBlock.__init__-49"><span class="linenos">49</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span>
</span><span id="ConvBlock.__init__-50"><a href="#ConvBlock.__init__-50"><span class="linenos">50</span></a>                <span class="n">filters</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-51"><a href="#ConvBlock.__init__-51"><span class="linenos">51</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl1</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-52"><a href="#ConvBlock.__init__-52"><span class="linenos">52</span></a>                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-53"><a href="#ConvBlock.__init__-53"><span class="linenos">53</span></a>                <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-54"><a href="#ConvBlock.__init__-54"><span class="linenos">54</span></a>                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConvBlock.__init__-55"><a href="#ConvBlock.__init__-55"><span class="linenos">55</span></a>                <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="ConvBlock.__init__-56"><a href="#ConvBlock.__init__-56"><span class="linenos">56</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span>
</span><span id="ConvBlock.__init__-57"><a href="#ConvBlock.__init__-57"><span class="linenos">57</span></a>                <span class="n">filters</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-58"><a href="#ConvBlock.__init__-58"><span class="linenos">58</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl2</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-59"><a href="#ConvBlock.__init__-59"><span class="linenos">59</span></a>                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="ConvBlock.__init__-60"><a href="#ConvBlock.__init__-60"><span class="linenos">60</span></a>                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ConvBlock.__init__-61"><a href="#ConvBlock.__init__-61"><span class="linenos">61</span></a>                <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="ConvBlock.__init__-62"><a href="#ConvBlock.__init__-62"><span class="linenos">62</span></a>
</span><span id="ConvBlock.__init__-63"><a href="#ConvBlock.__init__-63"><span class="linenos">63</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConvBlock.__init__-64"><a href="#ConvBlock.__init__-64"><span class="linenos">64</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bn&#39;</span><span class="p">,</span> <span class="s1">&#39;ln&#39;</span><span class="p">]:</span>
</span><span id="ConvBlock.__init__-65"><a href="#ConvBlock.__init__-65"><span class="linenos">65</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Normalization not supported, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span><span id="ConvBlock.__init__-66"><a href="#ConvBlock.__init__-66"><span class="linenos">66</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;bn&#39;</span><span class="p">:</span>
</span><span id="ConvBlock.__init__-67"><a href="#ConvBlock.__init__-67"><span class="linenos">67</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="ConvBlock.__init__-68"><a href="#ConvBlock.__init__-68"><span class="linenos">68</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="ConvBlock.__init__-69"><a href="#ConvBlock.__init__-69"><span class="linenos">69</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;ln&#39;</span><span class="p">:</span>
</span><span id="ConvBlock.__init__-70"><a href="#ConvBlock.__init__-70"><span class="linenos">70</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">()</span>
</span><span id="ConvBlock.__init__-71"><a href="#ConvBlock.__init__-71"><span class="linenos">71</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">()</span>
</span><span id="ConvBlock.__init__-72"><a href="#ConvBlock.__init__-72"><span class="linenos">72</span></a>
</span><span id="ConvBlock.__init__-73"><a href="#ConvBlock.__init__-73"><span class="linenos">73</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">:</span>
</span><span id="ConvBlock.__init__-74"><a href="#ConvBlock.__init__-74"><span class="linenos">74</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">att</span> <span class="o">=</span> <span class="n">ChannelAttention2D</span><span class="p">(</span><span class="n">filters</span><span class="p">)</span>
</span><span id="ConvBlock.__init__-75"><a href="#ConvBlock.__init__-75"><span class="linenos">75</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</span><span id="ConvBlock.__init__-76"><a href="#ConvBlock.__init__-76"><span class="linenos">76</span></a>        
</span><span id="ConvBlock.__init__-77"><a href="#ConvBlock.__init__-77"><span class="linenos">77</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="ConvBlock.__init__-78"><a href="#ConvBlock.__init__-78"><span class="linenos">78</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="ConvBlock.__init__-79"><a href="#ConvBlock.__init__-79"><span class="linenos">79</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">get_dropout_layer</span><span class="p">(</span>
</span><span id="ConvBlock.__init__-80"><a href="#ConvBlock.__init__-80"><span class="linenos">80</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="ConvBlock.__init__-81"><a href="#ConvBlock.__init__-81"><span class="linenos">81</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">get_dropout_layer</span><span class="p">(</span>
</span><span id="ConvBlock.__init__-82"><a href="#ConvBlock.__init__-82"><span class="linenos">82</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="ConvBlock.__init__-83"><a href="#ConvBlock.__init__-83"><span class="linenos">83</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="ConvBlock.__init__-84"><a href="#ConvBlock.__init__-84"><span class="linenos">84</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ConvBlock.__init__-85"><a href="#ConvBlock.__init__-85"><span class="linenos">85</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">False</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="ConvBlock.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ConvBlock.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, X)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConvBlock.call-87"><a href="#ConvBlock.call-87"><span class="linenos"> 87</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="ConvBlock.call-88"><a href="#ConvBlock.call-88"><span class="linenos"> 88</span></a>        <span class="sd">&quot;&quot;&quot;Model&#39;s forward pass.</span>
</span><span id="ConvBlock.call-89"><a href="#ConvBlock.call-89"><span class="linenos"> 89</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ConvBlock.call-90"><a href="#ConvBlock.call-90"><span class="linenos"> 90</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="k">else</span> <span class="n">X</span>
</span><span id="ConvBlock.call-91"><a href="#ConvBlock.call-91"><span class="linenos"> 91</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock.call-92"><a href="#ConvBlock.call-92"><span class="linenos"> 92</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConvBlock.call-93"><a href="#ConvBlock.call-93"><span class="linenos"> 93</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock.call-94"><a href="#ConvBlock.call-94"><span class="linenos"> 94</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock.call-95"><a href="#ConvBlock.call-95"><span class="linenos"> 95</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="ConvBlock.call-96"><a href="#ConvBlock.call-96"><span class="linenos"> 96</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock.call-97"><a href="#ConvBlock.call-97"><span class="linenos"> 97</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock.call-98"><a href="#ConvBlock.call-98"><span class="linenos"> 98</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConvBlock.call-99"><a href="#ConvBlock.call-99"><span class="linenos"> 99</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock.call-100"><a href="#ConvBlock.call-100"><span class="linenos">100</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock.call-101"><a href="#ConvBlock.call-101"><span class="linenos">101</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">:</span>
</span><span id="ConvBlock.call-102"><a href="#ConvBlock.call-102"><span class="linenos">102</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ConvBlock.call-103"><a href="#ConvBlock.call-103"><span class="linenos">103</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Model's forward pass.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="ConvBlock.build" class="function">build</dd>
                <dd id="ConvBlock.add_weight" class="function">add_weight</dd>
                <dd id="ConvBlock.get_config" class="function">get_config</dd>
                <dd id="ConvBlock.from_config" class="function">from_config</dd>
                <dd id="ConvBlock.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="ConvBlock.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="ConvBlock.compute_mask" class="function">compute_mask</dd>
                <dd id="ConvBlock.dtype" class="variable">dtype</dd>
                <dd id="ConvBlock.name" class="variable">name</dd>
                <dd id="ConvBlock.supports_masking" class="variable">supports_masking</dd>
                <dd id="ConvBlock.dynamic" class="variable">dynamic</dd>
                <dd id="ConvBlock.stateful" class="variable">stateful</dd>
                <dd id="ConvBlock.trainable" class="variable">trainable</dd>
                <dd id="ConvBlock.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="ConvBlock.input_spec" class="variable">input_spec</dd>
                <dd id="ConvBlock.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="ConvBlock.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="ConvBlock.weights" class="variable">weights</dd>
                <dd id="ConvBlock.updates" class="variable">updates</dd>
                <dd id="ConvBlock.losses" class="variable">losses</dd>
                <dd id="ConvBlock.add_loss" class="function">add_loss</dd>
                <dd id="ConvBlock.metrics" class="variable">metrics</dd>
                <dd id="ConvBlock.add_metric" class="function">add_metric</dd>
                <dd id="ConvBlock.add_update" class="function">add_update</dd>
                <dd id="ConvBlock.set_weights" class="function">set_weights</dd>
                <dd id="ConvBlock.get_weights" class="function">get_weights</dd>
                <dd id="ConvBlock.finalize_state" class="function">finalize_state</dd>
                <dd id="ConvBlock.get_updates_for" class="function">get_updates_for</dd>
                <dd id="ConvBlock.get_losses_for" class="function">get_losses_for</dd>
                <dd id="ConvBlock.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="ConvBlock.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="ConvBlock.input_mask" class="variable">input_mask</dd>
                <dd id="ConvBlock.output_mask" class="variable">output_mask</dd>
                <dd id="ConvBlock.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="ConvBlock.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="ConvBlock.get_input_at" class="function">get_input_at</dd>
                <dd id="ConvBlock.get_output_at" class="function">get_output_at</dd>
                <dd id="ConvBlock.input" class="variable">input</dd>
                <dd id="ConvBlock.output" class="variable">output</dd>
                <dd id="ConvBlock.input_shape" class="variable">input_shape</dd>
                <dd id="ConvBlock.count_params" class="function">count_params</dd>
                <dd id="ConvBlock.output_shape" class="variable">output_shape</dd>
                <dd id="ConvBlock.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="ConvBlock.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="ConvBlock.apply" class="function">apply</dd>
                <dd id="ConvBlock.add_variable" class="function">add_variable</dd>
                <dd id="ConvBlock.variables" class="variable">variables</dd>
                <dd id="ConvBlock.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="ConvBlock.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="ConvBlock.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="ConvBlock.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="ConvBlock.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="ConvBlock.name_scope" class="variable">name_scope</dd>
                <dd id="ConvBlock.submodules" class="variable">submodules</dd>
                <dd id="ConvBlock.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="DropPath">
                                <div class="attr class">
        <a class="headerlink" href="#DropPath">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">DropPath</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DropPath-106"><a href="#DropPath-106"><span class="linenos">106</span></a><span class="k">class</span> <span class="nc">DropPath</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="DropPath-107"><a href="#DropPath-107"><span class="linenos">107</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="DropPath-108"><a href="#DropPath-108"><span class="linenos">108</span></a><span class="sd">    Drop path layer. </span>
</span><span id="DropPath-109"><a href="#DropPath-109"><span class="linenos">109</span></a>
</span><span id="DropPath-110"><a href="#DropPath-110"><span class="linenos">110</span></a><span class="sd">    Adapted from </span>
</span><span id="DropPath-111"><a href="#DropPath-111"><span class="linenos">111</span></a><span class="sd">    https://github.com/rishigami/Swin-Transformer-TF/blob/main/swintransformer/model.py</span>
</span><span id="DropPath-112"><a href="#DropPath-112"><span class="linenos">112</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="DropPath-113"><a href="#DropPath-113"><span class="linenos">113</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_prob</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="DropPath-114"><a href="#DropPath-114"><span class="linenos">114</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="DropPath-115"><a href="#DropPath-115"><span class="linenos">115</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span> <span class="o">=</span> <span class="n">drop_prob</span>
</span><span id="DropPath-116"><a href="#DropPath-116"><span class="linenos">116</span></a>
</span><span id="DropPath-117"><a href="#DropPath-117"><span class="linenos">117</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span><span id="DropPath-118"><a href="#DropPath-118"><span class="linenos">118</span></a>        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">training</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">):</span>
</span><span id="DropPath-119"><a href="#DropPath-119"><span class="linenos">119</span></a>            <span class="k">return</span> <span class="n">x</span>
</span><span id="DropPath-120"><a href="#DropPath-120"><span class="linenos">120</span></a>
</span><span id="DropPath-121"><a href="#DropPath-121"><span class="linenos">121</span></a>        <span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span>    <span class="c1"># Compute keep_prob</span>
</span><span id="DropPath-122"><a href="#DropPath-122"><span class="linenos">122</span></a>
</span><span id="DropPath-123"><a href="#DropPath-123"><span class="linenos">123</span></a>        <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">keep_prob</span>   <span class="c1"># Compute drop_connect tensor</span>
</span><span id="DropPath-124"><a href="#DropPath-124"><span class="linenos">124</span></a>        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="DropPath-125"><a href="#DropPath-125"><span class="linenos">125</span></a>        <span class="n">random_tensor</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="DropPath-126"><a href="#DropPath-126"><span class="linenos">126</span></a>        <span class="n">binary_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">random_tensor</span><span class="p">)</span>
</span><span id="DropPath-127"><a href="#DropPath-127"><span class="linenos">127</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span> <span class="o">*</span> <span class="n">binary_tensor</span>
</span><span id="DropPath-128"><a href="#DropPath-128"><span class="linenos">128</span></a>        <span class="k">return</span> <span class="n">output</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Drop path layer. </p>

<p>Adapted from 
https://github.com/rishigami/Swin-Transformer-TF/blob/main/swintransformer/model.py</p>
</div>


                            <div id="DropPath.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#DropPath.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">DropPath</span><span class="signature">(drop_prob=None)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DropPath.__init__-113"><a href="#DropPath.__init__-113"><span class="linenos">113</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_prob</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="DropPath.__init__-114"><a href="#DropPath.__init__-114"><span class="linenos">114</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="DropPath.__init__-115"><a href="#DropPath.__init__-115"><span class="linenos">115</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span> <span class="o">=</span> <span class="n">drop_prob</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="DropPath.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#DropPath.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, x, training=False)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DropPath.call-117"><a href="#DropPath.call-117"><span class="linenos">117</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span><span id="DropPath.call-118"><a href="#DropPath.call-118"><span class="linenos">118</span></a>        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">training</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">):</span>
</span><span id="DropPath.call-119"><a href="#DropPath.call-119"><span class="linenos">119</span></a>            <span class="k">return</span> <span class="n">x</span>
</span><span id="DropPath.call-120"><a href="#DropPath.call-120"><span class="linenos">120</span></a>
</span><span id="DropPath.call-121"><a href="#DropPath.call-121"><span class="linenos">121</span></a>        <span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span>    <span class="c1"># Compute keep_prob</span>
</span><span id="DropPath.call-122"><a href="#DropPath.call-122"><span class="linenos">122</span></a>
</span><span id="DropPath.call-123"><a href="#DropPath.call-123"><span class="linenos">123</span></a>        <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">keep_prob</span>   <span class="c1"># Compute drop_connect tensor</span>
</span><span id="DropPath.call-124"><a href="#DropPath.call-124"><span class="linenos">124</span></a>        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="DropPath.call-125"><a href="#DropPath.call-125"><span class="linenos">125</span></a>        <span class="n">random_tensor</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="DropPath.call-126"><a href="#DropPath.call-126"><span class="linenos">126</span></a>        <span class="n">binary_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">random_tensor</span><span class="p">)</span>
</span><span id="DropPath.call-127"><a href="#DropPath.call-127"><span class="linenos">127</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span> <span class="o">*</span> <span class="n">binary_tensor</span>
</span><span id="DropPath.call-128"><a href="#DropPath.call-128"><span class="linenos">128</span></a>        <span class="k">return</span> <span class="n">output</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>The <code><a href="#DropPath.call">call()</a></code> method may not create state (except in its first invocation,
wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).
It is recommended to create state in <code><a href="#DropPath.__init__">__init__()</a></code>, or the <code><a href="#DropPath.build">build()</a></code> method
that is called automatically before <code><a href="#DropPath.call">call()</a></code> executes the first time.</p>

<p>Args:
  inputs: Input tensor, or dict/list/tuple of input tensors.
    The first positional <code>inputs</code> argument is subject to special rules:
    - <code>inputs</code> must be explicitly passed. A layer cannot have zero
      arguments, and <code>inputs</code> cannot be provided via the default value
      of a keyword argument.
    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.
    - Keras mask metadata is only collected from <code>inputs</code>.
    - Layers are built (<code>build(input_shape)</code> method)
      using shape info from <code>inputs</code> only.
    - <code><a href="#DropPath.input_spec">input_spec</a></code> compatibility is only checked against <code>inputs</code>.
    - Mixed precision input casting is only applied to <code>inputs</code>.
      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their
      casting behavior in mixed precision should be handled manually.
    - The SavedModel input specification is generated using <code>inputs</code> only.
    - Integration with various ecosystem packages like TFMOT, TFLite,
      TF.js, etc is only supported for <code>inputs</code> and not for tensors in
      positional and keyword arguments.
  <em>args: Additional positional arguments. May contain tensors, although
    this is not recommended, for the reasons above.
  *</em>kwargs: Additional keyword arguments. May contain tensors, although
    this is not recommended, for the reasons above.
    The following optional keyword arguments are reserved:
    - <code>training</code>: Boolean scalar tensor of Python boolean indicating
      whether the <code><a href="#DropPath.call">call</a></code> is meant for training or inference.
    - <code>mask</code>: Boolean input mask. If the layer's <code><a href="#DropPath.call">call()</a></code> method takes a
      <code>mask</code> argument, its default value will be set to the mask generated
      for <code>inputs</code> by the previous layer (if <code><a href="#DropPath.input">input</a></code> did come from a layer
      that generated a corresponding mask, i.e. if it came from a Keras
      layer with masking support).</p>

<p>Returns:
  A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="DropPath.build" class="function">build</dd>
                <dd id="DropPath.add_weight" class="function">add_weight</dd>
                <dd id="DropPath.get_config" class="function">get_config</dd>
                <dd id="DropPath.from_config" class="function">from_config</dd>
                <dd id="DropPath.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="DropPath.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="DropPath.compute_mask" class="function">compute_mask</dd>
                <dd id="DropPath.dtype" class="variable">dtype</dd>
                <dd id="DropPath.name" class="variable">name</dd>
                <dd id="DropPath.supports_masking" class="variable">supports_masking</dd>
                <dd id="DropPath.dynamic" class="variable">dynamic</dd>
                <dd id="DropPath.stateful" class="variable">stateful</dd>
                <dd id="DropPath.trainable" class="variable">trainable</dd>
                <dd id="DropPath.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="DropPath.input_spec" class="variable">input_spec</dd>
                <dd id="DropPath.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="DropPath.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="DropPath.weights" class="variable">weights</dd>
                <dd id="DropPath.updates" class="variable">updates</dd>
                <dd id="DropPath.losses" class="variable">losses</dd>
                <dd id="DropPath.add_loss" class="function">add_loss</dd>
                <dd id="DropPath.metrics" class="variable">metrics</dd>
                <dd id="DropPath.add_metric" class="function">add_metric</dd>
                <dd id="DropPath.add_update" class="function">add_update</dd>
                <dd id="DropPath.set_weights" class="function">set_weights</dd>
                <dd id="DropPath.get_weights" class="function">get_weights</dd>
                <dd id="DropPath.finalize_state" class="function">finalize_state</dd>
                <dd id="DropPath.get_updates_for" class="function">get_updates_for</dd>
                <dd id="DropPath.get_losses_for" class="function">get_losses_for</dd>
                <dd id="DropPath.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="DropPath.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="DropPath.input_mask" class="variable">input_mask</dd>
                <dd id="DropPath.output_mask" class="variable">output_mask</dd>
                <dd id="DropPath.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="DropPath.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="DropPath.get_input_at" class="function">get_input_at</dd>
                <dd id="DropPath.get_output_at" class="function">get_output_at</dd>
                <dd id="DropPath.input" class="variable">input</dd>
                <dd id="DropPath.output" class="variable">output</dd>
                <dd id="DropPath.input_shape" class="variable">input_shape</dd>
                <dd id="DropPath.count_params" class="function">count_params</dd>
                <dd id="DropPath.output_shape" class="variable">output_shape</dd>
                <dd id="DropPath.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="DropPath.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="DropPath.apply" class="function">apply</dd>
                <dd id="DropPath.add_variable" class="function">add_variable</dd>
                <dd id="DropPath.variables" class="variable">variables</dd>
                <dd id="DropPath.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="DropPath.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="DropPath.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="DropPath.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="DropPath.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="DropPath.name_scope" class="variable">name_scope</dd>
                <dd id="DropPath.submodules" class="variable">submodules</dd>
                <dd id="DropPath.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="ConvNextBlock">
                                <div class="attr class">
        <a class="headerlink" href="#ConvNextBlock">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">ConvNextBlock</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConvNextBlock-131"><a href="#ConvNextBlock-131"><span class="linenos">131</span></a><span class="k">class</span> <span class="nc">ConvNextBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span> 
</span><span id="ConvNextBlock-132"><a href="#ConvNextBlock-132"><span class="linenos">132</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="ConvNextBlock-133"><a href="#ConvNextBlock-133"><span class="linenos">133</span></a><span class="sd">    ConvNext block.</span>
</span><span id="ConvNextBlock-134"><a href="#ConvNextBlock-134"><span class="linenos">134</span></a>
</span><span id="ConvNextBlock-135"><a href="#ConvNextBlock-135"><span class="linenos">135</span></a><span class="sd">    References</span>
</span><span id="ConvNextBlock-136"><a href="#ConvNextBlock-136"><span class="linenos">136</span></a><span class="sd">    ----------</span>
</span><span id="ConvNextBlock-137"><a href="#ConvNextBlock-137"><span class="linenos">137</span></a><span class="sd">    [1] A ConvNet for the 2020s: https://arxiv.org/abs/2201.03545 </span>
</span><span id="ConvNextBlock-138"><a href="#ConvNextBlock-138"><span class="linenos">138</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ConvNextBlock-139"><a href="#ConvNextBlock-139"><span class="linenos">139</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">drop_path</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">layer_scale_init_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1">#1e-6 </span>
</span><span id="ConvNextBlock-140"><a href="#ConvNextBlock-140"><span class="linenos">140</span></a>                 <span class="n">use_1x1conv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span> 
</span><span id="ConvNextBlock-141"><a href="#ConvNextBlock-141"><span class="linenos">141</span></a>                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="ConvNextBlock-142"><a href="#ConvNextBlock-142"><span class="linenos">142</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</span><span id="ConvNextBlock-143"><a href="#ConvNextBlock-143"><span class="linenos">143</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">filters</span> <span class="o">=</span> <span class="n">filters</span>
</span><span id="ConvNextBlock-144"><a href="#ConvNextBlock-144"><span class="linenos">144</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="n">drop_path</span>
</span><span id="ConvNextBlock-145"><a href="#ConvNextBlock-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_scale_init_value</span> <span class="o">=</span> <span class="n">layer_scale_init_value</span>
</span><span id="ConvNextBlock-146"><a href="#ConvNextBlock-146"><span class="linenos">146</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dwconv</span> <span class="o">=</span> <span class="n">DepthwiseConv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="ConvNextBlock-147"><a href="#ConvNextBlock-147"><span class="linenos">147</span></a>                                      <span class="n">depth_multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="ConvNextBlock-148"><a href="#ConvNextBlock-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
</span><span id="ConvNextBlock-149"><a href="#ConvNextBlock-149"><span class="linenos">149</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pwconv1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">)</span>
</span><span id="ConvNextBlock-150"><a href="#ConvNextBlock-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</span><span id="ConvNextBlock-151"><a href="#ConvNextBlock-151"><span class="linenos">151</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="n">DropPath</span><span class="p">(</span><span class="n">drop_path</span><span class="p">)</span>
</span><span id="ConvNextBlock-152"><a href="#ConvNextBlock-152"><span class="linenos">152</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pwconv2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">)</span>
</span><span id="ConvNextBlock-153"><a href="#ConvNextBlock-153"><span class="linenos">153</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span> <span class="o">=</span> <span class="n">use_1x1conv</span>
</span><span id="ConvNextBlock-154"><a href="#ConvNextBlock-154"><span class="linenos">154</span></a>
</span><span id="ConvNextBlock-155"><a href="#ConvNextBlock-155"><span class="linenos">155</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConvNextBlock-156"><a href="#ConvNextBlock-156"><span class="linenos">156</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bn&#39;</span><span class="p">,</span> <span class="s1">&#39;ln&#39;</span><span class="p">]:</span>
</span><span id="ConvNextBlock-157"><a href="#ConvNextBlock-157"><span class="linenos">157</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Normalization not supported, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span><span id="ConvNextBlock-158"><a href="#ConvNextBlock-158"><span class="linenos">158</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;bn&#39;</span><span class="p">:</span>
</span><span id="ConvNextBlock-159"><a href="#ConvNextBlock-159"><span class="linenos">159</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="ConvNextBlock-160"><a href="#ConvNextBlock-160"><span class="linenos">160</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;ln&#39;</span><span class="p">:</span>
</span><span id="ConvNextBlock-161"><a href="#ConvNextBlock-161"><span class="linenos">161</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="ConvNextBlock-162"><a href="#ConvNextBlock-162"><span class="linenos">162</span></a>
</span><span id="ConvNextBlock-163"><a href="#ConvNextBlock-163"><span class="linenos">163</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span><span class="p">:</span>
</span><span id="ConvNextBlock-164"><a href="#ConvNextBlock-164"><span class="linenos">164</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv1x1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="ConvNextBlock-165"><a href="#ConvNextBlock-165"><span class="linenos">165</span></a>
</span><span id="ConvNextBlock-166"><a href="#ConvNextBlock-166"><span class="linenos">166</span></a>    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="ConvNextBlock-167"><a href="#ConvNextBlock-167"><span class="linenos">167</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
</span><span id="ConvNextBlock-168"><a href="#ConvNextBlock-168"><span class="linenos">168</span></a>            <span class="n">initial_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_scale_init_value</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">)),</span>
</span><span id="ConvNextBlock-169"><a href="#ConvNextBlock-169"><span class="linenos">169</span></a>            <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;gamma&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_scale_init_value</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="ConvNextBlock-170"><a href="#ConvNextBlock-170"><span class="linenos">170</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="ConvNextBlock-171"><a href="#ConvNextBlock-171"><span class="linenos">171</span></a>
</span><span id="ConvNextBlock-172"><a href="#ConvNextBlock-172"><span class="linenos">172</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="ConvNextBlock-173"><a href="#ConvNextBlock-173"><span class="linenos">173</span></a>        <span class="nb">input</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ConvNextBlock-174"><a href="#ConvNextBlock-174"><span class="linenos">174</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dwconv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvNextBlock-175"><a href="#ConvNextBlock-175"><span class="linenos">175</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvNextBlock-176"><a href="#ConvNextBlock-176"><span class="linenos">176</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pwconv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvNextBlock-177"><a href="#ConvNextBlock-177"><span class="linenos">177</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvNextBlock-178"><a href="#ConvNextBlock-178"><span class="linenos">178</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pwconv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvNextBlock-179"><a href="#ConvNextBlock-179"><span class="linenos">179</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConvNextBlock-180"><a href="#ConvNextBlock-180"><span class="linenos">180</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">x</span>
</span><span id="ConvNextBlock-181"><a href="#ConvNextBlock-181"><span class="linenos">181</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span><span class="p">:</span>
</span><span id="ConvNextBlock-182"><a href="#ConvNextBlock-182"><span class="linenos">182</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1x1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="ConvNextBlock-183"><a href="#ConvNextBlock-183"><span class="linenos">183</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvNextBlock-184"><a href="#ConvNextBlock-184"><span class="linenos">184</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>ConvNext block.</p>

<h6 id="references">References</h6>

<p>[1] A ConvNet for the 2020s: https://arxiv.org/abs/2201.03545</p>
</div>


                            <div id="ConvNextBlock.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ConvNextBlock.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">ConvNextBlock</span><span class="signature">(
    filters,
    drop_path=0.0,
    layer_scale_init_value=0,
    use_1x1conv=False,
    activation=&#39;gelu&#39;,
    normalization=&#39;ln&#39;,
    name=None,
    **conv_kwargs
)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConvNextBlock.__init__-139"><a href="#ConvNextBlock.__init__-139"><span class="linenos">139</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">drop_path</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">layer_scale_init_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1">#1e-6 </span>
</span><span id="ConvNextBlock.__init__-140"><a href="#ConvNextBlock.__init__-140"><span class="linenos">140</span></a>                 <span class="n">use_1x1conv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span> 
</span><span id="ConvNextBlock.__init__-141"><a href="#ConvNextBlock.__init__-141"><span class="linenos">141</span></a>                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="ConvNextBlock.__init__-142"><a href="#ConvNextBlock.__init__-142"><span class="linenos">142</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</span><span id="ConvNextBlock.__init__-143"><a href="#ConvNextBlock.__init__-143"><span class="linenos">143</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">filters</span> <span class="o">=</span> <span class="n">filters</span>
</span><span id="ConvNextBlock.__init__-144"><a href="#ConvNextBlock.__init__-144"><span class="linenos">144</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="n">drop_path</span>
</span><span id="ConvNextBlock.__init__-145"><a href="#ConvNextBlock.__init__-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_scale_init_value</span> <span class="o">=</span> <span class="n">layer_scale_init_value</span>
</span><span id="ConvNextBlock.__init__-146"><a href="#ConvNextBlock.__init__-146"><span class="linenos">146</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dwconv</span> <span class="o">=</span> <span class="n">DepthwiseConv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="ConvNextBlock.__init__-147"><a href="#ConvNextBlock.__init__-147"><span class="linenos">147</span></a>                                      <span class="n">depth_multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="ConvNextBlock.__init__-148"><a href="#ConvNextBlock.__init__-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
</span><span id="ConvNextBlock.__init__-149"><a href="#ConvNextBlock.__init__-149"><span class="linenos">149</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pwconv1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">)</span>
</span><span id="ConvNextBlock.__init__-150"><a href="#ConvNextBlock.__init__-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</span><span id="ConvNextBlock.__init__-151"><a href="#ConvNextBlock.__init__-151"><span class="linenos">151</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="n">DropPath</span><span class="p">(</span><span class="n">drop_path</span><span class="p">)</span>
</span><span id="ConvNextBlock.__init__-152"><a href="#ConvNextBlock.__init__-152"><span class="linenos">152</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pwconv2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">)</span>
</span><span id="ConvNextBlock.__init__-153"><a href="#ConvNextBlock.__init__-153"><span class="linenos">153</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span> <span class="o">=</span> <span class="n">use_1x1conv</span>
</span><span id="ConvNextBlock.__init__-154"><a href="#ConvNextBlock.__init__-154"><span class="linenos">154</span></a>
</span><span id="ConvNextBlock.__init__-155"><a href="#ConvNextBlock.__init__-155"><span class="linenos">155</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConvNextBlock.__init__-156"><a href="#ConvNextBlock.__init__-156"><span class="linenos">156</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bn&#39;</span><span class="p">,</span> <span class="s1">&#39;ln&#39;</span><span class="p">]:</span>
</span><span id="ConvNextBlock.__init__-157"><a href="#ConvNextBlock.__init__-157"><span class="linenos">157</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Normalization not supported, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span><span id="ConvNextBlock.__init__-158"><a href="#ConvNextBlock.__init__-158"><span class="linenos">158</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;bn&#39;</span><span class="p">:</span>
</span><span id="ConvNextBlock.__init__-159"><a href="#ConvNextBlock.__init__-159"><span class="linenos">159</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="ConvNextBlock.__init__-160"><a href="#ConvNextBlock.__init__-160"><span class="linenos">160</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;ln&#39;</span><span class="p">:</span>
</span><span id="ConvNextBlock.__init__-161"><a href="#ConvNextBlock.__init__-161"><span class="linenos">161</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="ConvNextBlock.__init__-162"><a href="#ConvNextBlock.__init__-162"><span class="linenos">162</span></a>
</span><span id="ConvNextBlock.__init__-163"><a href="#ConvNextBlock.__init__-163"><span class="linenos">163</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span><span class="p">:</span>
</span><span id="ConvNextBlock.__init__-164"><a href="#ConvNextBlock.__init__-164"><span class="linenos">164</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv1x1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="ConvNextBlock.build" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ConvNextBlock.build">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">build</span><span class="signature">(self, input_shape)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConvNextBlock.build-166"><a href="#ConvNextBlock.build-166"><span class="linenos">166</span></a>    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="ConvNextBlock.build-167"><a href="#ConvNextBlock.build-167"><span class="linenos">167</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
</span><span id="ConvNextBlock.build-168"><a href="#ConvNextBlock.build-168"><span class="linenos">168</span></a>            <span class="n">initial_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_scale_init_value</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">)),</span>
</span><span id="ConvNextBlock.build-169"><a href="#ConvNextBlock.build-169"><span class="linenos">169</span></a>            <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;gamma&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_scale_init_value</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="ConvNextBlock.build-170"><a href="#ConvNextBlock.build-170"><span class="linenos">170</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Creates the variables of the layer (optional, for subclass implementers).</p>

<p>This is a method that implementers of subclasses of <code>Layer</code> or <code>Model</code>
can override if they need a state-creation step in-between
layer instantiation and layer call. It is invoked automatically before
the first execution of <code><a href="#ConvNextBlock.call">call()</a></code>.</p>

<p>This is typically used to create the weights of <code>Layer</code> subclasses
(at the discretion of the subclass implementer).</p>

<p>Args:
  input_shape: Instance of <code>TensorShape</code>, or list of instances of
    <code>TensorShape</code> if the layer expects a list of inputs
    (one instance per input).</p>
</div>


                            </div>
                            <div id="ConvNextBlock.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ConvNextBlock.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, x)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ConvNextBlock.call-172"><a href="#ConvNextBlock.call-172"><span class="linenos">172</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="ConvNextBlock.call-173"><a href="#ConvNextBlock.call-173"><span class="linenos">173</span></a>        <span class="nb">input</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ConvNextBlock.call-174"><a href="#ConvNextBlock.call-174"><span class="linenos">174</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dwconv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvNextBlock.call-175"><a href="#ConvNextBlock.call-175"><span class="linenos">175</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvNextBlock.call-176"><a href="#ConvNextBlock.call-176"><span class="linenos">176</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pwconv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvNextBlock.call-177"><a href="#ConvNextBlock.call-177"><span class="linenos">177</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvNextBlock.call-178"><a href="#ConvNextBlock.call-178"><span class="linenos">178</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pwconv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvNextBlock.call-179"><a href="#ConvNextBlock.call-179"><span class="linenos">179</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ConvNextBlock.call-180"><a href="#ConvNextBlock.call-180"><span class="linenos">180</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">x</span>
</span><span id="ConvNextBlock.call-181"><a href="#ConvNextBlock.call-181"><span class="linenos">181</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span><span class="p">:</span>
</span><span id="ConvNextBlock.call-182"><a href="#ConvNextBlock.call-182"><span class="linenos">182</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1x1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="ConvNextBlock.call-183"><a href="#ConvNextBlock.call-183"><span class="linenos">183</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ConvNextBlock.call-184"><a href="#ConvNextBlock.call-184"><span class="linenos">184</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>The <code><a href="#ConvNextBlock.call">call()</a></code> method may not create state (except in its first invocation,
wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).
It is recommended to create state in <code><a href="#ConvNextBlock.__init__">__init__()</a></code>, or the <code><a href="#ConvNextBlock.build">build()</a></code> method
that is called automatically before <code><a href="#ConvNextBlock.call">call()</a></code> executes the first time.</p>

<p>Args:
  inputs: Input tensor, or dict/list/tuple of input tensors.
    The first positional <code>inputs</code> argument is subject to special rules:
    - <code>inputs</code> must be explicitly passed. A layer cannot have zero
      arguments, and <code>inputs</code> cannot be provided via the default value
      of a keyword argument.
    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.
    - Keras mask metadata is only collected from <code>inputs</code>.
    - Layers are built (<code>build(input_shape)</code> method)
      using shape info from <code>inputs</code> only.
    - <code><a href="#ConvNextBlock.input_spec">input_spec</a></code> compatibility is only checked against <code>inputs</code>.
    - Mixed precision input casting is only applied to <code>inputs</code>.
      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their
      casting behavior in mixed precision should be handled manually.
    - The SavedModel input specification is generated using <code>inputs</code> only.
    - Integration with various ecosystem packages like TFMOT, TFLite,
      TF.js, etc is only supported for <code>inputs</code> and not for tensors in
      positional and keyword arguments.
  <em>args: Additional positional arguments. May contain tensors, although
    this is not recommended, for the reasons above.
  *</em>kwargs: Additional keyword arguments. May contain tensors, although
    this is not recommended, for the reasons above.
    The following optional keyword arguments are reserved:
    - <code>training</code>: Boolean scalar tensor of Python boolean indicating
      whether the <code><a href="#ConvNextBlock.call">call</a></code> is meant for training or inference.
    - <code>mask</code>: Boolean input mask. If the layer's <code><a href="#ConvNextBlock.call">call()</a></code> method takes a
      <code>mask</code> argument, its default value will be set to the mask generated
      for <code>inputs</code> by the previous layer (if <code><a href="#ConvNextBlock.input">input</a></code> did come from a layer
      that generated a corresponding mask, i.e. if it came from a Keras
      layer with masking support).</p>

<p>Returns:
  A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="ConvNextBlock.add_weight" class="function">add_weight</dd>
                <dd id="ConvNextBlock.get_config" class="function">get_config</dd>
                <dd id="ConvNextBlock.from_config" class="function">from_config</dd>
                <dd id="ConvNextBlock.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="ConvNextBlock.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="ConvNextBlock.compute_mask" class="function">compute_mask</dd>
                <dd id="ConvNextBlock.dtype" class="variable">dtype</dd>
                <dd id="ConvNextBlock.name" class="variable">name</dd>
                <dd id="ConvNextBlock.supports_masking" class="variable">supports_masking</dd>
                <dd id="ConvNextBlock.dynamic" class="variable">dynamic</dd>
                <dd id="ConvNextBlock.stateful" class="variable">stateful</dd>
                <dd id="ConvNextBlock.trainable" class="variable">trainable</dd>
                <dd id="ConvNextBlock.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="ConvNextBlock.input_spec" class="variable">input_spec</dd>
                <dd id="ConvNextBlock.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="ConvNextBlock.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="ConvNextBlock.weights" class="variable">weights</dd>
                <dd id="ConvNextBlock.updates" class="variable">updates</dd>
                <dd id="ConvNextBlock.losses" class="variable">losses</dd>
                <dd id="ConvNextBlock.add_loss" class="function">add_loss</dd>
                <dd id="ConvNextBlock.metrics" class="variable">metrics</dd>
                <dd id="ConvNextBlock.add_metric" class="function">add_metric</dd>
                <dd id="ConvNextBlock.add_update" class="function">add_update</dd>
                <dd id="ConvNextBlock.set_weights" class="function">set_weights</dd>
                <dd id="ConvNextBlock.get_weights" class="function">get_weights</dd>
                <dd id="ConvNextBlock.finalize_state" class="function">finalize_state</dd>
                <dd id="ConvNextBlock.get_updates_for" class="function">get_updates_for</dd>
                <dd id="ConvNextBlock.get_losses_for" class="function">get_losses_for</dd>
                <dd id="ConvNextBlock.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="ConvNextBlock.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="ConvNextBlock.input_mask" class="variable">input_mask</dd>
                <dd id="ConvNextBlock.output_mask" class="variable">output_mask</dd>
                <dd id="ConvNextBlock.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="ConvNextBlock.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="ConvNextBlock.get_input_at" class="function">get_input_at</dd>
                <dd id="ConvNextBlock.get_output_at" class="function">get_output_at</dd>
                <dd id="ConvNextBlock.input" class="variable">input</dd>
                <dd id="ConvNextBlock.output" class="variable">output</dd>
                <dd id="ConvNextBlock.input_shape" class="variable">input_shape</dd>
                <dd id="ConvNextBlock.count_params" class="function">count_params</dd>
                <dd id="ConvNextBlock.output_shape" class="variable">output_shape</dd>
                <dd id="ConvNextBlock.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="ConvNextBlock.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="ConvNextBlock.apply" class="function">apply</dd>
                <dd id="ConvNextBlock.add_variable" class="function">add_variable</dd>
                <dd id="ConvNextBlock.variables" class="variable">variables</dd>
                <dd id="ConvNextBlock.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="ConvNextBlock.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="ConvNextBlock.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="ConvNextBlock.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="ConvNextBlock.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="ConvNextBlock.name_scope" class="variable">name_scope</dd>
                <dd id="ConvNextBlock.submodules" class="variable">submodules</dd>
                <dd id="ConvNextBlock.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="ResidualBlock">
                                <div class="attr class">
        <a class="headerlink" href="#ResidualBlock">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">ResidualBlock</span><wbr>(<span class="base"><a href="#ConvBlock">ConvBlock</a></span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResidualBlock-187"><a href="#ResidualBlock-187"><span class="linenos">187</span></a><span class="k">class</span> <span class="nc">ResidualBlock</span><span class="p">(</span><span class="n">ConvBlock</span><span class="p">):</span> 
</span><span id="ResidualBlock-188"><a href="#ResidualBlock-188"><span class="linenos">188</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="ResidualBlock-189"><a href="#ResidualBlock-189"><span class="linenos">189</span></a><span class="sd">    Residual block. </span>
</span><span id="ResidualBlock-190"><a href="#ResidualBlock-190"><span class="linenos">190</span></a><span class="sd">    </span>
</span><span id="ResidualBlock-191"><a href="#ResidualBlock-191"><span class="linenos">191</span></a><span class="sd">    Two examples:</span>
</span><span id="ResidualBlock-192"><a href="#ResidualBlock-192"><span class="linenos">192</span></a><span class="sd">    * Standard residual block [1]: Conv2D -&gt; BN -&gt; ReLU -&gt; Conv2D -&gt; BN -&gt; Add _&gt; ReLU</span>
</span><span id="ResidualBlock-193"><a href="#ResidualBlock-193"><span class="linenos">193</span></a><span class="sd">    * EDSR-style block: Conv2D -&gt; ReLU -&gt; Conv2D -&gt; Add -&gt; ReLU</span>
</span><span id="ResidualBlock-194"><a href="#ResidualBlock-194"><span class="linenos">194</span></a>
</span><span id="ResidualBlock-195"><a href="#ResidualBlock-195"><span class="linenos">195</span></a><span class="sd">    References</span>
</span><span id="ResidualBlock-196"><a href="#ResidualBlock-196"><span class="linenos">196</span></a><span class="sd">    ----------</span>
</span><span id="ResidualBlock-197"><a href="#ResidualBlock-197"><span class="linenos">197</span></a><span class="sd">    [1] Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385</span>
</span><span id="ResidualBlock-198"><a href="#ResidualBlock-198"><span class="linenos">198</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ResidualBlock-199"><a href="#ResidualBlock-199"><span class="linenos">199</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">ks_cl2</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> 
</span><span id="ResidualBlock-200"><a href="#ResidualBlock-200"><span class="linenos">200</span></a>                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span><span id="ResidualBlock-201"><a href="#ResidualBlock-201"><span class="linenos">201</span></a>                 <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_1x1conv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span><span id="ResidualBlock-202"><a href="#ResidualBlock-202"><span class="linenos">202</span></a>                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="ResidualBlock-203"><a href="#ResidualBlock-203"><span class="linenos">203</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="p">,</span> <span class="n">ks_cl2</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> 
</span><span id="ResidualBlock-204"><a href="#ResidualBlock-204"><span class="linenos">204</span></a>                         <span class="n">normalization</span><span class="p">,</span> <span class="n">attention</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> 
</span><span id="ResidualBlock-205"><a href="#ResidualBlock-205"><span class="linenos">205</span></a>                         <span class="n">dropout_variant</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="ResidualBlock-206"><a href="#ResidualBlock-206"><span class="linenos">206</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span> <span class="o">=</span> <span class="n">use_1x1conv</span>
</span><span id="ResidualBlock-207"><a href="#ResidualBlock-207"><span class="linenos">207</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span><span class="p">:</span>
</span><span id="ResidualBlock-208"><a href="#ResidualBlock-208"><span class="linenos">208</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv1x1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="ResidualBlock-209"><a href="#ResidualBlock-209"><span class="linenos">209</span></a>
</span><span id="ResidualBlock-210"><a href="#ResidualBlock-210"><span class="linenos">210</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="ResidualBlock-211"><a href="#ResidualBlock-211"><span class="linenos">211</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="ResidualBlock-212"><a href="#ResidualBlock-212"><span class="linenos">212</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  
</span><span id="ResidualBlock-213"><a href="#ResidualBlock-213"><span class="linenos">213</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ResidualBlock-214"><a href="#ResidualBlock-214"><span class="linenos">214</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span>
</span><span id="ResidualBlock-215"><a href="#ResidualBlock-215"><span class="linenos">215</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock-216"><a href="#ResidualBlock-216"><span class="linenos">216</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResidualBlock-217"><a href="#ResidualBlock-217"><span class="linenos">217</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock-218"><a href="#ResidualBlock-218"><span class="linenos">218</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock-219"><a href="#ResidualBlock-219"><span class="linenos">219</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="ResidualBlock-220"><a href="#ResidualBlock-220"><span class="linenos">220</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> 
</span><span id="ResidualBlock-221"><a href="#ResidualBlock-221"><span class="linenos">221</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock-222"><a href="#ResidualBlock-222"><span class="linenos">222</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResidualBlock-223"><a href="#ResidualBlock-223"><span class="linenos">223</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock-224"><a href="#ResidualBlock-224"><span class="linenos">224</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">:</span>
</span><span id="ResidualBlock-225"><a href="#ResidualBlock-225"><span class="linenos">225</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock-226"><a href="#ResidualBlock-226"><span class="linenos">226</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span><span class="p">:</span>
</span><span id="ResidualBlock-227"><a href="#ResidualBlock-227"><span class="linenos">227</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1x1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="ResidualBlock-228"><a href="#ResidualBlock-228"><span class="linenos">228</span></a>        <span class="n">Y</span> <span class="o">+=</span> <span class="n">X</span>
</span><span id="ResidualBlock-229"><a href="#ResidualBlock-229"><span class="linenos">229</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock-230"><a href="#ResidualBlock-230"><span class="linenos">230</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Residual block. </p>

<p>Two examples:</p>

<ul>
<li>Standard residual block [1]: Conv2D -> BN -> ReLU -> Conv2D -> BN -> Add _&gt; ReLU</li>
<li>EDSR-style block: Conv2D -> ReLU -> Conv2D -> Add -> ReLU</li>
</ul>

<h6 id="references">References</h6>

<p>[1] Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385</p>
</div>


                            <div id="ResidualBlock.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ResidualBlock.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">ResidualBlock</span><span class="signature">(
    filters,
    strides=1,
    ks_cl1=(3, 3),
    ks_cl2=(3, 3),
    activation=&#39;relu&#39;,
    normalization=None,
    attention=False,
    dropout_rate=0,
    dropout_variant=None,
    use_1x1conv=False,
    name=None,
    **conv_kwargs
)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResidualBlock.__init__-199"><a href="#ResidualBlock.__init__-199"><span class="linenos">199</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">ks_cl2</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> 
</span><span id="ResidualBlock.__init__-200"><a href="#ResidualBlock.__init__-200"><span class="linenos">200</span></a>                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span><span id="ResidualBlock.__init__-201"><a href="#ResidualBlock.__init__-201"><span class="linenos">201</span></a>                 <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_1x1conv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span><span id="ResidualBlock.__init__-202"><a href="#ResidualBlock.__init__-202"><span class="linenos">202</span></a>                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="ResidualBlock.__init__-203"><a href="#ResidualBlock.__init__-203"><span class="linenos">203</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="p">,</span> <span class="n">ks_cl2</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> 
</span><span id="ResidualBlock.__init__-204"><a href="#ResidualBlock.__init__-204"><span class="linenos">204</span></a>                         <span class="n">normalization</span><span class="p">,</span> <span class="n">attention</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> 
</span><span id="ResidualBlock.__init__-205"><a href="#ResidualBlock.__init__-205"><span class="linenos">205</span></a>                         <span class="n">dropout_variant</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="ResidualBlock.__init__-206"><a href="#ResidualBlock.__init__-206"><span class="linenos">206</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span> <span class="o">=</span> <span class="n">use_1x1conv</span>
</span><span id="ResidualBlock.__init__-207"><a href="#ResidualBlock.__init__-207"><span class="linenos">207</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span><span class="p">:</span>
</span><span id="ResidualBlock.__init__-208"><a href="#ResidualBlock.__init__-208"><span class="linenos">208</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv1x1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="ResidualBlock.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ResidualBlock.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, X)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResidualBlock.call-210"><a href="#ResidualBlock.call-210"><span class="linenos">210</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="ResidualBlock.call-211"><a href="#ResidualBlock.call-211"><span class="linenos">211</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="ResidualBlock.call-212"><a href="#ResidualBlock.call-212"><span class="linenos">212</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  
</span><span id="ResidualBlock.call-213"><a href="#ResidualBlock.call-213"><span class="linenos">213</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ResidualBlock.call-214"><a href="#ResidualBlock.call-214"><span class="linenos">214</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span>
</span><span id="ResidualBlock.call-215"><a href="#ResidualBlock.call-215"><span class="linenos">215</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock.call-216"><a href="#ResidualBlock.call-216"><span class="linenos">216</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResidualBlock.call-217"><a href="#ResidualBlock.call-217"><span class="linenos">217</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock.call-218"><a href="#ResidualBlock.call-218"><span class="linenos">218</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock.call-219"><a href="#ResidualBlock.call-219"><span class="linenos">219</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="ResidualBlock.call-220"><a href="#ResidualBlock.call-220"><span class="linenos">220</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> 
</span><span id="ResidualBlock.call-221"><a href="#ResidualBlock.call-221"><span class="linenos">221</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock.call-222"><a href="#ResidualBlock.call-222"><span class="linenos">222</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResidualBlock.call-223"><a href="#ResidualBlock.call-223"><span class="linenos">223</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock.call-224"><a href="#ResidualBlock.call-224"><span class="linenos">224</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">:</span>
</span><span id="ResidualBlock.call-225"><a href="#ResidualBlock.call-225"><span class="linenos">225</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock.call-226"><a href="#ResidualBlock.call-226"><span class="linenos">226</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_1x1conv</span><span class="p">:</span>
</span><span id="ResidualBlock.call-227"><a href="#ResidualBlock.call-227"><span class="linenos">227</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1x1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="ResidualBlock.call-228"><a href="#ResidualBlock.call-228"><span class="linenos">228</span></a>        <span class="n">Y</span> <span class="o">+=</span> <span class="n">X</span>
</span><span id="ResidualBlock.call-229"><a href="#ResidualBlock.call-229"><span class="linenos">229</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="ResidualBlock.call-230"><a href="#ResidualBlock.call-230"><span class="linenos">230</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Model's forward pass.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="ResidualBlock.build" class="function">build</dd>
                <dd id="ResidualBlock.add_weight" class="function">add_weight</dd>
                <dd id="ResidualBlock.get_config" class="function">get_config</dd>
                <dd id="ResidualBlock.from_config" class="function">from_config</dd>
                <dd id="ResidualBlock.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="ResidualBlock.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="ResidualBlock.compute_mask" class="function">compute_mask</dd>
                <dd id="ResidualBlock.dtype" class="variable">dtype</dd>
                <dd id="ResidualBlock.name" class="variable">name</dd>
                <dd id="ResidualBlock.supports_masking" class="variable">supports_masking</dd>
                <dd id="ResidualBlock.dynamic" class="variable">dynamic</dd>
                <dd id="ResidualBlock.stateful" class="variable">stateful</dd>
                <dd id="ResidualBlock.trainable" class="variable">trainable</dd>
                <dd id="ResidualBlock.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="ResidualBlock.input_spec" class="variable">input_spec</dd>
                <dd id="ResidualBlock.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="ResidualBlock.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="ResidualBlock.weights" class="variable">weights</dd>
                <dd id="ResidualBlock.updates" class="variable">updates</dd>
                <dd id="ResidualBlock.losses" class="variable">losses</dd>
                <dd id="ResidualBlock.add_loss" class="function">add_loss</dd>
                <dd id="ResidualBlock.metrics" class="variable">metrics</dd>
                <dd id="ResidualBlock.add_metric" class="function">add_metric</dd>
                <dd id="ResidualBlock.add_update" class="function">add_update</dd>
                <dd id="ResidualBlock.set_weights" class="function">set_weights</dd>
                <dd id="ResidualBlock.get_weights" class="function">get_weights</dd>
                <dd id="ResidualBlock.finalize_state" class="function">finalize_state</dd>
                <dd id="ResidualBlock.get_updates_for" class="function">get_updates_for</dd>
                <dd id="ResidualBlock.get_losses_for" class="function">get_losses_for</dd>
                <dd id="ResidualBlock.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="ResidualBlock.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="ResidualBlock.input_mask" class="variable">input_mask</dd>
                <dd id="ResidualBlock.output_mask" class="variable">output_mask</dd>
                <dd id="ResidualBlock.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="ResidualBlock.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="ResidualBlock.get_input_at" class="function">get_input_at</dd>
                <dd id="ResidualBlock.get_output_at" class="function">get_output_at</dd>
                <dd id="ResidualBlock.input" class="variable">input</dd>
                <dd id="ResidualBlock.output" class="variable">output</dd>
                <dd id="ResidualBlock.input_shape" class="variable">input_shape</dd>
                <dd id="ResidualBlock.count_params" class="function">count_params</dd>
                <dd id="ResidualBlock.output_shape" class="variable">output_shape</dd>
                <dd id="ResidualBlock.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="ResidualBlock.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="ResidualBlock.apply" class="function">apply</dd>
                <dd id="ResidualBlock.add_variable" class="function">add_variable</dd>
                <dd id="ResidualBlock.variables" class="variable">variables</dd>
                <dd id="ResidualBlock.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="ResidualBlock.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="ResidualBlock.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="ResidualBlock.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="ResidualBlock.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="ResidualBlock.name_scope" class="variable">name_scope</dd>
                <dd id="ResidualBlock.submodules" class="variable">submodules</dd>
                <dd id="ResidualBlock.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="DenseBlock">
                                <div class="attr class">
        <a class="headerlink" href="#DenseBlock">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">DenseBlock</span><wbr>(<span class="base"><a href="#ConvBlock">ConvBlock</a></span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DenseBlock-233"><a href="#DenseBlock-233"><span class="linenos">233</span></a><span class="k">class</span> <span class="nc">DenseBlock</span><span class="p">(</span><span class="n">ConvBlock</span><span class="p">):</span> 
</span><span id="DenseBlock-234"><a href="#DenseBlock-234"><span class="linenos">234</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="DenseBlock-235"><a href="#DenseBlock-235"><span class="linenos">235</span></a><span class="sd">    Dense block.</span>
</span><span id="DenseBlock-236"><a href="#DenseBlock-236"><span class="linenos">236</span></a>
</span><span id="DenseBlock-237"><a href="#DenseBlock-237"><span class="linenos">237</span></a><span class="sd">    References</span>
</span><span id="DenseBlock-238"><a href="#DenseBlock-238"><span class="linenos">238</span></a><span class="sd">    ----------</span>
</span><span id="DenseBlock-239"><a href="#DenseBlock-239"><span class="linenos">239</span></a><span class="sd">    [1] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger</span>
</span><span id="DenseBlock-240"><a href="#DenseBlock-240"><span class="linenos">240</span></a><span class="sd">        Densely Connected Convolutional Networks: </span>
</span><span id="DenseBlock-241"><a href="#DenseBlock-241"><span class="linenos">241</span></a><span class="sd">        https://arxiv.org/abs/1608.06993</span>
</span><span id="DenseBlock-242"><a href="#DenseBlock-242"><span class="linenos">242</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="DenseBlock-243"><a href="#DenseBlock-243"><span class="linenos">243</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">ks_cl2</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> 
</span><span id="DenseBlock-244"><a href="#DenseBlock-244"><span class="linenos">244</span></a>                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span><span id="DenseBlock-245"><a href="#DenseBlock-245"><span class="linenos">245</span></a>                 <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="DenseBlock-246"><a href="#DenseBlock-246"><span class="linenos">246</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="p">,</span> <span class="n">ks_cl2</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> 
</span><span id="DenseBlock-247"><a href="#DenseBlock-247"><span class="linenos">247</span></a>                         <span class="n">normalization</span><span class="p">,</span> <span class="n">attention</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> 
</span><span id="DenseBlock-248"><a href="#DenseBlock-248"><span class="linenos">248</span></a>                         <span class="n">dropout_variant</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="DenseBlock-249"><a href="#DenseBlock-249"><span class="linenos">249</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span>
</span><span id="DenseBlock-250"><a href="#DenseBlock-250"><span class="linenos">250</span></a>            <span class="mi">4</span> <span class="o">*</span> <span class="n">filters</span><span class="p">,</span> 
</span><span id="DenseBlock-251"><a href="#DenseBlock-251"><span class="linenos">251</span></a>            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="DenseBlock-252"><a href="#DenseBlock-252"><span class="linenos">252</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl1</span><span class="p">,</span> 
</span><span id="DenseBlock-253"><a href="#DenseBlock-253"><span class="linenos">253</span></a>            <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> 
</span><span id="DenseBlock-254"><a href="#DenseBlock-254"><span class="linenos">254</span></a>            <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="DenseBlock-255"><a href="#DenseBlock-255"><span class="linenos">255</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span>
</span><span id="DenseBlock-256"><a href="#DenseBlock-256"><span class="linenos">256</span></a>            <span class="n">filters</span><span class="p">,</span> 
</span><span id="DenseBlock-257"><a href="#DenseBlock-257"><span class="linenos">257</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl2</span><span class="p">,</span> 
</span><span id="DenseBlock-258"><a href="#DenseBlock-258"><span class="linenos">258</span></a>            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="DenseBlock-259"><a href="#DenseBlock-259"><span class="linenos">259</span></a>            <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="DenseBlock-260"><a href="#DenseBlock-260"><span class="linenos">260</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">()</span>
</span><span id="DenseBlock-261"><a href="#DenseBlock-261"><span class="linenos">261</span></a>
</span><span id="DenseBlock-262"><a href="#DenseBlock-262"><span class="linenos">262</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="DenseBlock-263"><a href="#DenseBlock-263"><span class="linenos">263</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">X</span>
</span><span id="DenseBlock-264"><a href="#DenseBlock-264"><span class="linenos">264</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock-265"><a href="#DenseBlock-265"><span class="linenos">265</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="DenseBlock-266"><a href="#DenseBlock-266"><span class="linenos">266</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock-267"><a href="#DenseBlock-267"><span class="linenos">267</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>       
</span><span id="DenseBlock-268"><a href="#DenseBlock-268"><span class="linenos">268</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="DenseBlock-269"><a href="#DenseBlock-269"><span class="linenos">269</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock-270"><a href="#DenseBlock-270"><span class="linenos">270</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock-271"><a href="#DenseBlock-271"><span class="linenos">271</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="DenseBlock-272"><a href="#DenseBlock-272"><span class="linenos">272</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock-273"><a href="#DenseBlock-273"><span class="linenos">273</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock-274"><a href="#DenseBlock-274"><span class="linenos">274</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">:</span>
</span><span id="DenseBlock-275"><a href="#DenseBlock-275"><span class="linenos">275</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock-276"><a href="#DenseBlock-276"><span class="linenos">276</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">Y</span><span class="p">,</span> <span class="n">X</span><span class="p">])</span>
</span><span id="DenseBlock-277"><a href="#DenseBlock-277"><span class="linenos">277</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Dense block.</p>

<h6 id="references">References</h6>

<p>[1] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger
    Densely Connected Convolutional Networks: 
    https://arxiv.org/abs/1608.06993</p>
</div>


                            <div id="DenseBlock.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#DenseBlock.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">DenseBlock</span><span class="signature">(
    filters,
    strides=1,
    ks_cl1=(1, 1),
    ks_cl2=(3, 3),
    activation=&#39;relu&#39;,
    normalization=None,
    attention=False,
    dropout_rate=0,
    dropout_variant=None,
    name=None,
    **conv_kwargs
)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DenseBlock.__init__-243"><a href="#DenseBlock.__init__-243"><span class="linenos">243</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">ks_cl2</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> 
</span><span id="DenseBlock.__init__-244"><a href="#DenseBlock.__init__-244"><span class="linenos">244</span></a>                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span><span id="DenseBlock.__init__-245"><a href="#DenseBlock.__init__-245"><span class="linenos">245</span></a>                 <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="DenseBlock.__init__-246"><a href="#DenseBlock.__init__-246"><span class="linenos">246</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="p">,</span> <span class="n">ks_cl2</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> 
</span><span id="DenseBlock.__init__-247"><a href="#DenseBlock.__init__-247"><span class="linenos">247</span></a>                         <span class="n">normalization</span><span class="p">,</span> <span class="n">attention</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> 
</span><span id="DenseBlock.__init__-248"><a href="#DenseBlock.__init__-248"><span class="linenos">248</span></a>                         <span class="n">dropout_variant</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="DenseBlock.__init__-249"><a href="#DenseBlock.__init__-249"><span class="linenos">249</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span>
</span><span id="DenseBlock.__init__-250"><a href="#DenseBlock.__init__-250"><span class="linenos">250</span></a>            <span class="mi">4</span> <span class="o">*</span> <span class="n">filters</span><span class="p">,</span> 
</span><span id="DenseBlock.__init__-251"><a href="#DenseBlock.__init__-251"><span class="linenos">251</span></a>            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="DenseBlock.__init__-252"><a href="#DenseBlock.__init__-252"><span class="linenos">252</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl1</span><span class="p">,</span> 
</span><span id="DenseBlock.__init__-253"><a href="#DenseBlock.__init__-253"><span class="linenos">253</span></a>            <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> 
</span><span id="DenseBlock.__init__-254"><a href="#DenseBlock.__init__-254"><span class="linenos">254</span></a>            <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="DenseBlock.__init__-255"><a href="#DenseBlock.__init__-255"><span class="linenos">255</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span>
</span><span id="DenseBlock.__init__-256"><a href="#DenseBlock.__init__-256"><span class="linenos">256</span></a>            <span class="n">filters</span><span class="p">,</span> 
</span><span id="DenseBlock.__init__-257"><a href="#DenseBlock.__init__-257"><span class="linenos">257</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl2</span><span class="p">,</span> 
</span><span id="DenseBlock.__init__-258"><a href="#DenseBlock.__init__-258"><span class="linenos">258</span></a>            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="DenseBlock.__init__-259"><a href="#DenseBlock.__init__-259"><span class="linenos">259</span></a>            <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="DenseBlock.__init__-260"><a href="#DenseBlock.__init__-260"><span class="linenos">260</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">()</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="DenseBlock.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#DenseBlock.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, X)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DenseBlock.call-262"><a href="#DenseBlock.call-262"><span class="linenos">262</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="DenseBlock.call-263"><a href="#DenseBlock.call-263"><span class="linenos">263</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">X</span>
</span><span id="DenseBlock.call-264"><a href="#DenseBlock.call-264"><span class="linenos">264</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock.call-265"><a href="#DenseBlock.call-265"><span class="linenos">265</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="DenseBlock.call-266"><a href="#DenseBlock.call-266"><span class="linenos">266</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock.call-267"><a href="#DenseBlock.call-267"><span class="linenos">267</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>       
</span><span id="DenseBlock.call-268"><a href="#DenseBlock.call-268"><span class="linenos">268</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="DenseBlock.call-269"><a href="#DenseBlock.call-269"><span class="linenos">269</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock.call-270"><a href="#DenseBlock.call-270"><span class="linenos">270</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock.call-271"><a href="#DenseBlock.call-271"><span class="linenos">271</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="DenseBlock.call-272"><a href="#DenseBlock.call-272"><span class="linenos">272</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock.call-273"><a href="#DenseBlock.call-273"><span class="linenos">273</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock.call-274"><a href="#DenseBlock.call-274"><span class="linenos">274</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">:</span>
</span><span id="DenseBlock.call-275"><a href="#DenseBlock.call-275"><span class="linenos">275</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="DenseBlock.call-276"><a href="#DenseBlock.call-276"><span class="linenos">276</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">Y</span><span class="p">,</span> <span class="n">X</span><span class="p">])</span>
</span><span id="DenseBlock.call-277"><a href="#DenseBlock.call-277"><span class="linenos">277</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Model's forward pass.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="DenseBlock.build" class="function">build</dd>
                <dd id="DenseBlock.add_weight" class="function">add_weight</dd>
                <dd id="DenseBlock.get_config" class="function">get_config</dd>
                <dd id="DenseBlock.from_config" class="function">from_config</dd>
                <dd id="DenseBlock.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="DenseBlock.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="DenseBlock.compute_mask" class="function">compute_mask</dd>
                <dd id="DenseBlock.dtype" class="variable">dtype</dd>
                <dd id="DenseBlock.name" class="variable">name</dd>
                <dd id="DenseBlock.supports_masking" class="variable">supports_masking</dd>
                <dd id="DenseBlock.dynamic" class="variable">dynamic</dd>
                <dd id="DenseBlock.stateful" class="variable">stateful</dd>
                <dd id="DenseBlock.trainable" class="variable">trainable</dd>
                <dd id="DenseBlock.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="DenseBlock.input_spec" class="variable">input_spec</dd>
                <dd id="DenseBlock.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="DenseBlock.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="DenseBlock.weights" class="variable">weights</dd>
                <dd id="DenseBlock.updates" class="variable">updates</dd>
                <dd id="DenseBlock.losses" class="variable">losses</dd>
                <dd id="DenseBlock.add_loss" class="function">add_loss</dd>
                <dd id="DenseBlock.metrics" class="variable">metrics</dd>
                <dd id="DenseBlock.add_metric" class="function">add_metric</dd>
                <dd id="DenseBlock.add_update" class="function">add_update</dd>
                <dd id="DenseBlock.set_weights" class="function">set_weights</dd>
                <dd id="DenseBlock.get_weights" class="function">get_weights</dd>
                <dd id="DenseBlock.finalize_state" class="function">finalize_state</dd>
                <dd id="DenseBlock.get_updates_for" class="function">get_updates_for</dd>
                <dd id="DenseBlock.get_losses_for" class="function">get_losses_for</dd>
                <dd id="DenseBlock.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="DenseBlock.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="DenseBlock.input_mask" class="variable">input_mask</dd>
                <dd id="DenseBlock.output_mask" class="variable">output_mask</dd>
                <dd id="DenseBlock.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="DenseBlock.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="DenseBlock.get_input_at" class="function">get_input_at</dd>
                <dd id="DenseBlock.get_output_at" class="function">get_output_at</dd>
                <dd id="DenseBlock.input" class="variable">input</dd>
                <dd id="DenseBlock.output" class="variable">output</dd>
                <dd id="DenseBlock.input_shape" class="variable">input_shape</dd>
                <dd id="DenseBlock.count_params" class="function">count_params</dd>
                <dd id="DenseBlock.output_shape" class="variable">output_shape</dd>
                <dd id="DenseBlock.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="DenseBlock.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="DenseBlock.apply" class="function">apply</dd>
                <dd id="DenseBlock.add_variable" class="function">add_variable</dd>
                <dd id="DenseBlock.variables" class="variable">variables</dd>
                <dd id="DenseBlock.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="DenseBlock.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="DenseBlock.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="DenseBlock.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="DenseBlock.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="DenseBlock.name_scope" class="variable">name_scope</dd>
                <dd id="DenseBlock.submodules" class="variable">submodules</dd>
                <dd id="DenseBlock.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="TransitionBlock">
                                <div class="attr class">
        <a class="headerlink" href="#TransitionBlock">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">TransitionBlock</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TransitionBlock-280"><a href="#TransitionBlock-280"><span class="linenos">280</span></a><span class="k">class</span> <span class="nc">TransitionBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="TransitionBlock-281"><a href="#TransitionBlock-281"><span class="linenos">281</span></a>    <span class="sd">&quot;&quot;&quot; </span>
</span><span id="TransitionBlock-282"><a href="#TransitionBlock-282"><span class="linenos">282</span></a><span class="sd">    Transition layer to control the complexity of the model by using 1x1 </span>
</span><span id="TransitionBlock-283"><a href="#TransitionBlock-283"><span class="linenos">283</span></a><span class="sd">    convolutions. Used in architectures, such as the Densenet.</span>
</span><span id="TransitionBlock-284"><a href="#TransitionBlock-284"><span class="linenos">284</span></a>
</span><span id="TransitionBlock-285"><a href="#TransitionBlock-285"><span class="linenos">285</span></a><span class="sd">    References</span>
</span><span id="TransitionBlock-286"><a href="#TransitionBlock-286"><span class="linenos">286</span></a><span class="sd">    ----------</span>
</span><span id="TransitionBlock-287"><a href="#TransitionBlock-287"><span class="linenos">287</span></a><span class="sd">    [1] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger</span>
</span><span id="TransitionBlock-288"><a href="#TransitionBlock-288"><span class="linenos">288</span></a><span class="sd">        Densely Connected Convolutional Networks: </span>
</span><span id="TransitionBlock-289"><a href="#TransitionBlock-289"><span class="linenos">289</span></a><span class="sd">        https://arxiv.org/abs/1608.06993</span>
</span><span id="TransitionBlock-290"><a href="#TransitionBlock-290"><span class="linenos">290</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="TransitionBlock-291"><a href="#TransitionBlock-291"><span class="linenos">291</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="TransitionBlock-292"><a href="#TransitionBlock-292"><span class="linenos">292</span></a>                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="TransitionBlock-293"><a href="#TransitionBlock-293"><span class="linenos">293</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="TransitionBlock-294"><a href="#TransitionBlock-294"><span class="linenos">294</span></a>        <span class="k">if</span> <span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;bn&#39;</span><span class="p">:</span>
</span><span id="TransitionBlock-295"><a href="#TransitionBlock-295"><span class="linenos">295</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="TransitionBlock-296"><a href="#TransitionBlock-296"><span class="linenos">296</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="TransitionBlock-297"><a href="#TransitionBlock-297"><span class="linenos">297</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="TransitionBlock-298"><a href="#TransitionBlock-298"><span class="linenos">298</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</span><span id="TransitionBlock-299"><a href="#TransitionBlock-299"><span class="linenos">299</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="TransitionBlock-300"><a href="#TransitionBlock-300"><span class="linenos">300</span></a>
</span><span id="TransitionBlock-301"><a href="#TransitionBlock-301"><span class="linenos">301</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="TransitionBlock-302"><a href="#TransitionBlock-302"><span class="linenos">302</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="TransitionBlock-303"><a href="#TransitionBlock-303"><span class="linenos">303</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="TransitionBlock-304"><a href="#TransitionBlock-304"><span class="linenos">304</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="TransitionBlock-305"><a href="#TransitionBlock-305"><span class="linenos">305</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="TransitionBlock-306"><a href="#TransitionBlock-306"><span class="linenos">306</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="TransitionBlock-307"><a href="#TransitionBlock-307"><span class="linenos">307</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="TransitionBlock-308"><a href="#TransitionBlock-308"><span class="linenos">308</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="TransitionBlock-309"><a href="#TransitionBlock-309"><span class="linenos">309</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Transition layer to control the complexity of the model by using 1x1 
convolutions. Used in architectures, such as the Densenet.</p>

<h6 id="references">References</h6>

<p>[1] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger
    Densely Connected Convolutional Networks: 
    https://arxiv.org/abs/1608.06993</p>
</div>


                            <div id="TransitionBlock.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#TransitionBlock.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">TransitionBlock</span><span class="signature">(filters, activation=&#39;relu&#39;, normalization=None, name=None, **kwargs)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TransitionBlock.__init__-291"><a href="#TransitionBlock.__init__-291"><span class="linenos">291</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="TransitionBlock.__init__-292"><a href="#TransitionBlock.__init__-292"><span class="linenos">292</span></a>                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="TransitionBlock.__init__-293"><a href="#TransitionBlock.__init__-293"><span class="linenos">293</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="TransitionBlock.__init__-294"><a href="#TransitionBlock.__init__-294"><span class="linenos">294</span></a>        <span class="k">if</span> <span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;bn&#39;</span><span class="p">:</span>
</span><span id="TransitionBlock.__init__-295"><a href="#TransitionBlock.__init__-295"><span class="linenos">295</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="TransitionBlock.__init__-296"><a href="#TransitionBlock.__init__-296"><span class="linenos">296</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="TransitionBlock.__init__-297"><a href="#TransitionBlock.__init__-297"><span class="linenos">297</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="TransitionBlock.__init__-298"><a href="#TransitionBlock.__init__-298"><span class="linenos">298</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</span><span id="TransitionBlock.__init__-299"><a href="#TransitionBlock.__init__-299"><span class="linenos">299</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="TransitionBlock.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#TransitionBlock.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, X)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TransitionBlock.call-301"><a href="#TransitionBlock.call-301"><span class="linenos">301</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="TransitionBlock.call-302"><a href="#TransitionBlock.call-302"><span class="linenos">302</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="TransitionBlock.call-303"><a href="#TransitionBlock.call-303"><span class="linenos">303</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="TransitionBlock.call-304"><a href="#TransitionBlock.call-304"><span class="linenos">304</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="TransitionBlock.call-305"><a href="#TransitionBlock.call-305"><span class="linenos">305</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="TransitionBlock.call-306"><a href="#TransitionBlock.call-306"><span class="linenos">306</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="TransitionBlock.call-307"><a href="#TransitionBlock.call-307"><span class="linenos">307</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="TransitionBlock.call-308"><a href="#TransitionBlock.call-308"><span class="linenos">308</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="TransitionBlock.call-309"><a href="#TransitionBlock.call-309"><span class="linenos">309</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>The <code><a href="#TransitionBlock.call">call()</a></code> method may not create state (except in its first invocation,
wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).
It is recommended to create state in <code><a href="#TransitionBlock.__init__">__init__()</a></code>, or the <code><a href="#TransitionBlock.build">build()</a></code> method
that is called automatically before <code><a href="#TransitionBlock.call">call()</a></code> executes the first time.</p>

<p>Args:
  inputs: Input tensor, or dict/list/tuple of input tensors.
    The first positional <code>inputs</code> argument is subject to special rules:
    - <code>inputs</code> must be explicitly passed. A layer cannot have zero
      arguments, and <code>inputs</code> cannot be provided via the default value
      of a keyword argument.
    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.
    - Keras mask metadata is only collected from <code>inputs</code>.
    - Layers are built (<code>build(input_shape)</code> method)
      using shape info from <code>inputs</code> only.
    - <code><a href="#TransitionBlock.input_spec">input_spec</a></code> compatibility is only checked against <code>inputs</code>.
    - Mixed precision input casting is only applied to <code>inputs</code>.
      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their
      casting behavior in mixed precision should be handled manually.
    - The SavedModel input specification is generated using <code>inputs</code> only.
    - Integration with various ecosystem packages like TFMOT, TFLite,
      TF.js, etc is only supported for <code>inputs</code> and not for tensors in
      positional and keyword arguments.
  <em>args: Additional positional arguments. May contain tensors, although
    this is not recommended, for the reasons above.
  *</em>kwargs: Additional keyword arguments. May contain tensors, although
    this is not recommended, for the reasons above.
    The following optional keyword arguments are reserved:
    - <code>training</code>: Boolean scalar tensor of Python boolean indicating
      whether the <code><a href="#TransitionBlock.call">call</a></code> is meant for training or inference.
    - <code>mask</code>: Boolean input mask. If the layer's <code><a href="#TransitionBlock.call">call()</a></code> method takes a
      <code>mask</code> argument, its default value will be set to the mask generated
      for <code>inputs</code> by the previous layer (if <code><a href="#TransitionBlock.input">input</a></code> did come from a layer
      that generated a corresponding mask, i.e. if it came from a Keras
      layer with masking support).</p>

<p>Returns:
  A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="TransitionBlock.build" class="function">build</dd>
                <dd id="TransitionBlock.add_weight" class="function">add_weight</dd>
                <dd id="TransitionBlock.get_config" class="function">get_config</dd>
                <dd id="TransitionBlock.from_config" class="function">from_config</dd>
                <dd id="TransitionBlock.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="TransitionBlock.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="TransitionBlock.compute_mask" class="function">compute_mask</dd>
                <dd id="TransitionBlock.dtype" class="variable">dtype</dd>
                <dd id="TransitionBlock.name" class="variable">name</dd>
                <dd id="TransitionBlock.supports_masking" class="variable">supports_masking</dd>
                <dd id="TransitionBlock.dynamic" class="variable">dynamic</dd>
                <dd id="TransitionBlock.stateful" class="variable">stateful</dd>
                <dd id="TransitionBlock.trainable" class="variable">trainable</dd>
                <dd id="TransitionBlock.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="TransitionBlock.input_spec" class="variable">input_spec</dd>
                <dd id="TransitionBlock.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="TransitionBlock.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="TransitionBlock.weights" class="variable">weights</dd>
                <dd id="TransitionBlock.updates" class="variable">updates</dd>
                <dd id="TransitionBlock.losses" class="variable">losses</dd>
                <dd id="TransitionBlock.add_loss" class="function">add_loss</dd>
                <dd id="TransitionBlock.metrics" class="variable">metrics</dd>
                <dd id="TransitionBlock.add_metric" class="function">add_metric</dd>
                <dd id="TransitionBlock.add_update" class="function">add_update</dd>
                <dd id="TransitionBlock.set_weights" class="function">set_weights</dd>
                <dd id="TransitionBlock.get_weights" class="function">get_weights</dd>
                <dd id="TransitionBlock.finalize_state" class="function">finalize_state</dd>
                <dd id="TransitionBlock.get_updates_for" class="function">get_updates_for</dd>
                <dd id="TransitionBlock.get_losses_for" class="function">get_losses_for</dd>
                <dd id="TransitionBlock.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="TransitionBlock.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="TransitionBlock.input_mask" class="variable">input_mask</dd>
                <dd id="TransitionBlock.output_mask" class="variable">output_mask</dd>
                <dd id="TransitionBlock.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="TransitionBlock.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="TransitionBlock.get_input_at" class="function">get_input_at</dd>
                <dd id="TransitionBlock.get_output_at" class="function">get_output_at</dd>
                <dd id="TransitionBlock.input" class="variable">input</dd>
                <dd id="TransitionBlock.output" class="variable">output</dd>
                <dd id="TransitionBlock.input_shape" class="variable">input_shape</dd>
                <dd id="TransitionBlock.count_params" class="function">count_params</dd>
                <dd id="TransitionBlock.output_shape" class="variable">output_shape</dd>
                <dd id="TransitionBlock.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="TransitionBlock.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="TransitionBlock.apply" class="function">apply</dd>
                <dd id="TransitionBlock.add_variable" class="function">add_variable</dd>
                <dd id="TransitionBlock.variables" class="variable">variables</dd>
                <dd id="TransitionBlock.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="TransitionBlock.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="TransitionBlock.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="TransitionBlock.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="TransitionBlock.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="TransitionBlock.name_scope" class="variable">name_scope</dd>
                <dd id="TransitionBlock.submodules" class="variable">submodules</dd>
                <dd id="TransitionBlock.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="LocalizedConvBlock">
                                <div class="attr class">
        <a class="headerlink" href="#LocalizedConvBlock">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">LocalizedConvBlock</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LocalizedConvBlock-312"><a href="#LocalizedConvBlock-312"><span class="linenos">312</span></a><span class="k">class</span> <span class="nc">LocalizedConvBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="LocalizedConvBlock-313"><a href="#LocalizedConvBlock-313"><span class="linenos">313</span></a>    <span class="sd">&quot;&quot;&quot; </span>
</span><span id="LocalizedConvBlock-314"><a href="#LocalizedConvBlock-314"><span class="linenos">314</span></a><span class="sd">    Localized convolutional block through a locally connected layer (1x1 kernel) </span>
</span><span id="LocalizedConvBlock-315"><a href="#LocalizedConvBlock-315"><span class="linenos">315</span></a><span class="sd">    with biases.</span>
</span><span id="LocalizedConvBlock-316"><a href="#LocalizedConvBlock-316"><span class="linenos">316</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="LocalizedConvBlock-317"><a href="#LocalizedConvBlock-317"><span class="linenos">317</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
</span><span id="LocalizedConvBlock-318"><a href="#LocalizedConvBlock-318"><span class="linenos">318</span></a>                 <span class="n">name_sufix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="LocalizedConvBlock-319"><a href="#LocalizedConvBlock-319"><span class="linenos">319</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;LocalizedConvBlock&#39;</span> <span class="o">+</span> <span class="n">name_sufix</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="LocalizedConvBlock-320"><a href="#LocalizedConvBlock-320"><span class="linenos">320</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">filters</span> <span class="o">=</span> <span class="n">filters</span>
</span><span id="LocalizedConvBlock-321"><a href="#LocalizedConvBlock-321"><span class="linenos">321</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">transition</span> <span class="o">=</span> <span class="n">TransitionBlock</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">)</span>
</span><span id="LocalizedConvBlock-322"><a href="#LocalizedConvBlock-322"><span class="linenos">322</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">localconv</span> <span class="o">=</span> <span class="n">LocallyConnected2D</span><span class="p">(</span>
</span><span id="LocalizedConvBlock-323"><a href="#LocalizedConvBlock-323"><span class="linenos">323</span></a>            <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
</span><span id="LocalizedConvBlock-324"><a href="#LocalizedConvBlock-324"><span class="linenos">324</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="LocalizedConvBlock-325"><a href="#LocalizedConvBlock-325"><span class="linenos">325</span></a>            <span class="n">implementation</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="LocalizedConvBlock-326"><a href="#LocalizedConvBlock-326"><span class="linenos">326</span></a>            <span class="n">bias_initializer</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
</span><span id="LocalizedConvBlock-327"><a href="#LocalizedConvBlock-327"><span class="linenos">327</span></a>            <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
</span><span id="LocalizedConvBlock-328"><a href="#LocalizedConvBlock-328"><span class="linenos">328</span></a>            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>
</span><span id="LocalizedConvBlock-329"><a href="#LocalizedConvBlock-329"><span class="linenos">329</span></a>
</span><span id="LocalizedConvBlock-330"><a href="#LocalizedConvBlock-330"><span class="linenos">330</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="LocalizedConvBlock-331"><a href="#LocalizedConvBlock-331"><span class="linenos">331</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transition</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="LocalizedConvBlock-332"><a href="#LocalizedConvBlock-332"><span class="linenos">332</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">localconv</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="LocalizedConvBlock-333"><a href="#LocalizedConvBlock-333"><span class="linenos">333</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span><span id="LocalizedConvBlock-334"><a href="#LocalizedConvBlock-334"><span class="linenos">334</span></a>
</span><span id="LocalizedConvBlock-335"><a href="#LocalizedConvBlock-335"><span class="linenos">335</span></a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="LocalizedConvBlock-336"><a href="#LocalizedConvBlock-336"><span class="linenos">336</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">)</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Localized convolutional block through a locally connected layer (1x1 kernel) 
with biases.</p>
</div>


                            <div id="LocalizedConvBlock.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#LocalizedConvBlock.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">LocalizedConvBlock</span><span class="signature">(filters=2, activation=None, use_bias=True, name_sufix=&#39;&#39;, **kwargs)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LocalizedConvBlock.__init__-317"><a href="#LocalizedConvBlock.__init__-317"><span class="linenos">317</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
</span><span id="LocalizedConvBlock.__init__-318"><a href="#LocalizedConvBlock.__init__-318"><span class="linenos">318</span></a>                 <span class="n">name_sufix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="LocalizedConvBlock.__init__-319"><a href="#LocalizedConvBlock.__init__-319"><span class="linenos">319</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;LocalizedConvBlock&#39;</span> <span class="o">+</span> <span class="n">name_sufix</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="LocalizedConvBlock.__init__-320"><a href="#LocalizedConvBlock.__init__-320"><span class="linenos">320</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">filters</span> <span class="o">=</span> <span class="n">filters</span>
</span><span id="LocalizedConvBlock.__init__-321"><a href="#LocalizedConvBlock.__init__-321"><span class="linenos">321</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">transition</span> <span class="o">=</span> <span class="n">TransitionBlock</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">)</span>
</span><span id="LocalizedConvBlock.__init__-322"><a href="#LocalizedConvBlock.__init__-322"><span class="linenos">322</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">localconv</span> <span class="o">=</span> <span class="n">LocallyConnected2D</span><span class="p">(</span>
</span><span id="LocalizedConvBlock.__init__-323"><a href="#LocalizedConvBlock.__init__-323"><span class="linenos">323</span></a>            <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
</span><span id="LocalizedConvBlock.__init__-324"><a href="#LocalizedConvBlock.__init__-324"><span class="linenos">324</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="LocalizedConvBlock.__init__-325"><a href="#LocalizedConvBlock.__init__-325"><span class="linenos">325</span></a>            <span class="n">implementation</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="LocalizedConvBlock.__init__-326"><a href="#LocalizedConvBlock.__init__-326"><span class="linenos">326</span></a>            <span class="n">bias_initializer</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
</span><span id="LocalizedConvBlock.__init__-327"><a href="#LocalizedConvBlock.__init__-327"><span class="linenos">327</span></a>            <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
</span><span id="LocalizedConvBlock.__init__-328"><a href="#LocalizedConvBlock.__init__-328"><span class="linenos">328</span></a>            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="LocalizedConvBlock.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#LocalizedConvBlock.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, X)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LocalizedConvBlock.call-330"><a href="#LocalizedConvBlock.call-330"><span class="linenos">330</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="LocalizedConvBlock.call-331"><a href="#LocalizedConvBlock.call-331"><span class="linenos">331</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transition</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="LocalizedConvBlock.call-332"><a href="#LocalizedConvBlock.call-332"><span class="linenos">332</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">localconv</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="LocalizedConvBlock.call-333"><a href="#LocalizedConvBlock.call-333"><span class="linenos">333</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>The <code><a href="#LocalizedConvBlock.call">call()</a></code> method may not create state (except in its first invocation,
wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).
It is recommended to create state in <code><a href="#LocalizedConvBlock.__init__">__init__()</a></code>, or the <code><a href="#LocalizedConvBlock.build">build()</a></code> method
that is called automatically before <code><a href="#LocalizedConvBlock.call">call()</a></code> executes the first time.</p>

<p>Args:
  inputs: Input tensor, or dict/list/tuple of input tensors.
    The first positional <code>inputs</code> argument is subject to special rules:
    - <code>inputs</code> must be explicitly passed. A layer cannot have zero
      arguments, and <code>inputs</code> cannot be provided via the default value
      of a keyword argument.
    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.
    - Keras mask metadata is only collected from <code>inputs</code>.
    - Layers are built (<code>build(input_shape)</code> method)
      using shape info from <code>inputs</code> only.
    - <code><a href="#LocalizedConvBlock.input_spec">input_spec</a></code> compatibility is only checked against <code>inputs</code>.
    - Mixed precision input casting is only applied to <code>inputs</code>.
      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their
      casting behavior in mixed precision should be handled manually.
    - The SavedModel input specification is generated using <code>inputs</code> only.
    - Integration with various ecosystem packages like TFMOT, TFLite,
      TF.js, etc is only supported for <code>inputs</code> and not for tensors in
      positional and keyword arguments.
  <em>args: Additional positional arguments. May contain tensors, although
    this is not recommended, for the reasons above.
  *</em>kwargs: Additional keyword arguments. May contain tensors, although
    this is not recommended, for the reasons above.
    The following optional keyword arguments are reserved:
    - <code>training</code>: Boolean scalar tensor of Python boolean indicating
      whether the <code><a href="#LocalizedConvBlock.call">call</a></code> is meant for training or inference.
    - <code>mask</code>: Boolean input mask. If the layer's <code><a href="#LocalizedConvBlock.call">call()</a></code> method takes a
      <code>mask</code> argument, its default value will be set to the mask generated
      for <code>inputs</code> by the previous layer (if <code><a href="#LocalizedConvBlock.input">input</a></code> did come from a layer
      that generated a corresponding mask, i.e. if it came from a Keras
      layer with masking support).</p>

<p>Returns:
  A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div id="LocalizedConvBlock.compute_output_shape" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#LocalizedConvBlock.compute_output_shape">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">compute_output_shape</span><span class="signature">(self, input_shape)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LocalizedConvBlock.compute_output_shape-335"><a href="#LocalizedConvBlock.compute_output_shape-335"><span class="linenos">335</span></a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="LocalizedConvBlock.compute_output_shape-336"><a href="#LocalizedConvBlock.compute_output_shape-336"><span class="linenos">336</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">)</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Computes the output shape of the layer.</p>

<p>This method will cause the layer's state to be built, if that has not
happened before. This requires that the layer will later be used with
inputs that match the input shape provided here.</p>

<p>Args:
    input_shape: Shape tuple (tuple of integers)
        or list of shape tuples (one per output tensor of the layer).
        Shape tuples can include None for free dimensions,
        instead of an integer.</p>

<p>Returns:
    An input shape tuple.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="LocalizedConvBlock.build" class="function">build</dd>
                <dd id="LocalizedConvBlock.add_weight" class="function">add_weight</dd>
                <dd id="LocalizedConvBlock.get_config" class="function">get_config</dd>
                <dd id="LocalizedConvBlock.from_config" class="function">from_config</dd>
                <dd id="LocalizedConvBlock.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="LocalizedConvBlock.compute_mask" class="function">compute_mask</dd>
                <dd id="LocalizedConvBlock.dtype" class="variable">dtype</dd>
                <dd id="LocalizedConvBlock.name" class="variable">name</dd>
                <dd id="LocalizedConvBlock.supports_masking" class="variable">supports_masking</dd>
                <dd id="LocalizedConvBlock.dynamic" class="variable">dynamic</dd>
                <dd id="LocalizedConvBlock.stateful" class="variable">stateful</dd>
                <dd id="LocalizedConvBlock.trainable" class="variable">trainable</dd>
                <dd id="LocalizedConvBlock.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="LocalizedConvBlock.input_spec" class="variable">input_spec</dd>
                <dd id="LocalizedConvBlock.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="LocalizedConvBlock.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="LocalizedConvBlock.weights" class="variable">weights</dd>
                <dd id="LocalizedConvBlock.updates" class="variable">updates</dd>
                <dd id="LocalizedConvBlock.losses" class="variable">losses</dd>
                <dd id="LocalizedConvBlock.add_loss" class="function">add_loss</dd>
                <dd id="LocalizedConvBlock.metrics" class="variable">metrics</dd>
                <dd id="LocalizedConvBlock.add_metric" class="function">add_metric</dd>
                <dd id="LocalizedConvBlock.add_update" class="function">add_update</dd>
                <dd id="LocalizedConvBlock.set_weights" class="function">set_weights</dd>
                <dd id="LocalizedConvBlock.get_weights" class="function">get_weights</dd>
                <dd id="LocalizedConvBlock.finalize_state" class="function">finalize_state</dd>
                <dd id="LocalizedConvBlock.get_updates_for" class="function">get_updates_for</dd>
                <dd id="LocalizedConvBlock.get_losses_for" class="function">get_losses_for</dd>
                <dd id="LocalizedConvBlock.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="LocalizedConvBlock.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="LocalizedConvBlock.input_mask" class="variable">input_mask</dd>
                <dd id="LocalizedConvBlock.output_mask" class="variable">output_mask</dd>
                <dd id="LocalizedConvBlock.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="LocalizedConvBlock.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="LocalizedConvBlock.get_input_at" class="function">get_input_at</dd>
                <dd id="LocalizedConvBlock.get_output_at" class="function">get_output_at</dd>
                <dd id="LocalizedConvBlock.input" class="variable">input</dd>
                <dd id="LocalizedConvBlock.output" class="variable">output</dd>
                <dd id="LocalizedConvBlock.input_shape" class="variable">input_shape</dd>
                <dd id="LocalizedConvBlock.count_params" class="function">count_params</dd>
                <dd id="LocalizedConvBlock.output_shape" class="variable">output_shape</dd>
                <dd id="LocalizedConvBlock.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="LocalizedConvBlock.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="LocalizedConvBlock.apply" class="function">apply</dd>
                <dd id="LocalizedConvBlock.add_variable" class="function">add_variable</dd>
                <dd id="LocalizedConvBlock.variables" class="variable">variables</dd>
                <dd id="LocalizedConvBlock.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="LocalizedConvBlock.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="LocalizedConvBlock.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="LocalizedConvBlock.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="LocalizedConvBlock.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="LocalizedConvBlock.name_scope" class="variable">name_scope</dd>
                <dd id="LocalizedConvBlock.submodules" class="variable">submodules</dd>
                <dd id="LocalizedConvBlock.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="RecurrentConvBlock">
                                <div class="attr class">
        <a class="headerlink" href="#RecurrentConvBlock">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">RecurrentConvBlock</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RecurrentConvBlock-339"><a href="#RecurrentConvBlock-339"><span class="linenos">339</span></a><span class="k">class</span> <span class="nc">RecurrentConvBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span> 
</span><span id="RecurrentConvBlock-340"><a href="#RecurrentConvBlock-340"><span class="linenos">340</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="RecurrentConvBlock-341"><a href="#RecurrentConvBlock-341"><span class="linenos">341</span></a><span class="sd">    Recurrent convolutional block.</span>
</span><span id="RecurrentConvBlock-342"><a href="#RecurrentConvBlock-342"><span class="linenos">342</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="RecurrentConvBlock-343"><a href="#RecurrentConvBlock-343"><span class="linenos">343</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">ks_cl2</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> 
</span><span id="RecurrentConvBlock-344"><a href="#RecurrentConvBlock-344"><span class="linenos">344</span></a>                 <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="RecurrentConvBlock-345"><a href="#RecurrentConvBlock-345"><span class="linenos">345</span></a>                 <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="RecurrentConvBlock-346"><a href="#RecurrentConvBlock-346"><span class="linenos">346</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;RecurrentConvBlock&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-347"><a href="#RecurrentConvBlock-347"><span class="linenos">347</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
</span><span id="RecurrentConvBlock-348"><a href="#RecurrentConvBlock-348"><span class="linenos">348</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>
</span><span id="RecurrentConvBlock-349"><a href="#RecurrentConvBlock-349"><span class="linenos">349</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span> <span class="o">=</span> <span class="n">dropout_variant</span>
</span><span id="RecurrentConvBlock-350"><a href="#RecurrentConvBlock-350"><span class="linenos">350</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">convlstm1</span> <span class="o">=</span> <span class="n">ConvLSTM2D</span><span class="p">(</span>
</span><span id="RecurrentConvBlock-351"><a href="#RecurrentConvBlock-351"><span class="linenos">351</span></a>            <span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl1</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="RecurrentConvBlock-352"><a href="#RecurrentConvBlock-352"><span class="linenos">352</span></a>            <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-353"><a href="#RecurrentConvBlock-353"><span class="linenos">353</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">convlstm2</span> <span class="o">=</span> <span class="n">ConvLSTM2D</span><span class="p">(</span>
</span><span id="RecurrentConvBlock-354"><a href="#RecurrentConvBlock-354"><span class="linenos">354</span></a>            <span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl2</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="RecurrentConvBlock-355"><a href="#RecurrentConvBlock-355"><span class="linenos">355</span></a>            <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-356"><a href="#RecurrentConvBlock-356"><span class="linenos">356</span></a>
</span><span id="RecurrentConvBlock-357"><a href="#RecurrentConvBlock-357"><span class="linenos">357</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="RecurrentConvBlock-358"><a href="#RecurrentConvBlock-358"><span class="linenos">358</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bn&#39;</span><span class="p">,</span> <span class="s1">&#39;ln&#39;</span><span class="p">]:</span>
</span><span id="RecurrentConvBlock-359"><a href="#RecurrentConvBlock-359"><span class="linenos">359</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Normalization not supported, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-360"><a href="#RecurrentConvBlock-360"><span class="linenos">360</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;bn&#39;</span><span class="p">:</span>
</span><span id="RecurrentConvBlock-361"><a href="#RecurrentConvBlock-361"><span class="linenos">361</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="RecurrentConvBlock-362"><a href="#RecurrentConvBlock-362"><span class="linenos">362</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="RecurrentConvBlock-363"><a href="#RecurrentConvBlock-363"><span class="linenos">363</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;ln&#39;</span><span class="p">:</span>
</span><span id="RecurrentConvBlock-364"><a href="#RecurrentConvBlock-364"><span class="linenos">364</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">()</span>
</span><span id="RecurrentConvBlock-365"><a href="#RecurrentConvBlock-365"><span class="linenos">365</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">()</span>
</span><span id="RecurrentConvBlock-366"><a href="#RecurrentConvBlock-366"><span class="linenos">366</span></a>
</span><span id="RecurrentConvBlock-367"><a href="#RecurrentConvBlock-367"><span class="linenos">367</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-368"><a href="#RecurrentConvBlock-368"><span class="linenos">368</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">skipconnection</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()</span>
</span><span id="RecurrentConvBlock-369"><a href="#RecurrentConvBlock-369"><span class="linenos">369</span></a>
</span><span id="RecurrentConvBlock-370"><a href="#RecurrentConvBlock-370"><span class="linenos">370</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="RecurrentConvBlock-371"><a href="#RecurrentConvBlock-371"><span class="linenos">371</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="RecurrentConvBlock-372"><a href="#RecurrentConvBlock-372"><span class="linenos">372</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">get_dropout_layer</span><span class="p">(</span>
</span><span id="RecurrentConvBlock-373"><a href="#RecurrentConvBlock-373"><span class="linenos">373</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-374"><a href="#RecurrentConvBlock-374"><span class="linenos">374</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">get_dropout_layer</span><span class="p">(</span>
</span><span id="RecurrentConvBlock-375"><a href="#RecurrentConvBlock-375"><span class="linenos">375</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>            
</span><span id="RecurrentConvBlock-376"><a href="#RecurrentConvBlock-376"><span class="linenos">376</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="RecurrentConvBlock-377"><a href="#RecurrentConvBlock-377"><span class="linenos">377</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="RecurrentConvBlock-378"><a href="#RecurrentConvBlock-378"><span class="linenos">378</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="RecurrentConvBlock-379"><a href="#RecurrentConvBlock-379"><span class="linenos">379</span></a>
</span><span id="RecurrentConvBlock-380"><a href="#RecurrentConvBlock-380"><span class="linenos">380</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="RecurrentConvBlock-381"><a href="#RecurrentConvBlock-381"><span class="linenos">381</span></a>        <span class="sd">&quot;&quot;&quot;</span>
</span><span id="RecurrentConvBlock-382"><a href="#RecurrentConvBlock-382"><span class="linenos">382</span></a><span class="sd">        Forward pass. </span>
</span><span id="RecurrentConvBlock-383"><a href="#RecurrentConvBlock-383"><span class="linenos">383</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="RecurrentConvBlock-384"><a href="#RecurrentConvBlock-384"><span class="linenos">384</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="RecurrentConvBlock-385"><a href="#RecurrentConvBlock-385"><span class="linenos">385</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  
</span><span id="RecurrentConvBlock-386"><a href="#RecurrentConvBlock-386"><span class="linenos">386</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="RecurrentConvBlock-387"><a href="#RecurrentConvBlock-387"><span class="linenos">387</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span>
</span><span id="RecurrentConvBlock-388"><a href="#RecurrentConvBlock-388"><span class="linenos">388</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convlstm1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-389"><a href="#RecurrentConvBlock-389"><span class="linenos">389</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="RecurrentConvBlock-390"><a href="#RecurrentConvBlock-390"><span class="linenos">390</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-391"><a href="#RecurrentConvBlock-391"><span class="linenos">391</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-392"><a href="#RecurrentConvBlock-392"><span class="linenos">392</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="RecurrentConvBlock-393"><a href="#RecurrentConvBlock-393"><span class="linenos">393</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-394"><a href="#RecurrentConvBlock-394"><span class="linenos">394</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convlstm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-395"><a href="#RecurrentConvBlock-395"><span class="linenos">395</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="RecurrentConvBlock-396"><a href="#RecurrentConvBlock-396"><span class="linenos">396</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-397"><a href="#RecurrentConvBlock-397"><span class="linenos">397</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock-398"><a href="#RecurrentConvBlock-398"><span class="linenos">398</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Recurrent convolutional block.</p>
</div>


                            <div id="RecurrentConvBlock.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#RecurrentConvBlock.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">RecurrentConvBlock</span><span class="signature">(
    filters,
    ks_cl1=(5, 5),
    ks_cl2=(3, 3),
    activation=&#39;relu&#39;,
    normalization=None,
    dropout_rate=0,
    dropout_variant=None,
    name_suffix=&#39;&#39;,
    **conv_kwargs
)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RecurrentConvBlock.__init__-343"><a href="#RecurrentConvBlock.__init__-343"><span class="linenos">343</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">ks_cl1</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">ks_cl2</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> 
</span><span id="RecurrentConvBlock.__init__-344"><a href="#RecurrentConvBlock.__init__-344"><span class="linenos">344</span></a>                 <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="RecurrentConvBlock.__init__-345"><a href="#RecurrentConvBlock.__init__-345"><span class="linenos">345</span></a>                 <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">):</span>
</span><span id="RecurrentConvBlock.__init__-346"><a href="#RecurrentConvBlock.__init__-346"><span class="linenos">346</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;RecurrentConvBlock&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.__init__-347"><a href="#RecurrentConvBlock.__init__-347"><span class="linenos">347</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
</span><span id="RecurrentConvBlock.__init__-348"><a href="#RecurrentConvBlock.__init__-348"><span class="linenos">348</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>
</span><span id="RecurrentConvBlock.__init__-349"><a href="#RecurrentConvBlock.__init__-349"><span class="linenos">349</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span> <span class="o">=</span> <span class="n">dropout_variant</span>
</span><span id="RecurrentConvBlock.__init__-350"><a href="#RecurrentConvBlock.__init__-350"><span class="linenos">350</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">convlstm1</span> <span class="o">=</span> <span class="n">ConvLSTM2D</span><span class="p">(</span>
</span><span id="RecurrentConvBlock.__init__-351"><a href="#RecurrentConvBlock.__init__-351"><span class="linenos">351</span></a>            <span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl1</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="RecurrentConvBlock.__init__-352"><a href="#RecurrentConvBlock.__init__-352"><span class="linenos">352</span></a>            <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.__init__-353"><a href="#RecurrentConvBlock.__init__-353"><span class="linenos">353</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">convlstm2</span> <span class="o">=</span> <span class="n">ConvLSTM2D</span><span class="p">(</span>
</span><span id="RecurrentConvBlock.__init__-354"><a href="#RecurrentConvBlock.__init__-354"><span class="linenos">354</span></a>            <span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks_cl2</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="RecurrentConvBlock.__init__-355"><a href="#RecurrentConvBlock.__init__-355"><span class="linenos">355</span></a>            <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.__init__-356"><a href="#RecurrentConvBlock.__init__-356"><span class="linenos">356</span></a>
</span><span id="RecurrentConvBlock.__init__-357"><a href="#RecurrentConvBlock.__init__-357"><span class="linenos">357</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="RecurrentConvBlock.__init__-358"><a href="#RecurrentConvBlock.__init__-358"><span class="linenos">358</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bn&#39;</span><span class="p">,</span> <span class="s1">&#39;ln&#39;</span><span class="p">]:</span>
</span><span id="RecurrentConvBlock.__init__-359"><a href="#RecurrentConvBlock.__init__-359"><span class="linenos">359</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Normalization not supported, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.__init__-360"><a href="#RecurrentConvBlock.__init__-360"><span class="linenos">360</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;bn&#39;</span><span class="p">:</span>
</span><span id="RecurrentConvBlock.__init__-361"><a href="#RecurrentConvBlock.__init__-361"><span class="linenos">361</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="RecurrentConvBlock.__init__-362"><a href="#RecurrentConvBlock.__init__-362"><span class="linenos">362</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()</span>
</span><span id="RecurrentConvBlock.__init__-363"><a href="#RecurrentConvBlock.__init__-363"><span class="linenos">363</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s1">&#39;ln&#39;</span><span class="p">:</span>
</span><span id="RecurrentConvBlock.__init__-364"><a href="#RecurrentConvBlock.__init__-364"><span class="linenos">364</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">()</span>
</span><span id="RecurrentConvBlock.__init__-365"><a href="#RecurrentConvBlock.__init__-365"><span class="linenos">365</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">()</span>
</span><span id="RecurrentConvBlock.__init__-366"><a href="#RecurrentConvBlock.__init__-366"><span class="linenos">366</span></a>
</span><span id="RecurrentConvBlock.__init__-367"><a href="#RecurrentConvBlock.__init__-367"><span class="linenos">367</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.__init__-368"><a href="#RecurrentConvBlock.__init__-368"><span class="linenos">368</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">skipconnection</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()</span>
</span><span id="RecurrentConvBlock.__init__-369"><a href="#RecurrentConvBlock.__init__-369"><span class="linenos">369</span></a>
</span><span id="RecurrentConvBlock.__init__-370"><a href="#RecurrentConvBlock.__init__-370"><span class="linenos">370</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="RecurrentConvBlock.__init__-371"><a href="#RecurrentConvBlock.__init__-371"><span class="linenos">371</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="RecurrentConvBlock.__init__-372"><a href="#RecurrentConvBlock.__init__-372"><span class="linenos">372</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">get_dropout_layer</span><span class="p">(</span>
</span><span id="RecurrentConvBlock.__init__-373"><a href="#RecurrentConvBlock.__init__-373"><span class="linenos">373</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.__init__-374"><a href="#RecurrentConvBlock.__init__-374"><span class="linenos">374</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">get_dropout_layer</span><span class="p">(</span>
</span><span id="RecurrentConvBlock.__init__-375"><a href="#RecurrentConvBlock.__init__-375"><span class="linenos">375</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>            
</span><span id="RecurrentConvBlock.__init__-376"><a href="#RecurrentConvBlock.__init__-376"><span class="linenos">376</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="RecurrentConvBlock.__init__-377"><a href="#RecurrentConvBlock.__init__-377"><span class="linenos">377</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="RecurrentConvBlock.__init__-378"><a href="#RecurrentConvBlock.__init__-378"><span class="linenos">378</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span> <span class="o">=</span> <span class="kc">False</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="RecurrentConvBlock.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#RecurrentConvBlock.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, X)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RecurrentConvBlock.call-380"><a href="#RecurrentConvBlock.call-380"><span class="linenos">380</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="RecurrentConvBlock.call-381"><a href="#RecurrentConvBlock.call-381"><span class="linenos">381</span></a>        <span class="sd">&quot;&quot;&quot;</span>
</span><span id="RecurrentConvBlock.call-382"><a href="#RecurrentConvBlock.call-382"><span class="linenos">382</span></a><span class="sd">        Forward pass. </span>
</span><span id="RecurrentConvBlock.call-383"><a href="#RecurrentConvBlock.call-383"><span class="linenos">383</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="RecurrentConvBlock.call-384"><a href="#RecurrentConvBlock.call-384"><span class="linenos">384</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="RecurrentConvBlock.call-385"><a href="#RecurrentConvBlock.call-385"><span class="linenos">385</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  
</span><span id="RecurrentConvBlock.call-386"><a href="#RecurrentConvBlock.call-386"><span class="linenos">386</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="RecurrentConvBlock.call-387"><a href="#RecurrentConvBlock.call-387"><span class="linenos">387</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span>
</span><span id="RecurrentConvBlock.call-388"><a href="#RecurrentConvBlock.call-388"><span class="linenos">388</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convlstm1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.call-389"><a href="#RecurrentConvBlock.call-389"><span class="linenos">389</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="RecurrentConvBlock.call-390"><a href="#RecurrentConvBlock.call-390"><span class="linenos">390</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.call-391"><a href="#RecurrentConvBlock.call-391"><span class="linenos">391</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.call-392"><a href="#RecurrentConvBlock.call-392"><span class="linenos">392</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">:</span>
</span><span id="RecurrentConvBlock.call-393"><a href="#RecurrentConvBlock.call-393"><span class="linenos">393</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.call-394"><a href="#RecurrentConvBlock.call-394"><span class="linenos">394</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convlstm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.call-395"><a href="#RecurrentConvBlock.call-395"><span class="linenos">395</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="RecurrentConvBlock.call-396"><a href="#RecurrentConvBlock.call-396"><span class="linenos">396</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.call-397"><a href="#RecurrentConvBlock.call-397"><span class="linenos">397</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="RecurrentConvBlock.call-398"><a href="#RecurrentConvBlock.call-398"><span class="linenos">398</span></a>        <span class="k">return</span> <span class="n">Y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Forward pass.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="RecurrentConvBlock.build" class="function">build</dd>
                <dd id="RecurrentConvBlock.add_weight" class="function">add_weight</dd>
                <dd id="RecurrentConvBlock.get_config" class="function">get_config</dd>
                <dd id="RecurrentConvBlock.from_config" class="function">from_config</dd>
                <dd id="RecurrentConvBlock.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="RecurrentConvBlock.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="RecurrentConvBlock.compute_mask" class="function">compute_mask</dd>
                <dd id="RecurrentConvBlock.dtype" class="variable">dtype</dd>
                <dd id="RecurrentConvBlock.name" class="variable">name</dd>
                <dd id="RecurrentConvBlock.supports_masking" class="variable">supports_masking</dd>
                <dd id="RecurrentConvBlock.dynamic" class="variable">dynamic</dd>
                <dd id="RecurrentConvBlock.stateful" class="variable">stateful</dd>
                <dd id="RecurrentConvBlock.trainable" class="variable">trainable</dd>
                <dd id="RecurrentConvBlock.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="RecurrentConvBlock.input_spec" class="variable">input_spec</dd>
                <dd id="RecurrentConvBlock.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="RecurrentConvBlock.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="RecurrentConvBlock.weights" class="variable">weights</dd>
                <dd id="RecurrentConvBlock.updates" class="variable">updates</dd>
                <dd id="RecurrentConvBlock.losses" class="variable">losses</dd>
                <dd id="RecurrentConvBlock.add_loss" class="function">add_loss</dd>
                <dd id="RecurrentConvBlock.metrics" class="variable">metrics</dd>
                <dd id="RecurrentConvBlock.add_metric" class="function">add_metric</dd>
                <dd id="RecurrentConvBlock.add_update" class="function">add_update</dd>
                <dd id="RecurrentConvBlock.set_weights" class="function">set_weights</dd>
                <dd id="RecurrentConvBlock.get_weights" class="function">get_weights</dd>
                <dd id="RecurrentConvBlock.finalize_state" class="function">finalize_state</dd>
                <dd id="RecurrentConvBlock.get_updates_for" class="function">get_updates_for</dd>
                <dd id="RecurrentConvBlock.get_losses_for" class="function">get_losses_for</dd>
                <dd id="RecurrentConvBlock.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="RecurrentConvBlock.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="RecurrentConvBlock.input_mask" class="variable">input_mask</dd>
                <dd id="RecurrentConvBlock.output_mask" class="variable">output_mask</dd>
                <dd id="RecurrentConvBlock.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="RecurrentConvBlock.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="RecurrentConvBlock.get_input_at" class="function">get_input_at</dd>
                <dd id="RecurrentConvBlock.get_output_at" class="function">get_output_at</dd>
                <dd id="RecurrentConvBlock.input" class="variable">input</dd>
                <dd id="RecurrentConvBlock.output" class="variable">output</dd>
                <dd id="RecurrentConvBlock.input_shape" class="variable">input_shape</dd>
                <dd id="RecurrentConvBlock.count_params" class="function">count_params</dd>
                <dd id="RecurrentConvBlock.output_shape" class="variable">output_shape</dd>
                <dd id="RecurrentConvBlock.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="RecurrentConvBlock.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="RecurrentConvBlock.apply" class="function">apply</dd>
                <dd id="RecurrentConvBlock.add_variable" class="function">add_variable</dd>
                <dd id="RecurrentConvBlock.variables" class="variable">variables</dd>
                <dd id="RecurrentConvBlock.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="RecurrentConvBlock.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="RecurrentConvBlock.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="RecurrentConvBlock.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="RecurrentConvBlock.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="RecurrentConvBlock.name_scope" class="variable">name_scope</dd>
                <dd id="RecurrentConvBlock.submodules" class="variable">submodules</dd>
                <dd id="RecurrentConvBlock.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="SubpixelConvolutionBlock">
                                <div class="attr class">
        <a class="headerlink" href="#SubpixelConvolutionBlock">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">SubpixelConvolutionBlock</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SubpixelConvolutionBlock-401"><a href="#SubpixelConvolutionBlock-401"><span class="linenos">401</span></a><span class="k">class</span> <span class="nc">SubpixelConvolutionBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="SubpixelConvolutionBlock-402"><a href="#SubpixelConvolutionBlock-402"><span class="linenos">402</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="SubpixelConvolutionBlock-403"><a href="#SubpixelConvolutionBlock-403"><span class="linenos">403</span></a><span class="sd">    Subpixel convolution (pixel shuffle) block.</span>
</span><span id="SubpixelConvolutionBlock-404"><a href="#SubpixelConvolutionBlock-404"><span class="linenos">404</span></a>
</span><span id="SubpixelConvolutionBlock-405"><a href="#SubpixelConvolutionBlock-405"><span class="linenos">405</span></a><span class="sd">    References</span>
</span><span id="SubpixelConvolutionBlock-406"><a href="#SubpixelConvolutionBlock-406"><span class="linenos">406</span></a><span class="sd">    ----------</span>
</span><span id="SubpixelConvolutionBlock-407"><a href="#SubpixelConvolutionBlock-407"><span class="linenos">407</span></a><span class="sd">    [1] Real-Time Single Image and Video Super-Resolution Using an Efficient </span>
</span><span id="SubpixelConvolutionBlock-408"><a href="#SubpixelConvolutionBlock-408"><span class="linenos">408</span></a><span class="sd">    Sub-Pixel Convolutional Neural Network: https://arxiv.org/abs/1609.05158</span>
</span><span id="SubpixelConvolutionBlock-409"><a href="#SubpixelConvolutionBlock-409"><span class="linenos">409</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="SubpixelConvolutionBlock-410"><a href="#SubpixelConvolutionBlock-410"><span class="linenos">410</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock-411"><a href="#SubpixelConvolutionBlock-411"><span class="linenos">411</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;SubpixelConvolution&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-412"><a href="#SubpixelConvolutionBlock-412"><span class="linenos">412</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
</span><span id="SubpixelConvolutionBlock-413"><a href="#SubpixelConvolutionBlock-413"><span class="linenos">413</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">=</span> <span class="n">n_filters</span>
</span><span id="SubpixelConvolutionBlock-414"><a href="#SubpixelConvolutionBlock-414"><span class="linenos">414</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-415"><a href="#SubpixelConvolutionBlock-415"><span class="linenos">415</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-416"><a href="#SubpixelConvolutionBlock-416"><span class="linenos">416</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">*</span> <span class="p">(</span><span class="mi">5</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-417"><a href="#SubpixelConvolutionBlock-417"><span class="linenos">417</span></a>
</span><span id="SubpixelConvolutionBlock-418"><a href="#SubpixelConvolutionBlock-418"><span class="linenos">418</span></a>    <span class="k">def</span> <span class="nf">upsample_conv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">factor</span><span class="p">):</span>
</span><span id="SubpixelConvolutionBlock-419"><a href="#SubpixelConvolutionBlock-419"><span class="linenos">419</span></a>        <span class="sd">&quot;&quot;&quot;Sub-pixel convolution (pixel shuffle)</span>
</span><span id="SubpixelConvolutionBlock-420"><a href="#SubpixelConvolutionBlock-420"><span class="linenos">420</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SubpixelConvolutionBlock-421"><a href="#SubpixelConvolutionBlock-421"><span class="linenos">421</span></a>        <span class="k">if</span> <span class="n">factor</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock-422"><a href="#SubpixelConvolutionBlock-422"><span class="linenos">422</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-423"><a href="#SubpixelConvolutionBlock-423"><span class="linenos">423</span></a>        <span class="k">elif</span> <span class="n">factor</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock-424"><a href="#SubpixelConvolutionBlock-424"><span class="linenos">424</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-425"><a href="#SubpixelConvolutionBlock-425"><span class="linenos">425</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock-426"><a href="#SubpixelConvolutionBlock-426"><span class="linenos">426</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-427"><a href="#SubpixelConvolutionBlock-427"><span class="linenos">427</span></a>        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">depth_to_space</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">factor</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-428"><a href="#SubpixelConvolutionBlock-428"><span class="linenos">428</span></a>
</span><span id="SubpixelConvolutionBlock-429"><a href="#SubpixelConvolutionBlock-429"><span class="linenos">429</span></a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="SubpixelConvolutionBlock-430"><a href="#SubpixelConvolutionBlock-430"><span class="linenos">430</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> 
</span><span id="SubpixelConvolutionBlock-431"><a href="#SubpixelConvolutionBlock-431"><span class="linenos">431</span></a>                <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</span><span id="SubpixelConvolutionBlock-432"><a href="#SubpixelConvolutionBlock-432"><span class="linenos">432</span></a>
</span><span id="SubpixelConvolutionBlock-433"><a href="#SubpixelConvolutionBlock-433"><span class="linenos">433</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="SubpixelConvolutionBlock-434"><a href="#SubpixelConvolutionBlock-434"><span class="linenos">434</span></a>        <span class="sd">&quot;&quot;&quot;  Forward pass.</span>
</span><span id="SubpixelConvolutionBlock-435"><a href="#SubpixelConvolutionBlock-435"><span class="linenos">435</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SubpixelConvolutionBlock-436"><a href="#SubpixelConvolutionBlock-436"><span class="linenos">436</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock-437"><a href="#SubpixelConvolutionBlock-437"><span class="linenos">437</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-438"><a href="#SubpixelConvolutionBlock-438"><span class="linenos">438</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock-439"><a href="#SubpixelConvolutionBlock-439"><span class="linenos">439</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-440"><a href="#SubpixelConvolutionBlock-440"><span class="linenos">440</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-441"><a href="#SubpixelConvolutionBlock-441"><span class="linenos">441</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock-442"><a href="#SubpixelConvolutionBlock-442"><span class="linenos">442</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-443"><a href="#SubpixelConvolutionBlock-443"><span class="linenos">443</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-444"><a href="#SubpixelConvolutionBlock-444"><span class="linenos">444</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-445"><a href="#SubpixelConvolutionBlock-445"><span class="linenos">445</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock-446"><a href="#SubpixelConvolutionBlock-446"><span class="linenos">446</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-447"><a href="#SubpixelConvolutionBlock-447"><span class="linenos">447</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-448"><a href="#SubpixelConvolutionBlock-448"><span class="linenos">448</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">20</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock-449"><a href="#SubpixelConvolutionBlock-449"><span class="linenos">449</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-450"><a href="#SubpixelConvolutionBlock-450"><span class="linenos">450</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-451"><a href="#SubpixelConvolutionBlock-451"><span class="linenos">451</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-452"><a href="#SubpixelConvolutionBlock-452"><span class="linenos">452</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock-453"><a href="#SubpixelConvolutionBlock-453"><span class="linenos">453</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock-454"><a href="#SubpixelConvolutionBlock-454"><span class="linenos">454</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Subpixel convolution (pixel shuffle) block.</p>

<h6 id="references">References</h6>

<p>[1] Real-Time Single Image and Video Super-Resolution Using an Efficient 
Sub-Pixel Convolutional Neural Network: https://arxiv.org/abs/1609.05158</p>
</div>


                            <div id="SubpixelConvolutionBlock.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#SubpixelConvolutionBlock.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">SubpixelConvolutionBlock</span><span class="signature">(scale, n_filters, name_suffix=&#39;&#39;, **kwargs)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SubpixelConvolutionBlock.__init__-410"><a href="#SubpixelConvolutionBlock.__init__-410"><span class="linenos">410</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock.__init__-411"><a href="#SubpixelConvolutionBlock.__init__-411"><span class="linenos">411</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;SubpixelConvolution&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.__init__-412"><a href="#SubpixelConvolutionBlock.__init__-412"><span class="linenos">412</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
</span><span id="SubpixelConvolutionBlock.__init__-413"><a href="#SubpixelConvolutionBlock.__init__-413"><span class="linenos">413</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">=</span> <span class="n">n_filters</span>
</span><span id="SubpixelConvolutionBlock.__init__-414"><a href="#SubpixelConvolutionBlock.__init__-414"><span class="linenos">414</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.__init__-415"><a href="#SubpixelConvolutionBlock.__init__-415"><span class="linenos">415</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.__init__-416"><a href="#SubpixelConvolutionBlock.__init__-416"><span class="linenos">416</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">*</span> <span class="p">(</span><span class="mi">5</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="SubpixelConvolutionBlock.upsample_conv" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#SubpixelConvolutionBlock.upsample_conv">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">upsample_conv</span><span class="signature">(self, x, factor)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SubpixelConvolutionBlock.upsample_conv-418"><a href="#SubpixelConvolutionBlock.upsample_conv-418"><span class="linenos">418</span></a>    <span class="k">def</span> <span class="nf">upsample_conv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">factor</span><span class="p">):</span>
</span><span id="SubpixelConvolutionBlock.upsample_conv-419"><a href="#SubpixelConvolutionBlock.upsample_conv-419"><span class="linenos">419</span></a>        <span class="sd">&quot;&quot;&quot;Sub-pixel convolution (pixel shuffle)</span>
</span><span id="SubpixelConvolutionBlock.upsample_conv-420"><a href="#SubpixelConvolutionBlock.upsample_conv-420"><span class="linenos">420</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SubpixelConvolutionBlock.upsample_conv-421"><a href="#SubpixelConvolutionBlock.upsample_conv-421"><span class="linenos">421</span></a>        <span class="k">if</span> <span class="n">factor</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock.upsample_conv-422"><a href="#SubpixelConvolutionBlock.upsample_conv-422"><span class="linenos">422</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.upsample_conv-423"><a href="#SubpixelConvolutionBlock.upsample_conv-423"><span class="linenos">423</span></a>        <span class="k">elif</span> <span class="n">factor</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock.upsample_conv-424"><a href="#SubpixelConvolutionBlock.upsample_conv-424"><span class="linenos">424</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.upsample_conv-425"><a href="#SubpixelConvolutionBlock.upsample_conv-425"><span class="linenos">425</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock.upsample_conv-426"><a href="#SubpixelConvolutionBlock.upsample_conv-426"><span class="linenos">426</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.upsample_conv-427"><a href="#SubpixelConvolutionBlock.upsample_conv-427"><span class="linenos">427</span></a>        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">depth_to_space</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">factor</span><span class="p">)</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Sub-pixel convolution (pixel shuffle)</p>
</div>


                            </div>
                            <div id="SubpixelConvolutionBlock.compute_output_shape" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#SubpixelConvolutionBlock.compute_output_shape">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">compute_output_shape</span><span class="signature">(self, input_shape)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SubpixelConvolutionBlock.compute_output_shape-429"><a href="#SubpixelConvolutionBlock.compute_output_shape-429"><span class="linenos">429</span></a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="SubpixelConvolutionBlock.compute_output_shape-430"><a href="#SubpixelConvolutionBlock.compute_output_shape-430"><span class="linenos">430</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> 
</span><span id="SubpixelConvolutionBlock.compute_output_shape-431"><a href="#SubpixelConvolutionBlock.compute_output_shape-431"><span class="linenos">431</span></a>                <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Computes the output shape of the layer.</p>

<p>This method will cause the layer's state to be built, if that has not
happened before. This requires that the layer will later be used with
inputs that match the input shape provided here.</p>

<p>Args:
    input_shape: Shape tuple (tuple of integers)
        or list of shape tuples (one per output tensor of the layer).
        Shape tuples can include None for free dimensions,
        instead of an integer.</p>

<p>Returns:
    An input shape tuple.</p>
</div>


                            </div>
                            <div id="SubpixelConvolutionBlock.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#SubpixelConvolutionBlock.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, x)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SubpixelConvolutionBlock.call-433"><a href="#SubpixelConvolutionBlock.call-433"><span class="linenos">433</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="SubpixelConvolutionBlock.call-434"><a href="#SubpixelConvolutionBlock.call-434"><span class="linenos">434</span></a>        <span class="sd">&quot;&quot;&quot;  Forward pass.</span>
</span><span id="SubpixelConvolutionBlock.call-435"><a href="#SubpixelConvolutionBlock.call-435"><span class="linenos">435</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SubpixelConvolutionBlock.call-436"><a href="#SubpixelConvolutionBlock.call-436"><span class="linenos">436</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock.call-437"><a href="#SubpixelConvolutionBlock.call-437"><span class="linenos">437</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.call-438"><a href="#SubpixelConvolutionBlock.call-438"><span class="linenos">438</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock.call-439"><a href="#SubpixelConvolutionBlock.call-439"><span class="linenos">439</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.call-440"><a href="#SubpixelConvolutionBlock.call-440"><span class="linenos">440</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.call-441"><a href="#SubpixelConvolutionBlock.call-441"><span class="linenos">441</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock.call-442"><a href="#SubpixelConvolutionBlock.call-442"><span class="linenos">442</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.call-443"><a href="#SubpixelConvolutionBlock.call-443"><span class="linenos">443</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.call-444"><a href="#SubpixelConvolutionBlock.call-444"><span class="linenos">444</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.call-445"><a href="#SubpixelConvolutionBlock.call-445"><span class="linenos">445</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock.call-446"><a href="#SubpixelConvolutionBlock.call-446"><span class="linenos">446</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.call-447"><a href="#SubpixelConvolutionBlock.call-447"><span class="linenos">447</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.call-448"><a href="#SubpixelConvolutionBlock.call-448"><span class="linenos">448</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">20</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock.call-449"><a href="#SubpixelConvolutionBlock.call-449"><span class="linenos">449</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.call-450"><a href="#SubpixelConvolutionBlock.call-450"><span class="linenos">450</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.call-451"><a href="#SubpixelConvolutionBlock.call-451"><span class="linenos">451</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.call-452"><a href="#SubpixelConvolutionBlock.call-452"><span class="linenos">452</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="SubpixelConvolutionBlock.call-453"><a href="#SubpixelConvolutionBlock.call-453"><span class="linenos">453</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span><span id="SubpixelConvolutionBlock.call-454"><a href="#SubpixelConvolutionBlock.call-454"><span class="linenos">454</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Forward pass.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="SubpixelConvolutionBlock.build" class="function">build</dd>
                <dd id="SubpixelConvolutionBlock.add_weight" class="function">add_weight</dd>
                <dd id="SubpixelConvolutionBlock.get_config" class="function">get_config</dd>
                <dd id="SubpixelConvolutionBlock.from_config" class="function">from_config</dd>
                <dd id="SubpixelConvolutionBlock.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="SubpixelConvolutionBlock.compute_mask" class="function">compute_mask</dd>
                <dd id="SubpixelConvolutionBlock.dtype" class="variable">dtype</dd>
                <dd id="SubpixelConvolutionBlock.name" class="variable">name</dd>
                <dd id="SubpixelConvolutionBlock.supports_masking" class="variable">supports_masking</dd>
                <dd id="SubpixelConvolutionBlock.dynamic" class="variable">dynamic</dd>
                <dd id="SubpixelConvolutionBlock.stateful" class="variable">stateful</dd>
                <dd id="SubpixelConvolutionBlock.trainable" class="variable">trainable</dd>
                <dd id="SubpixelConvolutionBlock.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="SubpixelConvolutionBlock.input_spec" class="variable">input_spec</dd>
                <dd id="SubpixelConvolutionBlock.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="SubpixelConvolutionBlock.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="SubpixelConvolutionBlock.weights" class="variable">weights</dd>
                <dd id="SubpixelConvolutionBlock.updates" class="variable">updates</dd>
                <dd id="SubpixelConvolutionBlock.losses" class="variable">losses</dd>
                <dd id="SubpixelConvolutionBlock.add_loss" class="function">add_loss</dd>
                <dd id="SubpixelConvolutionBlock.metrics" class="variable">metrics</dd>
                <dd id="SubpixelConvolutionBlock.add_metric" class="function">add_metric</dd>
                <dd id="SubpixelConvolutionBlock.add_update" class="function">add_update</dd>
                <dd id="SubpixelConvolutionBlock.set_weights" class="function">set_weights</dd>
                <dd id="SubpixelConvolutionBlock.get_weights" class="function">get_weights</dd>
                <dd id="SubpixelConvolutionBlock.finalize_state" class="function">finalize_state</dd>
                <dd id="SubpixelConvolutionBlock.get_updates_for" class="function">get_updates_for</dd>
                <dd id="SubpixelConvolutionBlock.get_losses_for" class="function">get_losses_for</dd>
                <dd id="SubpixelConvolutionBlock.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="SubpixelConvolutionBlock.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="SubpixelConvolutionBlock.input_mask" class="variable">input_mask</dd>
                <dd id="SubpixelConvolutionBlock.output_mask" class="variable">output_mask</dd>
                <dd id="SubpixelConvolutionBlock.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="SubpixelConvolutionBlock.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="SubpixelConvolutionBlock.get_input_at" class="function">get_input_at</dd>
                <dd id="SubpixelConvolutionBlock.get_output_at" class="function">get_output_at</dd>
                <dd id="SubpixelConvolutionBlock.input" class="variable">input</dd>
                <dd id="SubpixelConvolutionBlock.output" class="variable">output</dd>
                <dd id="SubpixelConvolutionBlock.input_shape" class="variable">input_shape</dd>
                <dd id="SubpixelConvolutionBlock.count_params" class="function">count_params</dd>
                <dd id="SubpixelConvolutionBlock.output_shape" class="variable">output_shape</dd>
                <dd id="SubpixelConvolutionBlock.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="SubpixelConvolutionBlock.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="SubpixelConvolutionBlock.apply" class="function">apply</dd>
                <dd id="SubpixelConvolutionBlock.add_variable" class="function">add_variable</dd>
                <dd id="SubpixelConvolutionBlock.variables" class="variable">variables</dd>
                <dd id="SubpixelConvolutionBlock.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="SubpixelConvolutionBlock.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="SubpixelConvolutionBlock.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="SubpixelConvolutionBlock.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="SubpixelConvolutionBlock.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="SubpixelConvolutionBlock.name_scope" class="variable">name_scope</dd>
                <dd id="SubpixelConvolutionBlock.submodules" class="variable">submodules</dd>
                <dd id="SubpixelConvolutionBlock.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="ResizeConvolutionBlock">
                                <div class="attr class">
        <a class="headerlink" href="#ResizeConvolutionBlock">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">ResizeConvolutionBlock</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResizeConvolutionBlock-457"><a href="#ResizeConvolutionBlock-457"><span class="linenos">457</span></a><span class="k">class</span> <span class="nc">ResizeConvolutionBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="ResizeConvolutionBlock-458"><a href="#ResizeConvolutionBlock-458"><span class="linenos">458</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="ResizeConvolutionBlock-459"><a href="#ResizeConvolutionBlock-459"><span class="linenos">459</span></a><span class="sd">    Upsampling via bilinear interpolation followed by a 2D Convolution. </span>
</span><span id="ResizeConvolutionBlock-460"><a href="#ResizeConvolutionBlock-460"><span class="linenos">460</span></a>
</span><span id="ResizeConvolutionBlock-461"><a href="#ResizeConvolutionBlock-461"><span class="linenos">461</span></a><span class="sd">    Parameters</span>
</span><span id="ResizeConvolutionBlock-462"><a href="#ResizeConvolutionBlock-462"><span class="linenos">462</span></a><span class="sd">    ----------</span>
</span><span id="ResizeConvolutionBlock-463"><a href="#ResizeConvolutionBlock-463"><span class="linenos">463</span></a><span class="sd">    interpolation : str</span>
</span><span id="ResizeConvolutionBlock-464"><a href="#ResizeConvolutionBlock-464"><span class="linenos">464</span></a><span class="sd">        The interpolation method. Defaults to &quot;bilinear&quot;. Supports &quot;bilinear&quot;, </span>
</span><span id="ResizeConvolutionBlock-465"><a href="#ResizeConvolutionBlock-465"><span class="linenos">465</span></a><span class="sd">        &quot;nearest&quot;, &quot;bicubic&quot;, &quot;area&quot;, &quot;lanczos3&quot;, &quot;lanczos5&quot;, &quot;gaussian&quot;, </span>
</span><span id="ResizeConvolutionBlock-466"><a href="#ResizeConvolutionBlock-466"><span class="linenos">466</span></a><span class="sd">        &quot;mitchellcubic&quot;.</span>
</span><span id="ResizeConvolutionBlock-467"><a href="#ResizeConvolutionBlock-467"><span class="linenos">467</span></a>
</span><span id="ResizeConvolutionBlock-468"><a href="#ResizeConvolutionBlock-468"><span class="linenos">468</span></a><span class="sd">    References</span>
</span><span id="ResizeConvolutionBlock-469"><a href="#ResizeConvolutionBlock-469"><span class="linenos">469</span></a><span class="sd">    ----------</span>
</span><span id="ResizeConvolutionBlock-470"><a href="#ResizeConvolutionBlock-470"><span class="linenos">470</span></a><span class="sd">    [1] Deconvolution and Checkerboard Artifacts: </span>
</span><span id="ResizeConvolutionBlock-471"><a href="#ResizeConvolutionBlock-471"><span class="linenos">471</span></a><span class="sd">    https://distill.pub/2016/deconv-checkerboard/</span>
</span><span id="ResizeConvolutionBlock-472"><a href="#ResizeConvolutionBlock-472"><span class="linenos">472</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ResizeConvolutionBlock-473"><a href="#ResizeConvolutionBlock-473"><span class="linenos">473</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> 
</span><span id="ResizeConvolutionBlock-474"><a href="#ResizeConvolutionBlock-474"><span class="linenos">474</span></a>                 <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResizeConvolutionBlock-475"><a href="#ResizeConvolutionBlock-475"><span class="linenos">475</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;ResizeConvolution&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="ResizeConvolutionBlock-476"><a href="#ResizeConvolutionBlock-476"><span class="linenos">476</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
</span><span id="ResizeConvolutionBlock-477"><a href="#ResizeConvolutionBlock-477"><span class="linenos">477</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">=</span> <span class="n">n_filters</span>
</span><span id="ResizeConvolutionBlock-478"><a href="#ResizeConvolutionBlock-478"><span class="linenos">478</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span> <span class="o">=</span> <span class="n">interpolation</span>
</span><span id="ResizeConvolutionBlock-479"><a href="#ResizeConvolutionBlock-479"><span class="linenos">479</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="ResizeConvolutionBlock-480"><a href="#ResizeConvolutionBlock-480"><span class="linenos">480</span></a>
</span><span id="ResizeConvolutionBlock-481"><a href="#ResizeConvolutionBlock-481"><span class="linenos">481</span></a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="ResizeConvolutionBlock-482"><a href="#ResizeConvolutionBlock-482"><span class="linenos">482</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> 
</span><span id="ResizeConvolutionBlock-483"><a href="#ResizeConvolutionBlock-483"><span class="linenos">483</span></a>                <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</span><span id="ResizeConvolutionBlock-484"><a href="#ResizeConvolutionBlock-484"><span class="linenos">484</span></a>
</span><span id="ResizeConvolutionBlock-485"><a href="#ResizeConvolutionBlock-485"><span class="linenos">485</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="ResizeConvolutionBlock-486"><a href="#ResizeConvolutionBlock-486"><span class="linenos">486</span></a>        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span><span id="ResizeConvolutionBlock-487"><a href="#ResizeConvolutionBlock-487"><span class="linenos">487</span></a>        <span class="n">height</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span><span id="ResizeConvolutionBlock-488"><a href="#ResizeConvolutionBlock-488"><span class="linenos">488</span></a>        <span class="n">width</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span><span id="ResizeConvolutionBlock-489"><a href="#ResizeConvolutionBlock-489"><span class="linenos">489</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">Resizing</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResizeConvolutionBlock-490"><a href="#ResizeConvolutionBlock-490"><span class="linenos">490</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="ResizeConvolutionBlock-491"><a href="#ResizeConvolutionBlock-491"><span class="linenos">491</span></a>        <span class="k">return</span> <span class="n">y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Upsampling via bilinear interpolation followed by a 2D Convolution. </p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>interpolation</strong> (str):
The interpolation method. Defaults to "bilinear". Supports "bilinear", 
"nearest", "bicubic", "area", "lanczos3", "lanczos5", "gaussian", 
"mitchellcubic".</li>
</ul>

<h6 id="references">References</h6>

<p>[1] Deconvolution and Checkerboard Artifacts: 
https://distill.pub/2016/deconv-checkerboard/</p>
</div>


                            <div id="ResizeConvolutionBlock.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ResizeConvolutionBlock.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">ResizeConvolutionBlock</span><span class="signature">(scale, n_filters, interpolation=&#39;bilinear&#39;, name_suffix=&#39;&#39;, **kwargs)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResizeConvolutionBlock.__init__-473"><a href="#ResizeConvolutionBlock.__init__-473"><span class="linenos">473</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> 
</span><span id="ResizeConvolutionBlock.__init__-474"><a href="#ResizeConvolutionBlock.__init__-474"><span class="linenos">474</span></a>                 <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResizeConvolutionBlock.__init__-475"><a href="#ResizeConvolutionBlock.__init__-475"><span class="linenos">475</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;ResizeConvolution&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="ResizeConvolutionBlock.__init__-476"><a href="#ResizeConvolutionBlock.__init__-476"><span class="linenos">476</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
</span><span id="ResizeConvolutionBlock.__init__-477"><a href="#ResizeConvolutionBlock.__init__-477"><span class="linenos">477</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">=</span> <span class="n">n_filters</span>
</span><span id="ResizeConvolutionBlock.__init__-478"><a href="#ResizeConvolutionBlock.__init__-478"><span class="linenos">478</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span> <span class="o">=</span> <span class="n">interpolation</span>
</span><span id="ResizeConvolutionBlock.__init__-479"><a href="#ResizeConvolutionBlock.__init__-479"><span class="linenos">479</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="ResizeConvolutionBlock.compute_output_shape" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ResizeConvolutionBlock.compute_output_shape">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">compute_output_shape</span><span class="signature">(self, input_shape)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResizeConvolutionBlock.compute_output_shape-481"><a href="#ResizeConvolutionBlock.compute_output_shape-481"><span class="linenos">481</span></a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="ResizeConvolutionBlock.compute_output_shape-482"><a href="#ResizeConvolutionBlock.compute_output_shape-482"><span class="linenos">482</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> 
</span><span id="ResizeConvolutionBlock.compute_output_shape-483"><a href="#ResizeConvolutionBlock.compute_output_shape-483"><span class="linenos">483</span></a>                <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Computes the output shape of the layer.</p>

<p>This method will cause the layer's state to be built, if that has not
happened before. This requires that the layer will later be used with
inputs that match the input shape provided here.</p>

<p>Args:
    input_shape: Shape tuple (tuple of integers)
        or list of shape tuples (one per output tensor of the layer).
        Shape tuples can include None for free dimensions,
        instead of an integer.</p>

<p>Returns:
    An input shape tuple.</p>
</div>


                            </div>
                            <div id="ResizeConvolutionBlock.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ResizeConvolutionBlock.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, x)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResizeConvolutionBlock.call-485"><a href="#ResizeConvolutionBlock.call-485"><span class="linenos">485</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="ResizeConvolutionBlock.call-486"><a href="#ResizeConvolutionBlock.call-486"><span class="linenos">486</span></a>        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span><span id="ResizeConvolutionBlock.call-487"><a href="#ResizeConvolutionBlock.call-487"><span class="linenos">487</span></a>        <span class="n">height</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span><span id="ResizeConvolutionBlock.call-488"><a href="#ResizeConvolutionBlock.call-488"><span class="linenos">488</span></a>        <span class="n">width</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span><span id="ResizeConvolutionBlock.call-489"><a href="#ResizeConvolutionBlock.call-489"><span class="linenos">489</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">Resizing</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResizeConvolutionBlock.call-490"><a href="#ResizeConvolutionBlock.call-490"><span class="linenos">490</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="ResizeConvolutionBlock.call-491"><a href="#ResizeConvolutionBlock.call-491"><span class="linenos">491</span></a>        <span class="k">return</span> <span class="n">y</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>The <code><a href="#ResizeConvolutionBlock.call">call()</a></code> method may not create state (except in its first invocation,
wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).
It is recommended to create state in <code><a href="#ResizeConvolutionBlock.__init__">__init__()</a></code>, or the <code><a href="#ResizeConvolutionBlock.build">build()</a></code> method
that is called automatically before <code><a href="#ResizeConvolutionBlock.call">call()</a></code> executes the first time.</p>

<p>Args:
  inputs: Input tensor, or dict/list/tuple of input tensors.
    The first positional <code>inputs</code> argument is subject to special rules:
    - <code>inputs</code> must be explicitly passed. A layer cannot have zero
      arguments, and <code>inputs</code> cannot be provided via the default value
      of a keyword argument.
    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.
    - Keras mask metadata is only collected from <code>inputs</code>.
    - Layers are built (<code>build(input_shape)</code> method)
      using shape info from <code>inputs</code> only.
    - <code><a href="#ResizeConvolutionBlock.input_spec">input_spec</a></code> compatibility is only checked against <code>inputs</code>.
    - Mixed precision input casting is only applied to <code>inputs</code>.
      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their
      casting behavior in mixed precision should be handled manually.
    - The SavedModel input specification is generated using <code>inputs</code> only.
    - Integration with various ecosystem packages like TFMOT, TFLite,
      TF.js, etc is only supported for <code>inputs</code> and not for tensors in
      positional and keyword arguments.
  <em>args: Additional positional arguments. May contain tensors, although
    this is not recommended, for the reasons above.
  *</em>kwargs: Additional keyword arguments. May contain tensors, although
    this is not recommended, for the reasons above.
    The following optional keyword arguments are reserved:
    - <code>training</code>: Boolean scalar tensor of Python boolean indicating
      whether the <code><a href="#ResizeConvolutionBlock.call">call</a></code> is meant for training or inference.
    - <code>mask</code>: Boolean input mask. If the layer's <code><a href="#ResizeConvolutionBlock.call">call()</a></code> method takes a
      <code>mask</code> argument, its default value will be set to the mask generated
      for <code>inputs</code> by the previous layer (if <code><a href="#ResizeConvolutionBlock.input">input</a></code> did come from a layer
      that generated a corresponding mask, i.e. if it came from a Keras
      layer with masking support).</p>

<p>Returns:
  A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="ResizeConvolutionBlock.build" class="function">build</dd>
                <dd id="ResizeConvolutionBlock.add_weight" class="function">add_weight</dd>
                <dd id="ResizeConvolutionBlock.get_config" class="function">get_config</dd>
                <dd id="ResizeConvolutionBlock.from_config" class="function">from_config</dd>
                <dd id="ResizeConvolutionBlock.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="ResizeConvolutionBlock.compute_mask" class="function">compute_mask</dd>
                <dd id="ResizeConvolutionBlock.dtype" class="variable">dtype</dd>
                <dd id="ResizeConvolutionBlock.name" class="variable">name</dd>
                <dd id="ResizeConvolutionBlock.supports_masking" class="variable">supports_masking</dd>
                <dd id="ResizeConvolutionBlock.dynamic" class="variable">dynamic</dd>
                <dd id="ResizeConvolutionBlock.stateful" class="variable">stateful</dd>
                <dd id="ResizeConvolutionBlock.trainable" class="variable">trainable</dd>
                <dd id="ResizeConvolutionBlock.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="ResizeConvolutionBlock.input_spec" class="variable">input_spec</dd>
                <dd id="ResizeConvolutionBlock.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="ResizeConvolutionBlock.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="ResizeConvolutionBlock.weights" class="variable">weights</dd>
                <dd id="ResizeConvolutionBlock.updates" class="variable">updates</dd>
                <dd id="ResizeConvolutionBlock.losses" class="variable">losses</dd>
                <dd id="ResizeConvolutionBlock.add_loss" class="function">add_loss</dd>
                <dd id="ResizeConvolutionBlock.metrics" class="variable">metrics</dd>
                <dd id="ResizeConvolutionBlock.add_metric" class="function">add_metric</dd>
                <dd id="ResizeConvolutionBlock.add_update" class="function">add_update</dd>
                <dd id="ResizeConvolutionBlock.set_weights" class="function">set_weights</dd>
                <dd id="ResizeConvolutionBlock.get_weights" class="function">get_weights</dd>
                <dd id="ResizeConvolutionBlock.finalize_state" class="function">finalize_state</dd>
                <dd id="ResizeConvolutionBlock.get_updates_for" class="function">get_updates_for</dd>
                <dd id="ResizeConvolutionBlock.get_losses_for" class="function">get_losses_for</dd>
                <dd id="ResizeConvolutionBlock.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="ResizeConvolutionBlock.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="ResizeConvolutionBlock.input_mask" class="variable">input_mask</dd>
                <dd id="ResizeConvolutionBlock.output_mask" class="variable">output_mask</dd>
                <dd id="ResizeConvolutionBlock.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="ResizeConvolutionBlock.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="ResizeConvolutionBlock.get_input_at" class="function">get_input_at</dd>
                <dd id="ResizeConvolutionBlock.get_output_at" class="function">get_output_at</dd>
                <dd id="ResizeConvolutionBlock.input" class="variable">input</dd>
                <dd id="ResizeConvolutionBlock.output" class="variable">output</dd>
                <dd id="ResizeConvolutionBlock.input_shape" class="variable">input_shape</dd>
                <dd id="ResizeConvolutionBlock.count_params" class="function">count_params</dd>
                <dd id="ResizeConvolutionBlock.output_shape" class="variable">output_shape</dd>
                <dd id="ResizeConvolutionBlock.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="ResizeConvolutionBlock.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="ResizeConvolutionBlock.apply" class="function">apply</dd>
                <dd id="ResizeConvolutionBlock.add_variable" class="function">add_variable</dd>
                <dd id="ResizeConvolutionBlock.variables" class="variable">variables</dd>
                <dd id="ResizeConvolutionBlock.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="ResizeConvolutionBlock.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="ResizeConvolutionBlock.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="ResizeConvolutionBlock.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="ResizeConvolutionBlock.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="ResizeConvolutionBlock.name_scope" class="variable">name_scope</dd>
                <dd id="ResizeConvolutionBlock.submodules" class="variable">submodules</dd>
                <dd id="ResizeConvolutionBlock.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="DeconvolutionBlock">
                                <div class="attr class">
        <a class="headerlink" href="#DeconvolutionBlock">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">DeconvolutionBlock</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DeconvolutionBlock-494"><a href="#DeconvolutionBlock-494"><span class="linenos">494</span></a><span class="k">class</span> <span class="nc">DeconvolutionBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="DeconvolutionBlock-495"><a href="#DeconvolutionBlock-495"><span class="linenos">495</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="DeconvolutionBlock-496"><a href="#DeconvolutionBlock-496"><span class="linenos">496</span></a><span class="sd">    Deconvolution or transposed convolution block.</span>
</span><span id="DeconvolutionBlock-497"><a href="#DeconvolutionBlock-497"><span class="linenos">497</span></a>
</span><span id="DeconvolutionBlock-498"><a href="#DeconvolutionBlock-498"><span class="linenos">498</span></a><span class="sd">    References</span>
</span><span id="DeconvolutionBlock-499"><a href="#DeconvolutionBlock-499"><span class="linenos">499</span></a><span class="sd">    ----------</span>
</span><span id="DeconvolutionBlock-500"><a href="#DeconvolutionBlock-500"><span class="linenos">500</span></a><span class="sd">    [1] FSRCNN - Accelerating the Super-Resolution Convolutional Neural Network: </span>
</span><span id="DeconvolutionBlock-501"><a href="#DeconvolutionBlock-501"><span class="linenos">501</span></a><span class="sd">    https://arxiv.org/abs/1608.00367</span>
</span><span id="DeconvolutionBlock-502"><a href="#DeconvolutionBlock-502"><span class="linenos">502</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="DeconvolutionBlock-503"><a href="#DeconvolutionBlock-503"><span class="linenos">503</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">output_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="DeconvolutionBlock-504"><a href="#DeconvolutionBlock-504"><span class="linenos">504</span></a>                 <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="DeconvolutionBlock-505"><a href="#DeconvolutionBlock-505"><span class="linenos">505</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Deconvolution&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="DeconvolutionBlock-506"><a href="#DeconvolutionBlock-506"><span class="linenos">506</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
</span><span id="DeconvolutionBlock-507"><a href="#DeconvolutionBlock-507"><span class="linenos">507</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_activation</span> <span class="o">=</span> <span class="n">output_activation</span>
</span><span id="DeconvolutionBlock-508"><a href="#DeconvolutionBlock-508"><span class="linenos">508</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose1</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> 
</span><span id="DeconvolutionBlock-509"><a href="#DeconvolutionBlock-509"><span class="linenos">509</span></a>            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;deconv_1of2_scale_x2&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="DeconvolutionBlock-510"><a href="#DeconvolutionBlock-510"><span class="linenos">510</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose2</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> 
</span><span id="DeconvolutionBlock-511"><a href="#DeconvolutionBlock-511"><span class="linenos">511</span></a>            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;deconv_2of2_scale_x2&#39;</span><span class="p">,</span> 
</span><span id="DeconvolutionBlock-512"><a href="#DeconvolutionBlock-512"><span class="linenos">512</span></a>            <span class="n">activation</span><span class="o">=</span><span class="n">output_activation</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="DeconvolutionBlock-513"><a href="#DeconvolutionBlock-513"><span class="linenos">513</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> 
</span><span id="DeconvolutionBlock-514"><a href="#DeconvolutionBlock-514"><span class="linenos">514</span></a>            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="DeconvolutionBlock-515"><a href="#DeconvolutionBlock-515"><span class="linenos">515</span></a>            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;deconv_scale_x&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> 
</span><span id="DeconvolutionBlock-516"><a href="#DeconvolutionBlock-516"><span class="linenos">516</span></a>            <span class="n">activation</span><span class="o">=</span><span class="n">output_activation</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="DeconvolutionBlock-517"><a href="#DeconvolutionBlock-517"><span class="linenos">517</span></a>
</span><span id="DeconvolutionBlock-518"><a href="#DeconvolutionBlock-518"><span class="linenos">518</span></a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="DeconvolutionBlock-519"><a href="#DeconvolutionBlock-519"><span class="linenos">519</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> 
</span><span id="DeconvolutionBlock-520"><a href="#DeconvolutionBlock-520"><span class="linenos">520</span></a>                <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</span><span id="DeconvolutionBlock-521"><a href="#DeconvolutionBlock-521"><span class="linenos">521</span></a>
</span><span id="DeconvolutionBlock-522"><a href="#DeconvolutionBlock-522"><span class="linenos">522</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="DeconvolutionBlock-523"><a href="#DeconvolutionBlock-523"><span class="linenos">523</span></a>        <span class="sd">&quot;&quot;&quot;</span>
</span><span id="DeconvolutionBlock-524"><a href="#DeconvolutionBlock-524"><span class="linenos">524</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="DeconvolutionBlock-525"><a href="#DeconvolutionBlock-525"><span class="linenos">525</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
</span><span id="DeconvolutionBlock-526"><a href="#DeconvolutionBlock-526"><span class="linenos">526</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="DeconvolutionBlock-527"><a href="#DeconvolutionBlock-527"><span class="linenos">527</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="DeconvolutionBlock-528"><a href="#DeconvolutionBlock-528"><span class="linenos">528</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
</span><span id="DeconvolutionBlock-529"><a href="#DeconvolutionBlock-529"><span class="linenos">529</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="DeconvolutionBlock-530"><a href="#DeconvolutionBlock-530"><span class="linenos">530</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="DeconvolutionBlock-531"><a href="#DeconvolutionBlock-531"><span class="linenos">531</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="DeconvolutionBlock-532"><a href="#DeconvolutionBlock-532"><span class="linenos">532</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="DeconvolutionBlock-533"><a href="#DeconvolutionBlock-533"><span class="linenos">533</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="DeconvolutionBlock-534"><a href="#DeconvolutionBlock-534"><span class="linenos">534</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Deconvolution or transposed convolution block.</p>

<h6 id="references">References</h6>

<p>[1] FSRCNN - Accelerating the Super-Resolution Convolutional Neural Network: 
https://arxiv.org/abs/1608.00367</p>
</div>


                            <div id="DeconvolutionBlock.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#DeconvolutionBlock.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">DeconvolutionBlock</span><span class="signature">(scale, n_filters, output_activation=None, name_suffix=&#39;&#39;)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DeconvolutionBlock.__init__-503"><a href="#DeconvolutionBlock.__init__-503"><span class="linenos">503</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">output_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="DeconvolutionBlock.__init__-504"><a href="#DeconvolutionBlock.__init__-504"><span class="linenos">504</span></a>                 <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="DeconvolutionBlock.__init__-505"><a href="#DeconvolutionBlock.__init__-505"><span class="linenos">505</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Deconvolution&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="DeconvolutionBlock.__init__-506"><a href="#DeconvolutionBlock.__init__-506"><span class="linenos">506</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
</span><span id="DeconvolutionBlock.__init__-507"><a href="#DeconvolutionBlock.__init__-507"><span class="linenos">507</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_activation</span> <span class="o">=</span> <span class="n">output_activation</span>
</span><span id="DeconvolutionBlock.__init__-508"><a href="#DeconvolutionBlock.__init__-508"><span class="linenos">508</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose1</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> 
</span><span id="DeconvolutionBlock.__init__-509"><a href="#DeconvolutionBlock.__init__-509"><span class="linenos">509</span></a>            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;deconv_1of2_scale_x2&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="DeconvolutionBlock.__init__-510"><a href="#DeconvolutionBlock.__init__-510"><span class="linenos">510</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose2</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> 
</span><span id="DeconvolutionBlock.__init__-511"><a href="#DeconvolutionBlock.__init__-511"><span class="linenos">511</span></a>            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;deconv_2of2_scale_x2&#39;</span><span class="p">,</span> 
</span><span id="DeconvolutionBlock.__init__-512"><a href="#DeconvolutionBlock.__init__-512"><span class="linenos">512</span></a>            <span class="n">activation</span><span class="o">=</span><span class="n">output_activation</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="DeconvolutionBlock.__init__-513"><a href="#DeconvolutionBlock.__init__-513"><span class="linenos">513</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> 
</span><span id="DeconvolutionBlock.__init__-514"><a href="#DeconvolutionBlock.__init__-514"><span class="linenos">514</span></a>            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> 
</span><span id="DeconvolutionBlock.__init__-515"><a href="#DeconvolutionBlock.__init__-515"><span class="linenos">515</span></a>            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;deconv_scale_x&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> 
</span><span id="DeconvolutionBlock.__init__-516"><a href="#DeconvolutionBlock.__init__-516"><span class="linenos">516</span></a>            <span class="n">activation</span><span class="o">=</span><span class="n">output_activation</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="DeconvolutionBlock.compute_output_shape" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#DeconvolutionBlock.compute_output_shape">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">compute_output_shape</span><span class="signature">(self, input_shape)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DeconvolutionBlock.compute_output_shape-518"><a href="#DeconvolutionBlock.compute_output_shape-518"><span class="linenos">518</span></a>    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
</span><span id="DeconvolutionBlock.compute_output_shape-519"><a href="#DeconvolutionBlock.compute_output_shape-519"><span class="linenos">519</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> 
</span><span id="DeconvolutionBlock.compute_output_shape-520"><a href="#DeconvolutionBlock.compute_output_shape-520"><span class="linenos">520</span></a>                <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Computes the output shape of the layer.</p>

<p>This method will cause the layer's state to be built, if that has not
happened before. This requires that the layer will later be used with
inputs that match the input shape provided here.</p>

<p>Args:
    input_shape: Shape tuple (tuple of integers)
        or list of shape tuples (one per output tensor of the layer).
        Shape tuples can include None for free dimensions,
        instead of an integer.</p>

<p>Returns:
    An input shape tuple.</p>
</div>


                            </div>
                            <div id="DeconvolutionBlock.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#DeconvolutionBlock.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, x)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DeconvolutionBlock.call-522"><a href="#DeconvolutionBlock.call-522"><span class="linenos">522</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="DeconvolutionBlock.call-523"><a href="#DeconvolutionBlock.call-523"><span class="linenos">523</span></a>        <span class="sd">&quot;&quot;&quot;</span>
</span><span id="DeconvolutionBlock.call-524"><a href="#DeconvolutionBlock.call-524"><span class="linenos">524</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="DeconvolutionBlock.call-525"><a href="#DeconvolutionBlock.call-525"><span class="linenos">525</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
</span><span id="DeconvolutionBlock.call-526"><a href="#DeconvolutionBlock.call-526"><span class="linenos">526</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="DeconvolutionBlock.call-527"><a href="#DeconvolutionBlock.call-527"><span class="linenos">527</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="DeconvolutionBlock.call-528"><a href="#DeconvolutionBlock.call-528"><span class="linenos">528</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
</span><span id="DeconvolutionBlock.call-529"><a href="#DeconvolutionBlock.call-529"><span class="linenos">529</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="DeconvolutionBlock.call-530"><a href="#DeconvolutionBlock.call-530"><span class="linenos">530</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="DeconvolutionBlock.call-531"><a href="#DeconvolutionBlock.call-531"><span class="linenos">531</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="DeconvolutionBlock.call-532"><a href="#DeconvolutionBlock.call-532"><span class="linenos">532</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="DeconvolutionBlock.call-533"><a href="#DeconvolutionBlock.call-533"><span class="linenos">533</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2dtranspose</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="DeconvolutionBlock.call-534"><a href="#DeconvolutionBlock.call-534"><span class="linenos">534</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="DeconvolutionBlock.build" class="function">build</dd>
                <dd id="DeconvolutionBlock.add_weight" class="function">add_weight</dd>
                <dd id="DeconvolutionBlock.get_config" class="function">get_config</dd>
                <dd id="DeconvolutionBlock.from_config" class="function">from_config</dd>
                <dd id="DeconvolutionBlock.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="DeconvolutionBlock.compute_mask" class="function">compute_mask</dd>
                <dd id="DeconvolutionBlock.dtype" class="variable">dtype</dd>
                <dd id="DeconvolutionBlock.name" class="variable">name</dd>
                <dd id="DeconvolutionBlock.supports_masking" class="variable">supports_masking</dd>
                <dd id="DeconvolutionBlock.dynamic" class="variable">dynamic</dd>
                <dd id="DeconvolutionBlock.stateful" class="variable">stateful</dd>
                <dd id="DeconvolutionBlock.trainable" class="variable">trainable</dd>
                <dd id="DeconvolutionBlock.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="DeconvolutionBlock.input_spec" class="variable">input_spec</dd>
                <dd id="DeconvolutionBlock.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="DeconvolutionBlock.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="DeconvolutionBlock.weights" class="variable">weights</dd>
                <dd id="DeconvolutionBlock.updates" class="variable">updates</dd>
                <dd id="DeconvolutionBlock.losses" class="variable">losses</dd>
                <dd id="DeconvolutionBlock.add_loss" class="function">add_loss</dd>
                <dd id="DeconvolutionBlock.metrics" class="variable">metrics</dd>
                <dd id="DeconvolutionBlock.add_metric" class="function">add_metric</dd>
                <dd id="DeconvolutionBlock.add_update" class="function">add_update</dd>
                <dd id="DeconvolutionBlock.set_weights" class="function">set_weights</dd>
                <dd id="DeconvolutionBlock.get_weights" class="function">get_weights</dd>
                <dd id="DeconvolutionBlock.finalize_state" class="function">finalize_state</dd>
                <dd id="DeconvolutionBlock.get_updates_for" class="function">get_updates_for</dd>
                <dd id="DeconvolutionBlock.get_losses_for" class="function">get_losses_for</dd>
                <dd id="DeconvolutionBlock.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="DeconvolutionBlock.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="DeconvolutionBlock.input_mask" class="variable">input_mask</dd>
                <dd id="DeconvolutionBlock.output_mask" class="variable">output_mask</dd>
                <dd id="DeconvolutionBlock.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="DeconvolutionBlock.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="DeconvolutionBlock.get_input_at" class="function">get_input_at</dd>
                <dd id="DeconvolutionBlock.get_output_at" class="function">get_output_at</dd>
                <dd id="DeconvolutionBlock.input" class="variable">input</dd>
                <dd id="DeconvolutionBlock.output" class="variable">output</dd>
                <dd id="DeconvolutionBlock.input_shape" class="variable">input_shape</dd>
                <dd id="DeconvolutionBlock.count_params" class="function">count_params</dd>
                <dd id="DeconvolutionBlock.output_shape" class="variable">output_shape</dd>
                <dd id="DeconvolutionBlock.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="DeconvolutionBlock.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="DeconvolutionBlock.apply" class="function">apply</dd>
                <dd id="DeconvolutionBlock.add_variable" class="function">add_variable</dd>
                <dd id="DeconvolutionBlock.variables" class="variable">variables</dd>
                <dd id="DeconvolutionBlock.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="DeconvolutionBlock.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="DeconvolutionBlock.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="DeconvolutionBlock.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="DeconvolutionBlock.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="DeconvolutionBlock.name_scope" class="variable">name_scope</dd>
                <dd id="DeconvolutionBlock.submodules" class="variable">submodules</dd>
                <dd id="DeconvolutionBlock.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="ChannelAttention2D">
                                <div class="attr class">
        <a class="headerlink" href="#ChannelAttention2D">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">ChannelAttention2D</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ChannelAttention2D-537"><a href="#ChannelAttention2D-537"><span class="linenos">537</span></a><span class="k">class</span> <span class="nc">ChannelAttention2D</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="ChannelAttention2D-538"><a href="#ChannelAttention2D-538"><span class="linenos">538</span></a>    <span class="sd">&quot;&quot;&quot;</span>
</span><span id="ChannelAttention2D-539"><a href="#ChannelAttention2D-539"><span class="linenos">539</span></a><span class="sd">    Channel Attention for CNNs. Inputs need to be Conv2D feature maps.</span>
</span><span id="ChannelAttention2D-540"><a href="#ChannelAttention2D-540"><span class="linenos">540</span></a><span class="sd">    The layer implements the following:</span>
</span><span id="ChannelAttention2D-541"><a href="#ChannelAttention2D-541"><span class="linenos">541</span></a><span class="sd">        1. Average Pooling to create `[1,1,C]` vectors</span>
</span><span id="ChannelAttention2D-542"><a href="#ChannelAttention2D-542"><span class="linenos">542</span></a><span class="sd">        2. Conv2D with k=1 for fully connected features and relu ac</span>
</span><span id="ChannelAttention2D-543"><a href="#ChannelAttention2D-543"><span class="linenos">543</span></a><span class="sd">        3. Sigmoid activation to create attention maps</span>
</span><span id="ChannelAttention2D-544"><a href="#ChannelAttention2D-544"><span class="linenos">544</span></a>
</span><span id="ChannelAttention2D-545"><a href="#ChannelAttention2D-545"><span class="linenos">545</span></a><span class="sd">    Adapted from visual_attention_tf: </span>
</span><span id="ChannelAttention2D-546"><a href="#ChannelAttention2D-546"><span class="linenos">546</span></a><span class="sd">    https://github.com/vinayak19th/Visual_attention_tf/blob/main/src/visual_attention/channel_attention.py</span>
</span><span id="ChannelAttention2D-547"><a href="#ChannelAttention2D-547"><span class="linenos">547</span></a><span class="sd">    </span>
</span><span id="ChannelAttention2D-548"><a href="#ChannelAttention2D-548"><span class="linenos">548</span></a><span class="sd">    Parameters</span>
</span><span id="ChannelAttention2D-549"><a href="#ChannelAttention2D-549"><span class="linenos">549</span></a><span class="sd">    ----------</span>
</span><span id="ChannelAttention2D-550"><a href="#ChannelAttention2D-550"><span class="linenos">550</span></a><span class="sd">    nf [int]: number of filters or channels</span>
</span><span id="ChannelAttention2D-551"><a href="#ChannelAttention2D-551"><span class="linenos">551</span></a><span class="sd">    r[int] : Reduction factor</span>
</span><span id="ChannelAttention2D-552"><a href="#ChannelAttention2D-552"><span class="linenos">552</span></a><span class="sd">    </span>
</span><span id="ChannelAttention2D-553"><a href="#ChannelAttention2D-553"><span class="linenos">553</span></a><span class="sd">    Input</span>
</span><span id="ChannelAttention2D-554"><a href="#ChannelAttention2D-554"><span class="linenos">554</span></a><span class="sd">    -----</span>
</span><span id="ChannelAttention2D-555"><a href="#ChannelAttention2D-555"><span class="linenos">555</span></a><span class="sd">    Feature maps : Conv2D feature maps of the shape `[batch,W,H,C]`.</span>
</span><span id="ChannelAttention2D-556"><a href="#ChannelAttention2D-556"><span class="linenos">556</span></a><span class="sd">    </span>
</span><span id="ChannelAttention2D-557"><a href="#ChannelAttention2D-557"><span class="linenos">557</span></a><span class="sd">    Output</span>
</span><span id="ChannelAttention2D-558"><a href="#ChannelAttention2D-558"><span class="linenos">558</span></a><span class="sd">    ------</span>
</span><span id="ChannelAttention2D-559"><a href="#ChannelAttention2D-559"><span class="linenos">559</span></a><span class="sd">    Attention activated Conv2D features of shape `[batch,W,H,C]`.</span>
</span><span id="ChannelAttention2D-560"><a href="#ChannelAttention2D-560"><span class="linenos">560</span></a><span class="sd">    </span>
</span><span id="ChannelAttention2D-561"><a href="#ChannelAttention2D-561"><span class="linenos">561</span></a><span class="sd">    Reference</span>
</span><span id="ChannelAttention2D-562"><a href="#ChannelAttention2D-562"><span class="linenos">562</span></a><span class="sd">    ---------</span>
</span><span id="ChannelAttention2D-563"><a href="#ChannelAttention2D-563"><span class="linenos">563</span></a><span class="sd">    CBAM: Convolutional Block Attention Module (Sanghyun Woo et al 2018): </span>
</span><span id="ChannelAttention2D-564"><a href="#ChannelAttention2D-564"><span class="linenos">564</span></a><span class="sd">    https://arxiv.org/abs/1807.06521</span>
</span><span id="ChannelAttention2D-565"><a href="#ChannelAttention2D-565"><span class="linenos">565</span></a>
</span><span id="ChannelAttention2D-566"><a href="#ChannelAttention2D-566"><span class="linenos">566</span></a><span class="sd">    Notes</span>
</span><span id="ChannelAttention2D-567"><a href="#ChannelAttention2D-567"><span class="linenos">567</span></a><span class="sd">    -----</span>
</span><span id="ChannelAttention2D-568"><a href="#ChannelAttention2D-568"><span class="linenos">568</span></a><span class="sd">    Here is a code example for using `ChannelAttention2D` in a CNN:</span>
</span><span id="ChannelAttention2D-569"><a href="#ChannelAttention2D-569"><span class="linenos">569</span></a><span class="sd">    ```python</span>
</span><span id="ChannelAttention2D-570"><a href="#ChannelAttention2D-570"><span class="linenos">570</span></a><span class="sd">    inp = Input(shape=(1920,1080,3))</span>
</span><span id="ChannelAttention2D-571"><a href="#ChannelAttention2D-571"><span class="linenos">571</span></a><span class="sd">    cnn_layer = Conv2D(32,3,,activation=&#39;relu&#39;, padding=&#39;same&#39;)(inp)</span>
</span><span id="ChannelAttention2D-572"><a href="#ChannelAttention2D-572"><span class="linenos">572</span></a><span class="sd">    # Using the .shape[-1] to simplify network modifications. Can directly input number of channels as well</span>
</span><span id="ChannelAttention2D-573"><a href="#ChannelAttention2D-573"><span class="linenos">573</span></a><span class="sd">    attention_cnn = ChannelAttention2D(cnn_layer.shape[-1],cnn_layer.shape[1:-1])(cnn_layer)</span>
</span><span id="ChannelAttention2D-574"><a href="#ChannelAttention2D-574"><span class="linenos">574</span></a><span class="sd">    #ADD DNN layers .....</span>
</span><span id="ChannelAttention2D-575"><a href="#ChannelAttention2D-575"><span class="linenos">575</span></a><span class="sd">    ```</span>
</span><span id="ChannelAttention2D-576"><a href="#ChannelAttention2D-576"><span class="linenos">576</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ChannelAttention2D-577"><a href="#ChannelAttention2D-577"><span class="linenos">577</span></a>
</span><span id="ChannelAttention2D-578"><a href="#ChannelAttention2D-578"><span class="linenos">578</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="ChannelAttention2D-579"><a href="#ChannelAttention2D-579"><span class="linenos">579</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="ChannelAttention2D-580"><a href="#ChannelAttention2D-580"><span class="linenos">580</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">nf</span> <span class="o">=</span> <span class="n">nf</span>
</span><span id="ChannelAttention2D-581"><a href="#ChannelAttention2D-581"><span class="linenos">581</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">r</span>
</span><span id="ChannelAttention2D-582"><a href="#ChannelAttention2D-582"><span class="linenos">582</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">nf</span><span class="o">/</span><span class="n">r</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="ChannelAttention2D-583"><a href="#ChannelAttention2D-583"><span class="linenos">583</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">nf</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="ChannelAttention2D-584"><a href="#ChannelAttention2D-584"><span class="linenos">584</span></a>
</span><span id="ChannelAttention2D-585"><a href="#ChannelAttention2D-585"><span class="linenos">585</span></a>    <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
</span><span id="ChannelAttention2D-586"><a href="#ChannelAttention2D-586"><span class="linenos">586</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="ChannelAttention2D-587"><a href="#ChannelAttention2D-587"><span class="linenos">587</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="ChannelAttention2D-588"><a href="#ChannelAttention2D-588"><span class="linenos">588</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="ChannelAttention2D-589"><a href="#ChannelAttention2D-589"><span class="linenos">589</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="ChannelAttention2D-590"><a href="#ChannelAttention2D-590"><span class="linenos">590</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="ChannelAttention2D-591"><a href="#ChannelAttention2D-591"><span class="linenos">591</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="ChannelAttention2D-592"><a href="#ChannelAttention2D-592"><span class="linenos">592</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="ChannelAttention2D-593"><a href="#ChannelAttention2D-593"><span class="linenos">593</span></a>        <span class="k">return</span> <span class="n">y</span>
</span><span id="ChannelAttention2D-594"><a href="#ChannelAttention2D-594"><span class="linenos">594</span></a>
</span><span id="ChannelAttention2D-595"><a href="#ChannelAttention2D-595"><span class="linenos">595</span></a>    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="ChannelAttention2D-596"><a href="#ChannelAttention2D-596"><span class="linenos">596</span></a>        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
</span><span id="ChannelAttention2D-597"><a href="#ChannelAttention2D-597"><span class="linenos">597</span></a>        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;Att_filters&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">nf</span><span class="p">})</span>
</span><span id="ChannelAttention2D-598"><a href="#ChannelAttention2D-598"><span class="linenos">598</span></a>        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;Red_factor&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">})</span>
</span><span id="ChannelAttention2D-599"><a href="#ChannelAttention2D-599"><span class="linenos">599</span></a>        <span class="k">return</span> <span class="n">config</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Channel Attention for CNNs. Inputs need to be Conv2D feature maps.
The layer implements the following:
    1. Average Pooling to create <code>[1,1,C]</code> vectors
    2. Conv2D with k=1 for fully connected features and relu ac
    3. Sigmoid activation to create attention maps</p>

<p>Adapted from visual_attention_tf: 
https://github.com/vinayak19th/Visual_attention_tf/blob/main/src/visual_attention/channel_attention.py</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><p><strong>nf [int]</strong> (number of filters or channels):</p></li>
<li><p><strong>r[int]</strong> (Reduction factor):</p></li>
</ul>

<h6 id="input">Input</h6>

<p>Feature maps : Conv2D feature maps of the shape <code>[batch,W,H,C]</code>.</p>

<h6 id="output">Output</h6>

<p>Attention activated Conv2D features of shape <code>[batch,W,H,C]</code>.</p>

<h6 id="reference">Reference</h6>

<p>CBAM: Convolutional Block Attention Module (Sanghyun Woo et al 2018): 
https://arxiv.org/abs/1807.06521</p>

<h6 id="notes">Notes</h6>

<p>Here is a code example for using <code><a href="#ChannelAttention2D">ChannelAttention2D</a></code> in a CNN:</p>

<div class="pdoc-code codehilite"><pre><span></span><code><span class="n">inp</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1920</span><span class="p">,</span><span class="mi">1080</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">cnn_layer</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">3</span><span class="p">,,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">inp</span><span class="p">)</span>
<span class="c1"># Using the .shape[-1] to simplify network modifications. Can directly input number of channels as well</span>
<span class="n">attention_cnn</span> <span class="o">=</span> <span class="n">ChannelAttention2D</span><span class="p">(</span><span class="n">cnn_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">cnn_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])(</span><span class="n">cnn_layer</span><span class="p">)</span>
<span class="c1">#ADD DNN layers .....</span>
</code></pre></div>
</div>


                            <div id="ChannelAttention2D.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ChannelAttention2D.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">ChannelAttention2D</span><span class="signature">(nf, r=4, **kwargs)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ChannelAttention2D.__init__-578"><a href="#ChannelAttention2D.__init__-578"><span class="linenos">578</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="ChannelAttention2D.__init__-579"><a href="#ChannelAttention2D.__init__-579"><span class="linenos">579</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="ChannelAttention2D.__init__-580"><a href="#ChannelAttention2D.__init__-580"><span class="linenos">580</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">nf</span> <span class="o">=</span> <span class="n">nf</span>
</span><span id="ChannelAttention2D.__init__-581"><a href="#ChannelAttention2D.__init__-581"><span class="linenos">581</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">r</span>
</span><span id="ChannelAttention2D.__init__-582"><a href="#ChannelAttention2D.__init__-582"><span class="linenos">582</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">nf</span><span class="o">/</span><span class="n">r</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="ChannelAttention2D.__init__-583"><a href="#ChannelAttention2D.__init__-583"><span class="linenos">583</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">nf</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="ChannelAttention2D.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ChannelAttention2D.call">#&nbsp;&nbsp</a>

                <div class="decorator">@tf.function</div>

            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, x)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ChannelAttention2D.call-585"><a href="#ChannelAttention2D.call-585"><span class="linenos">585</span></a>    <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
</span><span id="ChannelAttention2D.call-586"><a href="#ChannelAttention2D.call-586"><span class="linenos">586</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="ChannelAttention2D.call-587"><a href="#ChannelAttention2D.call-587"><span class="linenos">587</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="ChannelAttention2D.call-588"><a href="#ChannelAttention2D.call-588"><span class="linenos">588</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="ChannelAttention2D.call-589"><a href="#ChannelAttention2D.call-589"><span class="linenos">589</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="ChannelAttention2D.call-590"><a href="#ChannelAttention2D.call-590"><span class="linenos">590</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="ChannelAttention2D.call-591"><a href="#ChannelAttention2D.call-591"><span class="linenos">591</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="ChannelAttention2D.call-592"><a href="#ChannelAttention2D.call-592"><span class="linenos">592</span></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="ChannelAttention2D.call-593"><a href="#ChannelAttention2D.call-593"><span class="linenos">593</span></a>        <span class="k">return</span> <span class="n">y</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="ChannelAttention2D.get_config" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ChannelAttention2D.get_config">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">get_config</span><span class="signature">(self)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ChannelAttention2D.get_config-595"><a href="#ChannelAttention2D.get_config-595"><span class="linenos">595</span></a>    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="ChannelAttention2D.get_config-596"><a href="#ChannelAttention2D.get_config-596"><span class="linenos">596</span></a>        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
</span><span id="ChannelAttention2D.get_config-597"><a href="#ChannelAttention2D.get_config-597"><span class="linenos">597</span></a>        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;Att_filters&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">nf</span><span class="p">})</span>
</span><span id="ChannelAttention2D.get_config-598"><a href="#ChannelAttention2D.get_config-598"><span class="linenos">598</span></a>        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;Red_factor&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">})</span>
</span><span id="ChannelAttention2D.get_config-599"><a href="#ChannelAttention2D.get_config-599"><span class="linenos">599</span></a>        <span class="k">return</span> <span class="n">config</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Returns the config of the layer.</p>

<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>

<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>

<p>Note that <code><a href="#ChannelAttention2D.get_config">get_config()</a></code> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>

<p>Returns:
    Python dictionary.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="ChannelAttention2D.build" class="function">build</dd>
                <dd id="ChannelAttention2D.add_weight" class="function">add_weight</dd>
                <dd id="ChannelAttention2D.from_config" class="function">from_config</dd>
                <dd id="ChannelAttention2D.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="ChannelAttention2D.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="ChannelAttention2D.compute_mask" class="function">compute_mask</dd>
                <dd id="ChannelAttention2D.dtype" class="variable">dtype</dd>
                <dd id="ChannelAttention2D.name" class="variable">name</dd>
                <dd id="ChannelAttention2D.supports_masking" class="variable">supports_masking</dd>
                <dd id="ChannelAttention2D.dynamic" class="variable">dynamic</dd>
                <dd id="ChannelAttention2D.stateful" class="variable">stateful</dd>
                <dd id="ChannelAttention2D.trainable" class="variable">trainable</dd>
                <dd id="ChannelAttention2D.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="ChannelAttention2D.input_spec" class="variable">input_spec</dd>
                <dd id="ChannelAttention2D.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="ChannelAttention2D.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="ChannelAttention2D.weights" class="variable">weights</dd>
                <dd id="ChannelAttention2D.updates" class="variable">updates</dd>
                <dd id="ChannelAttention2D.losses" class="variable">losses</dd>
                <dd id="ChannelAttention2D.add_loss" class="function">add_loss</dd>
                <dd id="ChannelAttention2D.metrics" class="variable">metrics</dd>
                <dd id="ChannelAttention2D.add_metric" class="function">add_metric</dd>
                <dd id="ChannelAttention2D.add_update" class="function">add_update</dd>
                <dd id="ChannelAttention2D.set_weights" class="function">set_weights</dd>
                <dd id="ChannelAttention2D.get_weights" class="function">get_weights</dd>
                <dd id="ChannelAttention2D.finalize_state" class="function">finalize_state</dd>
                <dd id="ChannelAttention2D.get_updates_for" class="function">get_updates_for</dd>
                <dd id="ChannelAttention2D.get_losses_for" class="function">get_losses_for</dd>
                <dd id="ChannelAttention2D.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="ChannelAttention2D.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="ChannelAttention2D.input_mask" class="variable">input_mask</dd>
                <dd id="ChannelAttention2D.output_mask" class="variable">output_mask</dd>
                <dd id="ChannelAttention2D.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="ChannelAttention2D.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="ChannelAttention2D.get_input_at" class="function">get_input_at</dd>
                <dd id="ChannelAttention2D.get_output_at" class="function">get_output_at</dd>
                <dd id="ChannelAttention2D.input" class="variable">input</dd>
                <dd id="ChannelAttention2D.output" class="variable">output</dd>
                <dd id="ChannelAttention2D.input_shape" class="variable">input_shape</dd>
                <dd id="ChannelAttention2D.count_params" class="function">count_params</dd>
                <dd id="ChannelAttention2D.output_shape" class="variable">output_shape</dd>
                <dd id="ChannelAttention2D.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="ChannelAttention2D.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="ChannelAttention2D.apply" class="function">apply</dd>
                <dd id="ChannelAttention2D.add_variable" class="function">add_variable</dd>
                <dd id="ChannelAttention2D.variables" class="variable">variables</dd>
                <dd id="ChannelAttention2D.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="ChannelAttention2D.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="ChannelAttention2D.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="ChannelAttention2D.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="ChannelAttention2D.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="ChannelAttention2D.name_scope" class="variable">name_scope</dd>
                <dd id="ChannelAttention2D.submodules" class="variable">submodules</dd>
                <dd id="ChannelAttention2D.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="EncoderBlock">
                                <div class="attr class">
        <a class="headerlink" href="#EncoderBlock">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">EncoderBlock</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EncoderBlock-602"><a href="#EncoderBlock-602"><span class="linenos">602</span></a><span class="k">class</span> <span class="nc">EncoderBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="EncoderBlock-603"><a href="#EncoderBlock-603"><span class="linenos">603</span></a>    <span class="sd">&quot;&quot;&quot;Encoder block for a decoder-encoder architecture, such as the UNET.</span>
</span><span id="EncoderBlock-604"><a href="#EncoderBlock-604"><span class="linenos">604</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="EncoderBlock-605"><a href="#EncoderBlock-605"><span class="linenos">605</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="EncoderBlock-606"><a href="#EncoderBlock-606"><span class="linenos">606</span></a>                 <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="EncoderBlock-607"><a href="#EncoderBlock-607"><span class="linenos">607</span></a>                 <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
</span><span id="EncoderBlock-608"><a href="#EncoderBlock-608"><span class="linenos">608</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;EncoderBlock&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="EncoderBlock-609"><a href="#EncoderBlock-609"><span class="linenos">609</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">ConvBlock</span><span class="p">(</span>
</span><span id="EncoderBlock-610"><a href="#EncoderBlock-610"><span class="linenos">610</span></a>            <span class="n">n_filters</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> 
</span><span id="EncoderBlock-611"><a href="#EncoderBlock-611"><span class="linenos">611</span></a>            <span class="n">dropout_variant</span><span class="o">=</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="n">normalization</span><span class="p">,</span> 
</span><span id="EncoderBlock-612"><a href="#EncoderBlock-612"><span class="linenos">612</span></a>            <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">)</span>
</span><span id="EncoderBlock-613"><a href="#EncoderBlock-613"><span class="linenos">613</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span><span id="EncoderBlock-614"><a href="#EncoderBlock-614"><span class="linenos">614</span></a>
</span><span id="EncoderBlock-615"><a href="#EncoderBlock-615"><span class="linenos">615</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="EncoderBlock-616"><a href="#EncoderBlock-616"><span class="linenos">616</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="EncoderBlock-617"><a href="#EncoderBlock-617"><span class="linenos">617</span></a>        <span class="n">Y_downsampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="EncoderBlock-618"><a href="#EncoderBlock-618"><span class="linenos">618</span></a>        <span class="k">return</span> <span class="p">[</span><span class="n">Y_downsampled</span><span class="p">,</span> <span class="n">Y</span><span class="p">]</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Encoder block for a decoder-encoder architecture, such as the UNET.</p>
</div>


                            <div id="EncoderBlock.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#EncoderBlock.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">EncoderBlock</span><span class="signature">(
    n_filters,
    activation=None,
    dropout_rate=0,
    dropout_variant=None,
    normalization=None,
    attention=False,
    name_suffix=&#39;&#39;
)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EncoderBlock.__init__-605"><a href="#EncoderBlock.__init__-605"><span class="linenos">605</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="EncoderBlock.__init__-606"><a href="#EncoderBlock.__init__-606"><span class="linenos">606</span></a>                 <span class="n">dropout_variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="EncoderBlock.__init__-607"><a href="#EncoderBlock.__init__-607"><span class="linenos">607</span></a>                 <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
</span><span id="EncoderBlock.__init__-608"><a href="#EncoderBlock.__init__-608"><span class="linenos">608</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;EncoderBlock&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="EncoderBlock.__init__-609"><a href="#EncoderBlock.__init__-609"><span class="linenos">609</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">ConvBlock</span><span class="p">(</span>
</span><span id="EncoderBlock.__init__-610"><a href="#EncoderBlock.__init__-610"><span class="linenos">610</span></a>            <span class="n">n_filters</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> 
</span><span id="EncoderBlock.__init__-611"><a href="#EncoderBlock.__init__-611"><span class="linenos">611</span></a>            <span class="n">dropout_variant</span><span class="o">=</span><span class="n">dropout_variant</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="n">normalization</span><span class="p">,</span> 
</span><span id="EncoderBlock.__init__-612"><a href="#EncoderBlock.__init__-612"><span class="linenos">612</span></a>            <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">)</span>
</span><span id="EncoderBlock.__init__-613"><a href="#EncoderBlock.__init__-613"><span class="linenos">613</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="EncoderBlock.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#EncoderBlock.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, X)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EncoderBlock.call-615"><a href="#EncoderBlock.call-615"><span class="linenos">615</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="EncoderBlock.call-616"><a href="#EncoderBlock.call-616"><span class="linenos">616</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="EncoderBlock.call-617"><a href="#EncoderBlock.call-617"><span class="linenos">617</span></a>        <span class="n">Y_downsampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span><span id="EncoderBlock.call-618"><a href="#EncoderBlock.call-618"><span class="linenos">618</span></a>        <span class="k">return</span> <span class="p">[</span><span class="n">Y_downsampled</span><span class="p">,</span> <span class="n">Y</span><span class="p">]</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>The <code><a href="#EncoderBlock.call">call()</a></code> method may not create state (except in its first invocation,
wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).
It is recommended to create state in <code><a href="#EncoderBlock.__init__">__init__()</a></code>, or the <code><a href="#EncoderBlock.build">build()</a></code> method
that is called automatically before <code><a href="#EncoderBlock.call">call()</a></code> executes the first time.</p>

<p>Args:
  inputs: Input tensor, or dict/list/tuple of input tensors.
    The first positional <code>inputs</code> argument is subject to special rules:
    - <code>inputs</code> must be explicitly passed. A layer cannot have zero
      arguments, and <code>inputs</code> cannot be provided via the default value
      of a keyword argument.
    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.
    - Keras mask metadata is only collected from <code>inputs</code>.
    - Layers are built (<code>build(input_shape)</code> method)
      using shape info from <code>inputs</code> only.
    - <code><a href="#EncoderBlock.input_spec">input_spec</a></code> compatibility is only checked against <code>inputs</code>.
    - Mixed precision input casting is only applied to <code>inputs</code>.
      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their
      casting behavior in mixed precision should be handled manually.
    - The SavedModel input specification is generated using <code>inputs</code> only.
    - Integration with various ecosystem packages like TFMOT, TFLite,
      TF.js, etc is only supported for <code>inputs</code> and not for tensors in
      positional and keyword arguments.
  <em>args: Additional positional arguments. May contain tensors, although
    this is not recommended, for the reasons above.
  *</em>kwargs: Additional keyword arguments. May contain tensors, although
    this is not recommended, for the reasons above.
    The following optional keyword arguments are reserved:
    - <code>training</code>: Boolean scalar tensor of Python boolean indicating
      whether the <code><a href="#EncoderBlock.call">call</a></code> is meant for training or inference.
    - <code>mask</code>: Boolean input mask. If the layer's <code><a href="#EncoderBlock.call">call()</a></code> method takes a
      <code>mask</code> argument, its default value will be set to the mask generated
      for <code>inputs</code> by the previous layer (if <code><a href="#EncoderBlock.input">input</a></code> did come from a layer
      that generated a corresponding mask, i.e. if it came from a Keras
      layer with masking support).</p>

<p>Returns:
  A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="EncoderBlock.build" class="function">build</dd>
                <dd id="EncoderBlock.add_weight" class="function">add_weight</dd>
                <dd id="EncoderBlock.get_config" class="function">get_config</dd>
                <dd id="EncoderBlock.from_config" class="function">from_config</dd>
                <dd id="EncoderBlock.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="EncoderBlock.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="EncoderBlock.compute_mask" class="function">compute_mask</dd>
                <dd id="EncoderBlock.dtype" class="variable">dtype</dd>
                <dd id="EncoderBlock.name" class="variable">name</dd>
                <dd id="EncoderBlock.supports_masking" class="variable">supports_masking</dd>
                <dd id="EncoderBlock.dynamic" class="variable">dynamic</dd>
                <dd id="EncoderBlock.stateful" class="variable">stateful</dd>
                <dd id="EncoderBlock.trainable" class="variable">trainable</dd>
                <dd id="EncoderBlock.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="EncoderBlock.input_spec" class="variable">input_spec</dd>
                <dd id="EncoderBlock.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="EncoderBlock.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="EncoderBlock.weights" class="variable">weights</dd>
                <dd id="EncoderBlock.updates" class="variable">updates</dd>
                <dd id="EncoderBlock.losses" class="variable">losses</dd>
                <dd id="EncoderBlock.add_loss" class="function">add_loss</dd>
                <dd id="EncoderBlock.metrics" class="variable">metrics</dd>
                <dd id="EncoderBlock.add_metric" class="function">add_metric</dd>
                <dd id="EncoderBlock.add_update" class="function">add_update</dd>
                <dd id="EncoderBlock.set_weights" class="function">set_weights</dd>
                <dd id="EncoderBlock.get_weights" class="function">get_weights</dd>
                <dd id="EncoderBlock.finalize_state" class="function">finalize_state</dd>
                <dd id="EncoderBlock.get_updates_for" class="function">get_updates_for</dd>
                <dd id="EncoderBlock.get_losses_for" class="function">get_losses_for</dd>
                <dd id="EncoderBlock.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="EncoderBlock.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="EncoderBlock.input_mask" class="variable">input_mask</dd>
                <dd id="EncoderBlock.output_mask" class="variable">output_mask</dd>
                <dd id="EncoderBlock.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="EncoderBlock.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="EncoderBlock.get_input_at" class="function">get_input_at</dd>
                <dd id="EncoderBlock.get_output_at" class="function">get_output_at</dd>
                <dd id="EncoderBlock.input" class="variable">input</dd>
                <dd id="EncoderBlock.output" class="variable">output</dd>
                <dd id="EncoderBlock.input_shape" class="variable">input_shape</dd>
                <dd id="EncoderBlock.count_params" class="function">count_params</dd>
                <dd id="EncoderBlock.output_shape" class="variable">output_shape</dd>
                <dd id="EncoderBlock.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="EncoderBlock.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="EncoderBlock.apply" class="function">apply</dd>
                <dd id="EncoderBlock.add_variable" class="function">add_variable</dd>
                <dd id="EncoderBlock.variables" class="variable">variables</dd>
                <dd id="EncoderBlock.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="EncoderBlock.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="EncoderBlock.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="EncoderBlock.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="EncoderBlock.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="EncoderBlock.name_scope" class="variable">name_scope</dd>
                <dd id="EncoderBlock.submodules" class="variable">submodules</dd>
                <dd id="EncoderBlock.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="PadConcat">
                                <div class="attr class">
        <a class="headerlink" href="#PadConcat">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">PadConcat</span><wbr>(<span class="base">keras.engine.base_layer.Layer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PadConcat-621"><a href="#PadConcat-621"><span class="linenos">621</span></a><span class="k">class</span> <span class="nc">PadConcat</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
</span><span id="PadConcat-622"><a href="#PadConcat-622"><span class="linenos">622</span></a>    <span class="sd">&quot;&quot;&quot;Concatenate layer that takes two tensors, if needed it pads to match </span>
</span><span id="PadConcat-623"><a href="#PadConcat-623"><span class="linenos">623</span></a><span class="sd">    height and width. </span>
</span><span id="PadConcat-624"><a href="#PadConcat-624"><span class="linenos">624</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="PadConcat-625"><a href="#PadConcat-625"><span class="linenos">625</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
</span><span id="PadConcat-626"><a href="#PadConcat-626"><span class="linenos">626</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Concatenate&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="PadConcat-627"><a href="#PadConcat-627"><span class="linenos">627</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">debug</span> <span class="o">=</span> <span class="n">debug</span>
</span><span id="PadConcat-628"><a href="#PadConcat-628"><span class="linenos">628</span></a>
</span><span id="PadConcat-629"><a href="#PadConcat-629"><span class="linenos">629</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="PadConcat-630"><a href="#PadConcat-630"><span class="linenos">630</span></a>        <span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">)</span> <span class="o">=</span> <span class="n">X</span>
</span><span id="PadConcat-631"><a href="#PadConcat-631"><span class="linenos">631</span></a>        <span class="n">y1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="PadConcat-632"><a href="#PadConcat-632"><span class="linenos">632</span></a>        <span class="n">x1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="PadConcat-633"><a href="#PadConcat-633"><span class="linenos">633</span></a>        <span class="n">y2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="PadConcat-634"><a href="#PadConcat-634"><span class="linenos">634</span></a>        <span class="n">x2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="PadConcat-635"><a href="#PadConcat-635"><span class="linenos">635</span></a>
</span><span id="PadConcat-636"><a href="#PadConcat-636"><span class="linenos">636</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span>
</span><span id="PadConcat-637"><a href="#PadConcat-637"><span class="linenos">637</span></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;input1 (</span><span class="si">{</span><span class="n">y1</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">x1</span><span class="si">}</span><span class="s1">) input2 (</span><span class="si">{</span><span class="n">y2</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">x2</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
</span><span id="PadConcat-638"><a href="#PadConcat-638"><span class="linenos">638</span></a>
</span><span id="PadConcat-639"><a href="#PadConcat-639"><span class="linenos">639</span></a>        <span class="k">if</span> <span class="n">y2</span> <span class="o">&lt;</span> <span class="n">y1</span><span class="p">:</span>
</span><span id="PadConcat-640"><a href="#PadConcat-640"><span class="linenos">640</span></a>            <span class="n">t2</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">y1</span> <span class="o">-</span> <span class="n">y2</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))(</span><span class="n">t2</span><span class="p">)</span>
</span><span id="PadConcat-641"><a href="#PadConcat-641"><span class="linenos">641</span></a>        <span class="k">elif</span> <span class="n">y2</span> <span class="o">&gt;</span> <span class="n">y1</span><span class="p">:</span>
</span><span id="PadConcat-642"><a href="#PadConcat-642"><span class="linenos">642</span></a>            <span class="n">t1</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">y2</span> <span class="o">-</span> <span class="n">y1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))(</span><span class="n">t1</span><span class="p">)</span>
</span><span id="PadConcat-643"><a href="#PadConcat-643"><span class="linenos">643</span></a>
</span><span id="PadConcat-644"><a href="#PadConcat-644"><span class="linenos">644</span></a>        <span class="k">if</span> <span class="n">x2</span> <span class="o">&lt;</span> <span class="n">x1</span><span class="p">:</span>
</span><span id="PadConcat-645"><a href="#PadConcat-645"><span class="linenos">645</span></a>            <span class="n">t2</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)))(</span><span class="n">t2</span><span class="p">)</span>
</span><span id="PadConcat-646"><a href="#PadConcat-646"><span class="linenos">646</span></a>        <span class="k">elif</span> <span class="n">x2</span> <span class="o">&gt;</span> <span class="n">x1</span><span class="p">:</span>
</span><span id="PadConcat-647"><a href="#PadConcat-647"><span class="linenos">647</span></a>            <span class="n">t1</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="p">)))(</span><span class="n">t1</span><span class="p">)</span>
</span><span id="PadConcat-648"><a href="#PadConcat-648"><span class="linenos">648</span></a>
</span><span id="PadConcat-649"><a href="#PadConcat-649"><span class="linenos">649</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span>
</span><span id="PadConcat-650"><a href="#PadConcat-650"><span class="linenos">650</span></a>            <span class="n">y1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="PadConcat-651"><a href="#PadConcat-651"><span class="linenos">651</span></a>            <span class="n">x1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="PadConcat-652"><a href="#PadConcat-652"><span class="linenos">652</span></a>            <span class="n">y2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="PadConcat-653"><a href="#PadConcat-653"><span class="linenos">653</span></a>            <span class="n">x2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="PadConcat-654"><a href="#PadConcat-654"><span class="linenos">654</span></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;output1 (</span><span class="si">{</span><span class="n">y1</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">x1</span><span class="si">}</span><span class="s1">) output2 (</span><span class="si">{</span><span class="n">y2</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">x2</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
</span><span id="PadConcat-655"><a href="#PadConcat-655"><span class="linenos">655</span></a>
</span><span id="PadConcat-656"><a href="#PadConcat-656"><span class="linenos">656</span></a>        <span class="k">return</span> <span class="n">Concatenate</span><span class="p">()([</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">])</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Concatenate layer that takes two tensors, if needed it pads to match 
height and width.</p>
</div>


                            <div id="PadConcat.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#PadConcat.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">PadConcat</span><span class="signature">(debug=False, name_suffix=&#39;&#39;)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PadConcat.__init__-625"><a href="#PadConcat.__init__-625"><span class="linenos">625</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
</span><span id="PadConcat.__init__-626"><a href="#PadConcat.__init__-626"><span class="linenos">626</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Concatenate&#39;</span> <span class="o">+</span> <span class="n">name_suffix</span><span class="p">)</span>
</span><span id="PadConcat.__init__-627"><a href="#PadConcat.__init__-627"><span class="linenos">627</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">debug</span> <span class="o">=</span> <span class="n">debug</span>
</span></pre></div>

        </details>

    

                            </div>
                            <div id="PadConcat.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#PadConcat.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, X)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PadConcat.call-629"><a href="#PadConcat.call-629"><span class="linenos">629</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="PadConcat.call-630"><a href="#PadConcat.call-630"><span class="linenos">630</span></a>        <span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">)</span> <span class="o">=</span> <span class="n">X</span>
</span><span id="PadConcat.call-631"><a href="#PadConcat.call-631"><span class="linenos">631</span></a>        <span class="n">y1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="PadConcat.call-632"><a href="#PadConcat.call-632"><span class="linenos">632</span></a>        <span class="n">x1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="PadConcat.call-633"><a href="#PadConcat.call-633"><span class="linenos">633</span></a>        <span class="n">y2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="PadConcat.call-634"><a href="#PadConcat.call-634"><span class="linenos">634</span></a>        <span class="n">x2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="PadConcat.call-635"><a href="#PadConcat.call-635"><span class="linenos">635</span></a>
</span><span id="PadConcat.call-636"><a href="#PadConcat.call-636"><span class="linenos">636</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span>
</span><span id="PadConcat.call-637"><a href="#PadConcat.call-637"><span class="linenos">637</span></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;input1 (</span><span class="si">{</span><span class="n">y1</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">x1</span><span class="si">}</span><span class="s1">) input2 (</span><span class="si">{</span><span class="n">y2</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">x2</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
</span><span id="PadConcat.call-638"><a href="#PadConcat.call-638"><span class="linenos">638</span></a>
</span><span id="PadConcat.call-639"><a href="#PadConcat.call-639"><span class="linenos">639</span></a>        <span class="k">if</span> <span class="n">y2</span> <span class="o">&lt;</span> <span class="n">y1</span><span class="p">:</span>
</span><span id="PadConcat.call-640"><a href="#PadConcat.call-640"><span class="linenos">640</span></a>            <span class="n">t2</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">y1</span> <span class="o">-</span> <span class="n">y2</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))(</span><span class="n">t2</span><span class="p">)</span>
</span><span id="PadConcat.call-641"><a href="#PadConcat.call-641"><span class="linenos">641</span></a>        <span class="k">elif</span> <span class="n">y2</span> <span class="o">&gt;</span> <span class="n">y1</span><span class="p">:</span>
</span><span id="PadConcat.call-642"><a href="#PadConcat.call-642"><span class="linenos">642</span></a>            <span class="n">t1</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">y2</span> <span class="o">-</span> <span class="n">y1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))(</span><span class="n">t1</span><span class="p">)</span>
</span><span id="PadConcat.call-643"><a href="#PadConcat.call-643"><span class="linenos">643</span></a>
</span><span id="PadConcat.call-644"><a href="#PadConcat.call-644"><span class="linenos">644</span></a>        <span class="k">if</span> <span class="n">x2</span> <span class="o">&lt;</span> <span class="n">x1</span><span class="p">:</span>
</span><span id="PadConcat.call-645"><a href="#PadConcat.call-645"><span class="linenos">645</span></a>            <span class="n">t2</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)))(</span><span class="n">t2</span><span class="p">)</span>
</span><span id="PadConcat.call-646"><a href="#PadConcat.call-646"><span class="linenos">646</span></a>        <span class="k">elif</span> <span class="n">x2</span> <span class="o">&gt;</span> <span class="n">x1</span><span class="p">:</span>
</span><span id="PadConcat.call-647"><a href="#PadConcat.call-647"><span class="linenos">647</span></a>            <span class="n">t1</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="p">)))(</span><span class="n">t1</span><span class="p">)</span>
</span><span id="PadConcat.call-648"><a href="#PadConcat.call-648"><span class="linenos">648</span></a>
</span><span id="PadConcat.call-649"><a href="#PadConcat.call-649"><span class="linenos">649</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span>
</span><span id="PadConcat.call-650"><a href="#PadConcat.call-650"><span class="linenos">650</span></a>            <span class="n">y1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="PadConcat.call-651"><a href="#PadConcat.call-651"><span class="linenos">651</span></a>            <span class="n">x1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="PadConcat.call-652"><a href="#PadConcat.call-652"><span class="linenos">652</span></a>            <span class="n">y2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="PadConcat.call-653"><a href="#PadConcat.call-653"><span class="linenos">653</span></a>            <span class="n">x2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="PadConcat.call-654"><a href="#PadConcat.call-654"><span class="linenos">654</span></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;output1 (</span><span class="si">{</span><span class="n">y1</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">x1</span><span class="si">}</span><span class="s1">) output2 (</span><span class="si">{</span><span class="n">y2</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">x2</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
</span><span id="PadConcat.call-655"><a href="#PadConcat.call-655"><span class="linenos">655</span></a>
</span><span id="PadConcat.call-656"><a href="#PadConcat.call-656"><span class="linenos">656</span></a>        <span class="k">return</span> <span class="n">Concatenate</span><span class="p">()([</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">])</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>The <code><a href="#PadConcat.call">call()</a></code> method may not create state (except in its first invocation,
wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).
It is recommended to create state in <code><a href="#PadConcat.__init__">__init__()</a></code>, or the <code><a href="#PadConcat.build">build()</a></code> method
that is called automatically before <code><a href="#PadConcat.call">call()</a></code> executes the first time.</p>

<p>Args:
  inputs: Input tensor, or dict/list/tuple of input tensors.
    The first positional <code>inputs</code> argument is subject to special rules:
    - <code>inputs</code> must be explicitly passed. A layer cannot have zero
      arguments, and <code>inputs</code> cannot be provided via the default value
      of a keyword argument.
    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.
    - Keras mask metadata is only collected from <code>inputs</code>.
    - Layers are built (<code>build(input_shape)</code> method)
      using shape info from <code>inputs</code> only.
    - <code><a href="#PadConcat.input_spec">input_spec</a></code> compatibility is only checked against <code>inputs</code>.
    - Mixed precision input casting is only applied to <code>inputs</code>.
      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their
      casting behavior in mixed precision should be handled manually.
    - The SavedModel input specification is generated using <code>inputs</code> only.
    - Integration with various ecosystem packages like TFMOT, TFLite,
      TF.js, etc is only supported for <code>inputs</code> and not for tensors in
      positional and keyword arguments.
  <em>args: Additional positional arguments. May contain tensors, although
    this is not recommended, for the reasons above.
  *</em>kwargs: Additional keyword arguments. May contain tensors, although
    this is not recommended, for the reasons above.
    The following optional keyword arguments are reserved:
    - <code>training</code>: Boolean scalar tensor of Python boolean indicating
      whether the <code><a href="#PadConcat.call">call</a></code> is meant for training or inference.
    - <code>mask</code>: Boolean input mask. If the layer's <code><a href="#PadConcat.call">call()</a></code> method takes a
      <code>mask</code> argument, its default value will be set to the mask generated
      for <code>inputs</code> by the previous layer (if <code><a href="#PadConcat.input">input</a></code> did come from a layer
      that generated a corresponding mask, i.e. if it came from a Keras
      layer with masking support).</p>

<p>Returns:
  A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="PadConcat.build" class="function">build</dd>
                <dd id="PadConcat.add_weight" class="function">add_weight</dd>
                <dd id="PadConcat.get_config" class="function">get_config</dd>
                <dd id="PadConcat.from_config" class="function">from_config</dd>
                <dd id="PadConcat.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="PadConcat.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="PadConcat.compute_mask" class="function">compute_mask</dd>
                <dd id="PadConcat.dtype" class="variable">dtype</dd>
                <dd id="PadConcat.name" class="variable">name</dd>
                <dd id="PadConcat.supports_masking" class="variable">supports_masking</dd>
                <dd id="PadConcat.dynamic" class="variable">dynamic</dd>
                <dd id="PadConcat.stateful" class="variable">stateful</dd>
                <dd id="PadConcat.trainable" class="variable">trainable</dd>
                <dd id="PadConcat.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="PadConcat.input_spec" class="variable">input_spec</dd>
                <dd id="PadConcat.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="PadConcat.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="PadConcat.weights" class="variable">weights</dd>
                <dd id="PadConcat.updates" class="variable">updates</dd>
                <dd id="PadConcat.losses" class="variable">losses</dd>
                <dd id="PadConcat.add_loss" class="function">add_loss</dd>
                <dd id="PadConcat.metrics" class="variable">metrics</dd>
                <dd id="PadConcat.add_metric" class="function">add_metric</dd>
                <dd id="PadConcat.add_update" class="function">add_update</dd>
                <dd id="PadConcat.set_weights" class="function">set_weights</dd>
                <dd id="PadConcat.get_weights" class="function">get_weights</dd>
                <dd id="PadConcat.finalize_state" class="function">finalize_state</dd>
                <dd id="PadConcat.get_updates_for" class="function">get_updates_for</dd>
                <dd id="PadConcat.get_losses_for" class="function">get_losses_for</dd>
                <dd id="PadConcat.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="PadConcat.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="PadConcat.input_mask" class="variable">input_mask</dd>
                <dd id="PadConcat.output_mask" class="variable">output_mask</dd>
                <dd id="PadConcat.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="PadConcat.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="PadConcat.get_input_at" class="function">get_input_at</dd>
                <dd id="PadConcat.get_output_at" class="function">get_output_at</dd>
                <dd id="PadConcat.input" class="variable">input</dd>
                <dd id="PadConcat.output" class="variable">output</dd>
                <dd id="PadConcat.input_shape" class="variable">input_shape</dd>
                <dd id="PadConcat.count_params" class="function">count_params</dd>
                <dd id="PadConcat.output_shape" class="variable">output_shape</dd>
                <dd id="PadConcat.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="PadConcat.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="PadConcat.apply" class="function">apply</dd>
                <dd id="PadConcat.add_variable" class="function">add_variable</dd>
                <dd id="PadConcat.variables" class="variable">variables</dd>
                <dd id="PadConcat.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="PadConcat.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="PadConcat.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="PadConcat.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="PadConcat.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="PadConcat.name_scope" class="variable">name_scope</dd>
                <dd id="PadConcat.submodules" class="variable">submodules</dd>
                <dd id="PadConcat.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="MCDropout">
                                <div class="attr class">
        <a class="headerlink" href="#MCDropout">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">MCDropout</span><wbr>(<span class="base">keras.layers.core.dropout.Dropout</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MCDropout-659"><a href="#MCDropout-659"><span class="linenos">659</span></a><span class="k">class</span> <span class="nc">MCDropout</span><span class="p">(</span><span class="n">Dropout</span><span class="p">):</span>
</span><span id="MCDropout-660"><a href="#MCDropout-660"><span class="linenos">660</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="MCDropout-661"><a href="#MCDropout-661"><span class="linenos">661</span></a>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Applies Dropout to the input.</p>

<p>The Dropout layer randomly sets input units to 0 with a frequency of <code>rate</code>
at each step during training time, which helps prevent overfitting.
Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over
all inputs is unchanged.</p>

<p>Note that the Dropout layer only applies when <code>training</code> is set to True
such that no values are dropped during inference. When using <code>model.fit</code>,
<code>training</code> will be appropriately set to True automatically, and in other
contexts, you can set the kwarg explicitly to True when calling the layer.</p>

<p>(This is in contrast to setting <code>trainable=False</code> for a Dropout layer.
<code><a href="#MCDropout.trainable">trainable</a></code> does not affect the layer's behavior, as Dropout does
not have any variables/weights that can be frozen during training.)</p>

<div class="pdoc-code codehilite"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">.2</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="go">[[0. 1.]</span>
<span class="go"> [2. 3.]</span>
<span class="go"> [4. 5.]</span>
<span class="go"> [6. 7.]</span>
<span class="go"> [8. 9.]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
<span class="go">tf.Tensor(</span>
<span class="go">[[ 0.    1.25]</span>
<span class="go"> [ 2.5   3.75]</span>
<span class="go"> [ 5.    6.25]</span>
<span class="go"> [ 7.5   8.75]</span>
<span class="go"> [10.    0.  ]], shape=(5, 2), dtype=float32)</span>
</code></pre></div>

<p>Args:
  rate: Float between 0 and 1. Fraction of the input units to drop.
  noise_shape: 1D integer tensor representing the shape of the
    binary dropout mask that will be multiplied with the input.
    For instance, if your inputs have shape
    <code>(batch_size, timesteps, features)</code> and
    you want the dropout mask to be the same for all timesteps,
    you can use <code>noise_shape=(batch_size, 1, features)</code>.
  seed: A Python integer to use as random seed.</p>

<p>Call arguments:
  inputs: Input tensor (of any rank).
  training: Python boolean indicating whether the layer should behave in
    training mode (adding dropout) or in inference mode (doing nothing).</p>
</div>


                            <div id="MCDropout.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#MCDropout.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, inputs)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MCDropout.call-660"><a href="#MCDropout.call-660"><span class="linenos">660</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="MCDropout.call-661"><a href="#MCDropout.call-661"><span class="linenos">661</span></a>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>The <code><a href="#MCDropout.call">call()</a></code> method may not create state (except in its first invocation,
wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).
It is recommended to create state in <code><a href="#MCDropout.__init__">__init__()</a></code>, or the <code><a href="#MCDropout.build">build()</a></code> method
that is called automatically before <code><a href="#MCDropout.call">call()</a></code> executes the first time.</p>

<p>Args:
  inputs: Input tensor, or dict/list/tuple of input tensors.
    The first positional <code>inputs</code> argument is subject to special rules:
    - <code>inputs</code> must be explicitly passed. A layer cannot have zero
      arguments, and <code>inputs</code> cannot be provided via the default value
      of a keyword argument.
    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.
    - Keras mask metadata is only collected from <code>inputs</code>.
    - Layers are built (<code>build(input_shape)</code> method)
      using shape info from <code>inputs</code> only.
    - <code><a href="#MCDropout.input_spec">input_spec</a></code> compatibility is only checked against <code>inputs</code>.
    - Mixed precision input casting is only applied to <code>inputs</code>.
      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their
      casting behavior in mixed precision should be handled manually.
    - The SavedModel input specification is generated using <code>inputs</code> only.
    - Integration with various ecosystem packages like TFMOT, TFLite,
      TF.js, etc is only supported for <code>inputs</code> and not for tensors in
      positional and keyword arguments.
  <em>args: Additional positional arguments. May contain tensors, although
    this is not recommended, for the reasons above.
  *</em>kwargs: Additional keyword arguments. May contain tensors, although
    this is not recommended, for the reasons above.
    The following optional keyword arguments are reserved:
    - <code>training</code>: Boolean scalar tensor of Python boolean indicating
      whether the <code><a href="#MCDropout.call">call</a></code> is meant for training or inference.
    - <code>mask</code>: Boolean input mask. If the layer's <code><a href="#MCDropout.call">call()</a></code> method takes a
      <code>mask</code> argument, its default value will be set to the mask generated
      for <code>inputs</code> by the previous layer (if <code><a href="#MCDropout.input">input</a></code> did come from a layer
      that generated a corresponding mask, i.e. if it came from a Keras
      layer with masking support).</p>

<p>Returns:
  A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.layers.core.dropout.Dropout</dt>
                                <dd id="MCDropout.__init__" class="function">Dropout</dd>
                <dd id="MCDropout.build" class="function">build</dd>
                <dd id="MCDropout.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="MCDropout.get_config" class="function">get_config</dd>

            </div>
            <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="MCDropout.supports_masking" class="variable">supports_masking</dd>
                <dd id="MCDropout.add_weight" class="function">add_weight</dd>
                <dd id="MCDropout.from_config" class="function">from_config</dd>
                <dd id="MCDropout.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="MCDropout.compute_mask" class="function">compute_mask</dd>
                <dd id="MCDropout.dtype" class="variable">dtype</dd>
                <dd id="MCDropout.name" class="variable">name</dd>
                <dd id="MCDropout.dynamic" class="variable">dynamic</dd>
                <dd id="MCDropout.stateful" class="variable">stateful</dd>
                <dd id="MCDropout.trainable" class="variable">trainable</dd>
                <dd id="MCDropout.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="MCDropout.input_spec" class="variable">input_spec</dd>
                <dd id="MCDropout.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="MCDropout.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="MCDropout.weights" class="variable">weights</dd>
                <dd id="MCDropout.updates" class="variable">updates</dd>
                <dd id="MCDropout.losses" class="variable">losses</dd>
                <dd id="MCDropout.add_loss" class="function">add_loss</dd>
                <dd id="MCDropout.metrics" class="variable">metrics</dd>
                <dd id="MCDropout.add_metric" class="function">add_metric</dd>
                <dd id="MCDropout.add_update" class="function">add_update</dd>
                <dd id="MCDropout.set_weights" class="function">set_weights</dd>
                <dd id="MCDropout.get_weights" class="function">get_weights</dd>
                <dd id="MCDropout.finalize_state" class="function">finalize_state</dd>
                <dd id="MCDropout.get_updates_for" class="function">get_updates_for</dd>
                <dd id="MCDropout.get_losses_for" class="function">get_losses_for</dd>
                <dd id="MCDropout.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="MCDropout.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="MCDropout.input_mask" class="variable">input_mask</dd>
                <dd id="MCDropout.output_mask" class="variable">output_mask</dd>
                <dd id="MCDropout.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="MCDropout.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="MCDropout.get_input_at" class="function">get_input_at</dd>
                <dd id="MCDropout.get_output_at" class="function">get_output_at</dd>
                <dd id="MCDropout.input" class="variable">input</dd>
                <dd id="MCDropout.output" class="variable">output</dd>
                <dd id="MCDropout.input_shape" class="variable">input_shape</dd>
                <dd id="MCDropout.count_params" class="function">count_params</dd>
                <dd id="MCDropout.output_shape" class="variable">output_shape</dd>
                <dd id="MCDropout.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="MCDropout.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="MCDropout.apply" class="function">apply</dd>
                <dd id="MCDropout.add_variable" class="function">add_variable</dd>
                <dd id="MCDropout.variables" class="variable">variables</dd>
                <dd id="MCDropout.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="MCDropout.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="MCDropout.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="MCDropout.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="MCDropout.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="MCDropout.name_scope" class="variable">name_scope</dd>
                <dd id="MCDropout.submodules" class="variable">submodules</dd>
                <dd id="MCDropout.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="MCGaussianDropout">
                                <div class="attr class">
        <a class="headerlink" href="#MCGaussianDropout">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">MCGaussianDropout</span><wbr>(<span class="base">keras.layers.noise.GaussianDropout</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MCGaussianDropout-664"><a href="#MCGaussianDropout-664"><span class="linenos">664</span></a><span class="k">class</span> <span class="nc">MCGaussianDropout</span><span class="p">(</span><span class="n">GaussianDropout</span><span class="p">):</span>
</span><span id="MCGaussianDropout-665"><a href="#MCGaussianDropout-665"><span class="linenos">665</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="MCGaussianDropout-666"><a href="#MCGaussianDropout-666"><span class="linenos">666</span></a>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Apply multiplicative 1-centered Gaussian noise.</p>

<p>As it is a regularization layer, it is only active at training time.</p>

<p>Args:
  rate: Float, drop probability (as with <code>Dropout</code>).
    The multiplicative noise will have
    standard deviation <code>sqrt(rate / (1 - rate))</code>.
  seed: Integer, optional random seed to enable deterministic behavior.</p>

<p>Call arguments:
  inputs: Input tensor (of any rank).
  training: Python boolean indicating whether the layer should behave in
    training mode (adding dropout) or in inference mode (doing nothing).</p>

<p>Input shape:
  Arbitrary. Use the keyword argument <code><a href="#MCGaussianDropout.input_shape">input_shape</a></code>
  (tuple of integers, does not include the samples axis)
  when using this layer as the first layer in a model.</p>

<p>Output shape:
  Same shape as input.</p>
</div>


                            <div id="MCGaussianDropout.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#MCGaussianDropout.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, inputs)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MCGaussianDropout.call-665"><a href="#MCGaussianDropout.call-665"><span class="linenos">665</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="MCGaussianDropout.call-666"><a href="#MCGaussianDropout.call-666"><span class="linenos">666</span></a>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>The <code><a href="#MCGaussianDropout.call">call()</a></code> method may not create state (except in its first invocation,
wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).
It is recommended to create state in <code><a href="#MCGaussianDropout.__init__">__init__()</a></code>, or the <code><a href="#MCGaussianDropout.build">build()</a></code> method
that is called automatically before <code><a href="#MCGaussianDropout.call">call()</a></code> executes the first time.</p>

<p>Args:
  inputs: Input tensor, or dict/list/tuple of input tensors.
    The first positional <code>inputs</code> argument is subject to special rules:
    - <code>inputs</code> must be explicitly passed. A layer cannot have zero
      arguments, and <code>inputs</code> cannot be provided via the default value
      of a keyword argument.
    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.
    - Keras mask metadata is only collected from <code>inputs</code>.
    - Layers are built (<code>build(input_shape)</code> method)
      using shape info from <code>inputs</code> only.
    - <code><a href="#MCGaussianDropout.input_spec">input_spec</a></code> compatibility is only checked against <code>inputs</code>.
    - Mixed precision input casting is only applied to <code>inputs</code>.
      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their
      casting behavior in mixed precision should be handled manually.
    - The SavedModel input specification is generated using <code>inputs</code> only.
    - Integration with various ecosystem packages like TFMOT, TFLite,
      TF.js, etc is only supported for <code>inputs</code> and not for tensors in
      positional and keyword arguments.
  <em>args: Additional positional arguments. May contain tensors, although
    this is not recommended, for the reasons above.
  *</em>kwargs: Additional keyword arguments. May contain tensors, although
    this is not recommended, for the reasons above.
    The following optional keyword arguments are reserved:
    - <code>training</code>: Boolean scalar tensor of Python boolean indicating
      whether the <code><a href="#MCGaussianDropout.call">call</a></code> is meant for training or inference.
    - <code>mask</code>: Boolean input mask. If the layer's <code><a href="#MCGaussianDropout.call">call()</a></code> method takes a
      <code>mask</code> argument, its default value will be set to the mask generated
      for <code>inputs</code> by the previous layer (if <code><a href="#MCGaussianDropout.input">input</a></code> did come from a layer
      that generated a corresponding mask, i.e. if it came from a Keras
      layer with masking support).</p>

<p>Returns:
  A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.layers.noise.GaussianDropout</dt>
                                <dd id="MCGaussianDropout.__init__" class="function">GaussianDropout</dd>
                <dd id="MCGaussianDropout.get_config" class="function">get_config</dd>
                <dd id="MCGaussianDropout.compute_output_shape" class="function">compute_output_shape</dd>

            </div>
            <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="MCGaussianDropout.supports_masking" class="variable">supports_masking</dd>
                <dd id="MCGaussianDropout.build" class="function">build</dd>
                <dd id="MCGaussianDropout.add_weight" class="function">add_weight</dd>
                <dd id="MCGaussianDropout.from_config" class="function">from_config</dd>
                <dd id="MCGaussianDropout.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="MCGaussianDropout.compute_mask" class="function">compute_mask</dd>
                <dd id="MCGaussianDropout.dtype" class="variable">dtype</dd>
                <dd id="MCGaussianDropout.name" class="variable">name</dd>
                <dd id="MCGaussianDropout.dynamic" class="variable">dynamic</dd>
                <dd id="MCGaussianDropout.stateful" class="variable">stateful</dd>
                <dd id="MCGaussianDropout.trainable" class="variable">trainable</dd>
                <dd id="MCGaussianDropout.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="MCGaussianDropout.input_spec" class="variable">input_spec</dd>
                <dd id="MCGaussianDropout.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="MCGaussianDropout.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="MCGaussianDropout.weights" class="variable">weights</dd>
                <dd id="MCGaussianDropout.updates" class="variable">updates</dd>
                <dd id="MCGaussianDropout.losses" class="variable">losses</dd>
                <dd id="MCGaussianDropout.add_loss" class="function">add_loss</dd>
                <dd id="MCGaussianDropout.metrics" class="variable">metrics</dd>
                <dd id="MCGaussianDropout.add_metric" class="function">add_metric</dd>
                <dd id="MCGaussianDropout.add_update" class="function">add_update</dd>
                <dd id="MCGaussianDropout.set_weights" class="function">set_weights</dd>
                <dd id="MCGaussianDropout.get_weights" class="function">get_weights</dd>
                <dd id="MCGaussianDropout.finalize_state" class="function">finalize_state</dd>
                <dd id="MCGaussianDropout.get_updates_for" class="function">get_updates_for</dd>
                <dd id="MCGaussianDropout.get_losses_for" class="function">get_losses_for</dd>
                <dd id="MCGaussianDropout.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="MCGaussianDropout.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="MCGaussianDropout.input_mask" class="variable">input_mask</dd>
                <dd id="MCGaussianDropout.output_mask" class="variable">output_mask</dd>
                <dd id="MCGaussianDropout.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="MCGaussianDropout.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="MCGaussianDropout.get_input_at" class="function">get_input_at</dd>
                <dd id="MCGaussianDropout.get_output_at" class="function">get_output_at</dd>
                <dd id="MCGaussianDropout.input" class="variable">input</dd>
                <dd id="MCGaussianDropout.output" class="variable">output</dd>
                <dd id="MCGaussianDropout.input_shape" class="variable">input_shape</dd>
                <dd id="MCGaussianDropout.count_params" class="function">count_params</dd>
                <dd id="MCGaussianDropout.output_shape" class="variable">output_shape</dd>
                <dd id="MCGaussianDropout.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="MCGaussianDropout.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="MCGaussianDropout.apply" class="function">apply</dd>
                <dd id="MCGaussianDropout.add_variable" class="function">add_variable</dd>
                <dd id="MCGaussianDropout.variables" class="variable">variables</dd>
                <dd id="MCGaussianDropout.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="MCGaussianDropout.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="MCGaussianDropout.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="MCGaussianDropout.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="MCGaussianDropout.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="MCGaussianDropout.name_scope" class="variable">name_scope</dd>
                <dd id="MCGaussianDropout.submodules" class="variable">submodules</dd>
                <dd id="MCGaussianDropout.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="MCSpatialDropout2D">
                                <div class="attr class">
        <a class="headerlink" href="#MCSpatialDropout2D">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">MCSpatialDropout2D</span><wbr>(<span class="base">keras.layers.core.spatial_dropout.SpatialDropout2D</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MCSpatialDropout2D-669"><a href="#MCSpatialDropout2D-669"><span class="linenos">669</span></a><span class="k">class</span> <span class="nc">MCSpatialDropout2D</span><span class="p">(</span><span class="n">SpatialDropout2D</span><span class="p">):</span>
</span><span id="MCSpatialDropout2D-670"><a href="#MCSpatialDropout2D-670"><span class="linenos">670</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="MCSpatialDropout2D-671"><a href="#MCSpatialDropout2D-671"><span class="linenos">671</span></a>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Spatial 2D version of Dropout.</p>

<p>This version performs the same function as Dropout, however, it drops
entire 2D feature maps instead of individual elements. If adjacent pixels
within feature maps are strongly correlated (as is normally the case in
early convolution layers) then regular dropout will not regularize the
activations and will otherwise just result in an effective learning rate
decrease. In this case, SpatialDropout2D will help promote independence
between feature maps and should be used instead.</p>

<p>Args:
  rate: Float between 0 and 1. Fraction of the input units to drop.
  data_format: 'channels_first' or 'channels_last'. In 'channels_first' mode,
    the channels dimension (the depth) is at index 1, in 'channels_last' mode
    is it at index 3. It defaults to the <code>image_data_format</code> value found in
    your Keras config file at <code>~/.keras/keras.json</code>. If you never set it, then
    it will be "channels_last".
Call arguments:
  inputs: A 4D tensor.
  training: Python boolean indicating whether the layer should behave in
    training mode (adding dropout) or in inference mode (doing nothing).
Input shape:
  4D tensor with shape: <code>(samples, channels, rows, cols)</code> if
    data_format='channels_first'
  or 4D tensor with shape: <code>(samples, rows, cols, channels)</code> if
    data_format='channels_last'.
Output shape: Same as input.
References: - <a href="https://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional
    Networks</a></p>
</div>


                            <div id="MCSpatialDropout2D.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#MCSpatialDropout2D.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, inputs)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MCSpatialDropout2D.call-670"><a href="#MCSpatialDropout2D.call-670"><span class="linenos">670</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="MCSpatialDropout2D.call-671"><a href="#MCSpatialDropout2D.call-671"><span class="linenos">671</span></a>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>The <code><a href="#MCSpatialDropout2D.call">call()</a></code> method may not create state (except in its first invocation,
wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).
It is recommended to create state in <code><a href="#MCSpatialDropout2D.__init__">__init__()</a></code>, or the <code><a href="#MCSpatialDropout2D.build">build()</a></code> method
that is called automatically before <code><a href="#MCSpatialDropout2D.call">call()</a></code> executes the first time.</p>

<p>Args:
  inputs: Input tensor, or dict/list/tuple of input tensors.
    The first positional <code>inputs</code> argument is subject to special rules:
    - <code>inputs</code> must be explicitly passed. A layer cannot have zero
      arguments, and <code>inputs</code> cannot be provided via the default value
      of a keyword argument.
    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.
    - Keras mask metadata is only collected from <code>inputs</code>.
    - Layers are built (<code>build(input_shape)</code> method)
      using shape info from <code>inputs</code> only.
    - <code><a href="#MCSpatialDropout2D.input_spec">input_spec</a></code> compatibility is only checked against <code>inputs</code>.
    - Mixed precision input casting is only applied to <code>inputs</code>.
      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their
      casting behavior in mixed precision should be handled manually.
    - The SavedModel input specification is generated using <code>inputs</code> only.
    - Integration with various ecosystem packages like TFMOT, TFLite,
      TF.js, etc is only supported for <code>inputs</code> and not for tensors in
      positional and keyword arguments.
  <em>args: Additional positional arguments. May contain tensors, although
    this is not recommended, for the reasons above.
  *</em>kwargs: Additional keyword arguments. May contain tensors, although
    this is not recommended, for the reasons above.
    The following optional keyword arguments are reserved:
    - <code>training</code>: Boolean scalar tensor of Python boolean indicating
      whether the <code><a href="#MCSpatialDropout2D.call">call</a></code> is meant for training or inference.
    - <code>mask</code>: Boolean input mask. If the layer's <code><a href="#MCSpatialDropout2D.call">call()</a></code> method takes a
      <code>mask</code> argument, its default value will be set to the mask generated
      for <code>inputs</code> by the previous layer (if <code><a href="#MCSpatialDropout2D.input">input</a></code> did come from a layer
      that generated a corresponding mask, i.e. if it came from a Keras
      layer with masking support).</p>

<p>Returns:
  A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.layers.core.spatial_dropout.SpatialDropout2D</dt>
                                <dd id="MCSpatialDropout2D.__init__" class="function">SpatialDropout2D</dd>

            </div>
            <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="MCSpatialDropout2D.input_spec" class="variable">input_spec</dd>
                <dd id="MCSpatialDropout2D.supports_masking" class="variable">supports_masking</dd>
                <dd id="MCSpatialDropout2D.add_weight" class="function">add_weight</dd>
                <dd id="MCSpatialDropout2D.from_config" class="function">from_config</dd>
                <dd id="MCSpatialDropout2D.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="MCSpatialDropout2D.compute_mask" class="function">compute_mask</dd>
                <dd id="MCSpatialDropout2D.dtype" class="variable">dtype</dd>
                <dd id="MCSpatialDropout2D.name" class="variable">name</dd>
                <dd id="MCSpatialDropout2D.dynamic" class="variable">dynamic</dd>
                <dd id="MCSpatialDropout2D.stateful" class="variable">stateful</dd>
                <dd id="MCSpatialDropout2D.trainable" class="variable">trainable</dd>
                <dd id="MCSpatialDropout2D.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="MCSpatialDropout2D.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="MCSpatialDropout2D.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="MCSpatialDropout2D.weights" class="variable">weights</dd>
                <dd id="MCSpatialDropout2D.updates" class="variable">updates</dd>
                <dd id="MCSpatialDropout2D.losses" class="variable">losses</dd>
                <dd id="MCSpatialDropout2D.add_loss" class="function">add_loss</dd>
                <dd id="MCSpatialDropout2D.metrics" class="variable">metrics</dd>
                <dd id="MCSpatialDropout2D.add_metric" class="function">add_metric</dd>
                <dd id="MCSpatialDropout2D.add_update" class="function">add_update</dd>
                <dd id="MCSpatialDropout2D.set_weights" class="function">set_weights</dd>
                <dd id="MCSpatialDropout2D.get_weights" class="function">get_weights</dd>
                <dd id="MCSpatialDropout2D.finalize_state" class="function">finalize_state</dd>
                <dd id="MCSpatialDropout2D.get_updates_for" class="function">get_updates_for</dd>
                <dd id="MCSpatialDropout2D.get_losses_for" class="function">get_losses_for</dd>
                <dd id="MCSpatialDropout2D.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="MCSpatialDropout2D.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="MCSpatialDropout2D.input_mask" class="variable">input_mask</dd>
                <dd id="MCSpatialDropout2D.output_mask" class="variable">output_mask</dd>
                <dd id="MCSpatialDropout2D.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="MCSpatialDropout2D.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="MCSpatialDropout2D.get_input_at" class="function">get_input_at</dd>
                <dd id="MCSpatialDropout2D.get_output_at" class="function">get_output_at</dd>
                <dd id="MCSpatialDropout2D.input" class="variable">input</dd>
                <dd id="MCSpatialDropout2D.output" class="variable">output</dd>
                <dd id="MCSpatialDropout2D.input_shape" class="variable">input_shape</dd>
                <dd id="MCSpatialDropout2D.count_params" class="function">count_params</dd>
                <dd id="MCSpatialDropout2D.output_shape" class="variable">output_shape</dd>
                <dd id="MCSpatialDropout2D.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="MCSpatialDropout2D.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="MCSpatialDropout2D.apply" class="function">apply</dd>
                <dd id="MCSpatialDropout2D.add_variable" class="function">add_variable</dd>
                <dd id="MCSpatialDropout2D.variables" class="variable">variables</dd>
                <dd id="MCSpatialDropout2D.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="MCSpatialDropout2D.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="MCSpatialDropout2D.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="MCSpatialDropout2D.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="MCSpatialDropout2D.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>keras.layers.core.dropout.Dropout</dt>
                                <dd id="MCSpatialDropout2D.build" class="function">build</dd>
                <dd id="MCSpatialDropout2D.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="MCSpatialDropout2D.get_config" class="function">get_config</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="MCSpatialDropout2D.name_scope" class="variable">name_scope</dd>
                <dd id="MCSpatialDropout2D.submodules" class="variable">submodules</dd>
                <dd id="MCSpatialDropout2D.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="MCSpatialDropout3D">
                                <div class="attr class">
        <a class="headerlink" href="#MCSpatialDropout3D">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">MCSpatialDropout3D</span><wbr>(<span class="base">keras.layers.core.spatial_dropout.SpatialDropout3D</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MCSpatialDropout3D-674"><a href="#MCSpatialDropout3D-674"><span class="linenos">674</span></a><span class="k">class</span> <span class="nc">MCSpatialDropout3D</span><span class="p">(</span><span class="n">SpatialDropout3D</span><span class="p">):</span>
</span><span id="MCSpatialDropout3D-675"><a href="#MCSpatialDropout3D-675"><span class="linenos">675</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="MCSpatialDropout3D-676"><a href="#MCSpatialDropout3D-676"><span class="linenos">676</span></a>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Spatial 3D version of Dropout.</p>

<p>This version performs the same function as Dropout, however, it drops
entire 3D feature maps instead of individual elements. If adjacent voxels
within feature maps are strongly correlated (as is normally the case in
early convolution layers) then regular dropout will not regularize the
activations and will otherwise just result in an effective learning rate
decrease. In this case, SpatialDropout3D will help promote independence
between feature maps and should be used instead.</p>

<p>Args:
  rate: Float between 0 and 1. Fraction of the input units to drop.
  data_format: 'channels_first' or 'channels_last'. In 'channels_first' mode,
    the channels dimension (the depth) is at index 1, in 'channels_last' mode
    is it at index 4. It defaults to the <code>image_data_format</code> value found in
    your Keras config file at <code>~/.keras/keras.json</code>. If you never set it, then
    it will be "channels_last".
Call arguments:
  inputs: A 5D tensor.
  training: Python boolean indicating whether the layer should behave in
    training mode (adding dropout) or in inference mode (doing nothing).
Input shape:
  5D tensor with shape: <code>(samples, channels, dim1, dim2, dim3)</code> if
    data_format='channels_first'
  or 5D tensor with shape: <code>(samples, dim1, dim2, dim3, channels)</code> if
    data_format='channels_last'.
Output shape: Same as input.
References: - <a href="https://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional
    Networks</a></p>
</div>


                            <div id="MCSpatialDropout3D.call" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#MCSpatialDropout3D.call">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">call</span><span class="signature">(self, inputs)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MCSpatialDropout3D.call-675"><a href="#MCSpatialDropout3D.call-675"><span class="linenos">675</span></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="MCSpatialDropout3D.call-676"><a href="#MCSpatialDropout3D.call-676"><span class="linenos">676</span></a>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>This is where the layer's logic lives.</p>

<p>The <code><a href="#MCSpatialDropout3D.call">call()</a></code> method may not create state (except in its first invocation,
wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).
It is recommended to create state in <code><a href="#MCSpatialDropout3D.__init__">__init__()</a></code>, or the <code><a href="#MCSpatialDropout3D.build">build()</a></code> method
that is called automatically before <code><a href="#MCSpatialDropout3D.call">call()</a></code> executes the first time.</p>

<p>Args:
  inputs: Input tensor, or dict/list/tuple of input tensors.
    The first positional <code>inputs</code> argument is subject to special rules:
    - <code>inputs</code> must be explicitly passed. A layer cannot have zero
      arguments, and <code>inputs</code> cannot be provided via the default value
      of a keyword argument.
    - NumPy array or Python scalar values in <code>inputs</code> get cast as tensors.
    - Keras mask metadata is only collected from <code>inputs</code>.
    - Layers are built (<code>build(input_shape)</code> method)
      using shape info from <code>inputs</code> only.
    - <code><a href="#MCSpatialDropout3D.input_spec">input_spec</a></code> compatibility is only checked against <code>inputs</code>.
    - Mixed precision input casting is only applied to <code>inputs</code>.
      If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their
      casting behavior in mixed precision should be handled manually.
    - The SavedModel input specification is generated using <code>inputs</code> only.
    - Integration with various ecosystem packages like TFMOT, TFLite,
      TF.js, etc is only supported for <code>inputs</code> and not for tensors in
      positional and keyword arguments.
  <em>args: Additional positional arguments. May contain tensors, although
    this is not recommended, for the reasons above.
  *</em>kwargs: Additional keyword arguments. May contain tensors, although
    this is not recommended, for the reasons above.
    The following optional keyword arguments are reserved:
    - <code>training</code>: Boolean scalar tensor of Python boolean indicating
      whether the <code><a href="#MCSpatialDropout3D.call">call</a></code> is meant for training or inference.
    - <code>mask</code>: Boolean input mask. If the layer's <code><a href="#MCSpatialDropout3D.call">call()</a></code> method takes a
      <code>mask</code> argument, its default value will be set to the mask generated
      for <code>inputs</code> by the previous layer (if <code><a href="#MCSpatialDropout3D.input">input</a></code> did come from a layer
      that generated a corresponding mask, i.e. if it came from a Keras
      layer with masking support).</p>

<p>Returns:
  A tensor or list/tuple of tensors.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>keras.layers.core.spatial_dropout.SpatialDropout3D</dt>
                                <dd id="MCSpatialDropout3D.__init__" class="function">SpatialDropout3D</dd>

            </div>
            <div><dt>keras.engine.base_layer.Layer</dt>
                                <dd id="MCSpatialDropout3D.input_spec" class="variable">input_spec</dd>
                <dd id="MCSpatialDropout3D.supports_masking" class="variable">supports_masking</dd>
                <dd id="MCSpatialDropout3D.add_weight" class="function">add_weight</dd>
                <dd id="MCSpatialDropout3D.from_config" class="function">from_config</dd>
                <dd id="MCSpatialDropout3D.compute_output_signature" class="function">compute_output_signature</dd>
                <dd id="MCSpatialDropout3D.compute_mask" class="function">compute_mask</dd>
                <dd id="MCSpatialDropout3D.dtype" class="variable">dtype</dd>
                <dd id="MCSpatialDropout3D.name" class="variable">name</dd>
                <dd id="MCSpatialDropout3D.dynamic" class="variable">dynamic</dd>
                <dd id="MCSpatialDropout3D.stateful" class="variable">stateful</dd>
                <dd id="MCSpatialDropout3D.trainable" class="variable">trainable</dd>
                <dd id="MCSpatialDropout3D.activity_regularizer" class="variable">activity_regularizer</dd>
                <dd id="MCSpatialDropout3D.trainable_weights" class="variable">trainable_weights</dd>
                <dd id="MCSpatialDropout3D.non_trainable_weights" class="variable">non_trainable_weights</dd>
                <dd id="MCSpatialDropout3D.weights" class="variable">weights</dd>
                <dd id="MCSpatialDropout3D.updates" class="variable">updates</dd>
                <dd id="MCSpatialDropout3D.losses" class="variable">losses</dd>
                <dd id="MCSpatialDropout3D.add_loss" class="function">add_loss</dd>
                <dd id="MCSpatialDropout3D.metrics" class="variable">metrics</dd>
                <dd id="MCSpatialDropout3D.add_metric" class="function">add_metric</dd>
                <dd id="MCSpatialDropout3D.add_update" class="function">add_update</dd>
                <dd id="MCSpatialDropout3D.set_weights" class="function">set_weights</dd>
                <dd id="MCSpatialDropout3D.get_weights" class="function">get_weights</dd>
                <dd id="MCSpatialDropout3D.finalize_state" class="function">finalize_state</dd>
                <dd id="MCSpatialDropout3D.get_updates_for" class="function">get_updates_for</dd>
                <dd id="MCSpatialDropout3D.get_losses_for" class="function">get_losses_for</dd>
                <dd id="MCSpatialDropout3D.get_input_mask_at" class="function">get_input_mask_at</dd>
                <dd id="MCSpatialDropout3D.get_output_mask_at" class="function">get_output_mask_at</dd>
                <dd id="MCSpatialDropout3D.input_mask" class="variable">input_mask</dd>
                <dd id="MCSpatialDropout3D.output_mask" class="variable">output_mask</dd>
                <dd id="MCSpatialDropout3D.get_input_shape_at" class="function">get_input_shape_at</dd>
                <dd id="MCSpatialDropout3D.get_output_shape_at" class="function">get_output_shape_at</dd>
                <dd id="MCSpatialDropout3D.get_input_at" class="function">get_input_at</dd>
                <dd id="MCSpatialDropout3D.get_output_at" class="function">get_output_at</dd>
                <dd id="MCSpatialDropout3D.input" class="variable">input</dd>
                <dd id="MCSpatialDropout3D.output" class="variable">output</dd>
                <dd id="MCSpatialDropout3D.input_shape" class="variable">input_shape</dd>
                <dd id="MCSpatialDropout3D.count_params" class="function">count_params</dd>
                <dd id="MCSpatialDropout3D.output_shape" class="variable">output_shape</dd>
                <dd id="MCSpatialDropout3D.inbound_nodes" class="variable">inbound_nodes</dd>
                <dd id="MCSpatialDropout3D.outbound_nodes" class="variable">outbound_nodes</dd>
                <dd id="MCSpatialDropout3D.apply" class="function">apply</dd>
                <dd id="MCSpatialDropout3D.add_variable" class="function">add_variable</dd>
                <dd id="MCSpatialDropout3D.variables" class="variable">variables</dd>
                <dd id="MCSpatialDropout3D.trainable_variables" class="variable">trainable_variables</dd>
                <dd id="MCSpatialDropout3D.non_trainable_variables" class="variable">non_trainable_variables</dd>
                <dd id="MCSpatialDropout3D.dtype_policy" class="variable">dtype_policy</dd>
                <dd id="MCSpatialDropout3D.compute_dtype" class="variable">compute_dtype</dd>
                <dd id="MCSpatialDropout3D.variable_dtype" class="variable">variable_dtype</dd>

            </div>
            <div><dt>keras.layers.core.dropout.Dropout</dt>
                                <dd id="MCSpatialDropout3D.build" class="function">build</dd>
                <dd id="MCSpatialDropout3D.compute_output_shape" class="function">compute_output_shape</dd>
                <dd id="MCSpatialDropout3D.get_config" class="function">get_config</dd>

            </div>
            <div><dt>tensorflow.python.module.module.Module</dt>
                                <dd id="MCSpatialDropout3D.name_scope" class="variable">name_scope</dd>
                <dd id="MCSpatialDropout3D.submodules" class="variable">submodules</dd>
                <dd id="MCSpatialDropout3D.with_name_scope" class="function">with_name_scope</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="get_dropout_layer">
                            <div class="attr function"><a class="headerlink" href="#get_dropout_layer">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">get_dropout_layer</span><span class="signature">(dropout_rate, dropout_variant, dim=2)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span id="get_dropout_layer-679"><a href="#get_dropout_layer-679"><span class="linenos">679</span></a><span class="k">def</span> <span class="nf">get_dropout_layer</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">dropout_variant</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
</span><span id="get_dropout_layer-680"><a href="#get_dropout_layer-680"><span class="linenos">680</span></a>    <span class="sd">&quot;&quot;&quot;Choose an return a dropout layer depending on the input arguments. If</span>
</span><span id="get_dropout_layer-681"><a href="#get_dropout_layer-681"><span class="linenos">681</span></a><span class="sd">    ``dropout_rate=0`` then an identity layer is returned (the input tensor </span>
</span><span id="get_dropout_layer-682"><a href="#get_dropout_layer-682"><span class="linenos">682</span></a><span class="sd">    is returned without any modification). </span>
</span><span id="get_dropout_layer-683"><a href="#get_dropout_layer-683"><span class="linenos">683</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="get_dropout_layer-684"><a href="#get_dropout_layer-684"><span class="linenos">684</span></a>    <span class="n">dropout_variant</span> <span class="o">=</span> <span class="n">checkarg_dropout_variant</span><span class="p">(</span><span class="n">dropout_variant</span><span class="p">)</span>
</span><span id="get_dropout_layer-685"><a href="#get_dropout_layer-685"><span class="linenos">685</span></a>    <span class="k">if</span> <span class="n">dropout_rate</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="get_dropout_layer-686"><a href="#get_dropout_layer-686"><span class="linenos">686</span></a>        <span class="k">if</span> <span class="n">dropout_variant</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">dropout_variant</span> <span class="o">==</span> <span class="s1">&#39;vanilla&#39;</span><span class="p">:</span>
</span><span id="get_dropout_layer-687"><a href="#get_dropout_layer-687"><span class="linenos">687</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="get_dropout_layer-688"><a href="#get_dropout_layer-688"><span class="linenos">688</span></a>        <span class="k">elif</span> <span class="n">dropout_variant</span> <span class="o">==</span> <span class="s1">&#39;gaussian&#39;</span><span class="p">:</span>
</span><span id="get_dropout_layer-689"><a href="#get_dropout_layer-689"><span class="linenos">689</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="n">GaussianDropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="get_dropout_layer-690"><a href="#get_dropout_layer-690"><span class="linenos">690</span></a>        <span class="k">elif</span> <span class="n">dropout_variant</span> <span class="o">==</span> <span class="s1">&#39;spatial&#39;</span><span class="p">:</span>
</span><span id="get_dropout_layer-691"><a href="#get_dropout_layer-691"><span class="linenos">691</span></a>            <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="get_dropout_layer-692"><a href="#get_dropout_layer-692"><span class="linenos">692</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="n">SpatialDropout2D</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="get_dropout_layer-693"><a href="#get_dropout_layer-693"><span class="linenos">693</span></a>            <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
</span><span id="get_dropout_layer-694"><a href="#get_dropout_layer-694"><span class="linenos">694</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="n">SpatialDropout3D</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="get_dropout_layer-695"><a href="#get_dropout_layer-695"><span class="linenos">695</span></a>        <span class="k">elif</span> <span class="n">dropout_variant</span> <span class="o">==</span> <span class="s1">&#39;mcdrop&#39;</span><span class="p">:</span>
</span><span id="get_dropout_layer-696"><a href="#get_dropout_layer-696"><span class="linenos">696</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="n">MCDropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="get_dropout_layer-697"><a href="#get_dropout_layer-697"><span class="linenos">697</span></a>        <span class="k">elif</span> <span class="n">dropout_variant</span> <span class="o">==</span> <span class="s1">&#39;mcgaussiandrop&#39;</span><span class="p">:</span>
</span><span id="get_dropout_layer-698"><a href="#get_dropout_layer-698"><span class="linenos">698</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="n">MCGaussianDropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="get_dropout_layer-699"><a href="#get_dropout_layer-699"><span class="linenos">699</span></a>        <span class="k">elif</span> <span class="n">dropout_variant</span> <span class="o">==</span> <span class="s1">&#39;mcspatialdrop&#39;</span><span class="p">:</span>
</span><span id="get_dropout_layer-700"><a href="#get_dropout_layer-700"><span class="linenos">700</span></a>            <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="get_dropout_layer-701"><a href="#get_dropout_layer-701"><span class="linenos">701</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="n">MCSpatialDropout2D</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="get_dropout_layer-702"><a href="#get_dropout_layer-702"><span class="linenos">702</span></a>            <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
</span><span id="get_dropout_layer-703"><a href="#get_dropout_layer-703"><span class="linenos">703</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="n">MCSpatialDropout3D</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
</span><span id="get_dropout_layer-704"><a href="#get_dropout_layer-704"><span class="linenos">704</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="get_dropout_layer-705"><a href="#get_dropout_layer-705"><span class="linenos">705</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="get_dropout_layer-706"><a href="#get_dropout_layer-706"><span class="linenos">706</span></a>    <span class="k">return</span> <span class="n">layer</span>
</span></pre></div>

        </details>

            <div class="docstring"><p>Choose an return a dropout layer depending on the input arguments. If
<code>dropout_rate=0</code> then an identity layer is returned (the input tensor 
is returned without any modification).</p>
</div>


                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.type) {
                    case "function":
                        heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span><span class="signature">${doc.signature}:</span>`;
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value">${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.type}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>